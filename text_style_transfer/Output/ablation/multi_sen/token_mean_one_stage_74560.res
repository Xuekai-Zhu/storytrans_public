0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 1.079 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:01,  1.09s/it]32it [00:01, 36.70it/s]71it [00:01, 86.43it/s]105it [00:01, 128.66it/s]139it [00:01, 169.25it/s]174it [00:01, 207.35it/s]209it [00:01, 240.12it/s]245it [00:01, 268.97it/s]279it [00:01, 287.42it/s]313it [00:02, 300.47it/s]347it [00:02, 310.15it/s]381it [00:02, 315.49it/s]415it [00:02, 318.71it/s]449it [00:02, 322.48it/s]483it [00:02, 325.13it/s]517it [00:02, 324.38it/s]552it [00:02, 328.16it/s]589it [00:02, 338.71it/s]625it [00:02, 342.31it/s]662it [00:03, 349.88it/s]698it [00:03, 351.39it/s]729it [00:03, 226.52it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:04<53:56,  4.45s/it]  1%|          | 5/729 [00:04<08:18,  1.45it/s]  1%|          | 7/729 [00:04<05:39,  2.13it/s]  2%|▏         | 11/729 [00:04<02:59,  4.01it/s]  2%|▏         | 18/729 [00:05<01:23,  8.55it/s]  4%|▎         | 26/729 [00:05<00:47, 14.81it/s]  5%|▍         | 33/729 [00:05<00:33, 20.97it/s]  6%|▌         | 41/729 [00:05<00:23, 28.83it/s]  7%|▋         | 50/729 [00:05<00:17, 38.12it/s]  8%|▊         | 59/729 [00:05<00:14, 47.00it/s]  9%|▉         | 68/729 [00:05<00:12, 54.96it/s] 10%|█         | 76/729 [00:06<00:19, 33.07it/s] 11%|█         | 82/729 [00:06<00:17, 36.59it/s] 12%|█▏        | 90/729 [00:06<00:14, 43.16it/s] 13%|█▎        | 97/729 [00:06<00:13, 45.49it/s] 14%|█▍        | 104/729 [00:06<00:12, 50.50it/s] 15%|█▌        | 112/729 [00:06<00:11, 55.55it/s] 16%|█▋        | 120/729 [00:06<00:10, 59.41it/s] 18%|█▊        | 129/729 [00:06<00:09, 65.34it/s] 19%|█▉        | 138/729 [00:07<00:08, 69.70it/s] 20%|██        | 147/729 [00:07<00:07, 73.07it/s] 21%|██▏       | 156/729 [00:07<00:07, 75.62it/s] 23%|██▎       | 165/729 [00:07<00:07, 77.68it/s] 24%|██▍       | 174/729 [00:07<00:07, 78.59it/s] 25%|██▌       | 183/729 [00:07<00:06, 80.00it/s] 26%|██▋       | 192/729 [00:07<00:06, 79.50it/s] 28%|██▊       | 201/729 [00:07<00:06, 80.03it/s] 29%|██▉       | 210/729 [00:07<00:06, 80.03it/s] 30%|███       | 219/729 [00:08<00:06, 81.02it/s] 31%|███▏      | 228/729 [00:08<00:06, 81.72it/s] 33%|███▎      | 237/729 [00:08<00:06, 81.69it/s] 34%|███▎      | 246/729 [00:08<00:05, 82.34it/s] 35%|███▍      | 255/729 [00:08<00:05, 82.47it/s] 36%|███▌      | 264/729 [00:08<00:05, 82.62it/s] 37%|███▋      | 273/729 [00:08<00:05, 81.95it/s] 39%|███▊      | 282/729 [00:08<00:05, 82.16it/s] 40%|███▉      | 291/729 [00:08<00:05, 82.79it/s] 41%|████      | 300/729 [00:09<00:05, 82.30it/s] 42%|████▏     | 309/729 [00:09<00:05, 82.30it/s] 44%|████▎     | 318/729 [00:09<00:05, 81.75it/s] 45%|████▍     | 327/729 [00:09<00:04, 82.37it/s] 46%|████▌     | 336/729 [00:09<00:04, 81.96it/s] 47%|████▋     | 345/729 [00:09<00:04, 81.88it/s] 49%|████▊     | 354/729 [00:09<00:04, 81.78it/s] 50%|████▉     | 363/729 [00:09<00:04, 81.72it/s] 51%|█████     | 372/729 [00:09<00:04, 82.30it/s] 52%|█████▏    | 381/729 [00:10<00:04, 81.89it/s] 53%|█████▎    | 390/729 [00:10<00:04, 81.55it/s] 55%|█████▍    | 399/729 [00:10<00:04, 81.03it/s] 56%|█████▌    | 408/729 [00:10<00:03, 81.60it/s] 57%|█████▋    | 417/729 [00:10<00:03, 82.02it/s] 58%|█████▊    | 426/729 [00:10<00:03, 81.51it/s] 60%|█████▉    | 435/729 [00:10<00:03, 81.69it/s] 61%|██████    | 444/729 [00:10<00:03, 81.78it/s] 62%|██████▏   | 453/729 [00:10<00:03, 81.48it/s] 63%|██████▎   | 462/729 [00:11<00:03, 81.97it/s] 65%|██████▍   | 471/729 [00:11<00:03, 81.39it/s] 66%|██████▌   | 480/729 [00:11<00:03, 81.79it/s] 67%|██████▋   | 489/729 [00:11<00:02, 82.59it/s] 68%|██████▊   | 498/729 [00:11<00:02, 82.13it/s] 70%|██████▉   | 507/729 [00:11<00:02, 82.18it/s] 71%|███████   | 516/729 [00:11<00:02, 80.43it/s] 72%|███████▏  | 525/729 [00:11<00:02, 80.95it/s] 73%|███████▎  | 534/729 [00:11<00:02, 81.86it/s] 74%|███████▍  | 543/729 [00:12<00:02, 82.09it/s] 76%|███████▌  | 552/729 [00:12<00:02, 80.13it/s] 77%|███████▋  | 561/729 [00:12<00:02, 81.37it/s] 78%|███████▊  | 570/729 [00:12<00:01, 81.30it/s] 79%|███████▉  | 579/729 [00:12<00:01, 81.54it/s] 81%|████████  | 588/729 [00:12<00:01, 82.44it/s] 82%|████████▏ | 597/729 [00:12<00:01, 82.83it/s] 83%|████████▎ | 606/729 [00:12<00:01, 82.59it/s] 84%|████████▍ | 615/729 [00:12<00:01, 82.21it/s] 86%|████████▌ | 624/729 [00:13<00:01, 82.13it/s] 87%|████████▋ | 633/729 [00:13<00:01, 82.53it/s] 88%|████████▊ | 642/729 [00:13<00:01, 82.52it/s] 89%|████████▉ | 651/729 [00:13<00:00, 82.39it/s] 91%|█████████ | 660/729 [00:13<00:00, 82.44it/s] 92%|█████████▏| 669/729 [00:13<00:00, 82.51it/s] 93%|█████████▎| 678/729 [00:13<00:00, 81.91it/s] 94%|█████████▍| 687/729 [00:13<00:00, 81.54it/s] 95%|█████████▌| 696/729 [00:13<00:00, 81.70it/s] 97%|█████████▋| 705/729 [00:14<00:00, 81.34it/s] 98%|█████████▊| 714/729 [00:14<00:00, 81.85it/s] 99%|█████████▉| 723/729 [00:14<00:00, 82.40it/s]100%|██████████| 729/729 [00:14<00:00, 50.93it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'lm_head.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:03,  6.53it/s]  9%|▊         | 2/23 [00:00<00:02,  7.56it/s] 13%|█▎        | 3/23 [00:00<00:05,  3.74it/s] 17%|█▋        | 4/23 [00:00<00:04,  4.70it/s] 22%|██▏       | 5/23 [00:01<00:03,  4.86it/s] 26%|██▌       | 6/23 [00:01<00:03,  5.41it/s] 30%|███       | 7/23 [00:01<00:02,  5.87it/s] 35%|███▍      | 8/23 [00:01<00:02,  6.29it/s] 39%|███▉      | 9/23 [00:01<00:02,  6.68it/s] 43%|████▎     | 10/23 [00:01<00:01,  6.80it/s] 48%|████▊     | 11/23 [00:01<00:01,  7.11it/s] 52%|█████▏    | 12/23 [00:01<00:01,  7.23it/s] 57%|█████▋    | 13/23 [00:02<00:01,  6.97it/s] 61%|██████    | 14/23 [00:02<00:01,  7.18it/s] 65%|██████▌   | 15/23 [00:02<00:01,  7.49it/s] 70%|██████▉   | 16/23 [00:02<00:01,  4.49it/s] 74%|███████▍  | 17/23 [00:02<00:01,  5.06it/s] 78%|███████▊  | 18/23 [00:03<00:01,  3.74it/s] 83%|████████▎ | 19/23 [00:03<00:00,  4.40it/s] 87%|████████▋ | 20/23 [00:03<00:00,  5.02it/s] 91%|█████████▏| 21/23 [00:03<00:00,  5.43it/s] 96%|█████████▌| 22/23 [00:03<00:00,  6.08it/s]100%|██████████| 23/23 [00:04<00:00,  5.74it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
17it [00:00, 166.45it/s]37it [00:00, 185.19it/s]60it [00:00, 204.14it/s]81it [00:00, 204.67it/s]102it [00:00, 191.31it/s]123it [00:00, 196.14it/s]144it [00:00, 199.63it/s]168it [00:00, 209.94it/s]191it [00:00, 213.52it/s]214it [00:01, 215.13it/s]236it [00:01, 214.10it/s]259it [00:01, 215.85it/s]281it [00:01, 215.67it/s]303it [00:01, 215.42it/s]325it [00:01, 216.04it/s]349it [00:01, 221.60it/s]372it [00:01, 221.31it/s]395it [00:01, 221.31it/s]418it [00:01, 211.89it/s]441it [00:02, 214.91it/s]463it [00:02, 213.66it/s]485it [00:02, 210.90it/s]507it [00:02, 210.12it/s]531it [00:02, 218.48it/s]554it [00:02, 220.00it/s]577it [00:02, 221.19it/s]600it [00:02, 219.33it/s]622it [00:02, 213.52it/s]647it [00:03, 221.49it/s]671it [00:03, 224.61it/s]694it [00:03, 219.65it/s]718it [00:03, 224.34it/s]729it [00:03, 214.14it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 8/729 [00:00<00:10, 69.64it/s]  2%|▏         | 15/729 [00:00<00:10, 69.62it/s]  3%|▎         | 23/729 [00:00<00:10, 70.50it/s]  4%|▍         | 31/729 [00:00<00:09, 71.95it/s]  5%|▌         | 39/729 [00:00<00:09, 72.91it/s]  6%|▋         | 47/729 [00:00<00:09, 72.92it/s]  8%|▊         | 55/729 [00:00<00:09, 73.30it/s]  9%|▊         | 63/729 [00:00<00:09, 72.71it/s] 10%|▉         | 71/729 [00:00<00:08, 73.13it/s] 11%|█         | 79/729 [00:01<00:08, 72.85it/s] 12%|█▏        | 87/729 [00:01<00:08, 72.23it/s] 13%|█▎        | 95/729 [00:01<00:08, 72.40it/s] 14%|█▍        | 103/729 [00:01<00:08, 71.87it/s] 15%|█▌        | 111/729 [00:01<00:08, 71.35it/s] 16%|█▋        | 119/729 [00:01<00:08, 71.78it/s] 17%|█▋        | 127/729 [00:01<00:08, 70.75it/s] 19%|█▊        | 135/729 [00:01<00:08, 71.42it/s] 20%|█▉        | 143/729 [00:01<00:08, 71.81it/s] 21%|██        | 151/729 [00:02<00:07, 72.43it/s] 22%|██▏       | 159/729 [00:02<00:07, 71.51it/s] 23%|██▎       | 167/729 [00:02<00:07, 72.89it/s] 24%|██▍       | 175/729 [00:02<00:07, 72.25it/s] 25%|██▌       | 183/729 [00:02<00:07, 73.11it/s] 26%|██▌       | 191/729 [00:02<00:07, 73.31it/s] 27%|██▋       | 199/729 [00:02<00:07, 73.02it/s] 28%|██▊       | 207/729 [00:02<00:07, 74.20it/s] 29%|██▉       | 215/729 [00:02<00:06, 73.63it/s] 31%|███       | 223/729 [00:03<00:06, 73.35it/s] 32%|███▏      | 231/729 [00:03<00:07, 71.05it/s] 33%|███▎      | 239/729 [00:03<00:06, 71.91it/s] 34%|███▍      | 247/729 [00:03<00:06, 71.72it/s] 35%|███▍      | 255/729 [00:03<00:06, 72.46it/s] 36%|███▌      | 263/729 [00:03<00:06, 72.20it/s] 37%|███▋      | 271/729 [00:03<00:06, 72.95it/s] 38%|███▊      | 279/729 [00:03<00:06, 72.37it/s] 39%|███▉      | 287/729 [00:03<00:06, 71.60it/s] 40%|████      | 295/729 [00:04<00:06, 71.05it/s] 42%|████▏     | 303/729 [00:04<00:05, 72.41it/s] 43%|████▎     | 311/729 [00:04<00:06, 62.91it/s] 44%|████▎     | 318/729 [00:04<00:06, 60.19it/s] 45%|████▍     | 325/729 [00:04<00:06, 60.04it/s] 46%|████▌     | 332/729 [00:04<00:06, 61.42it/s] 47%|████▋     | 339/729 [00:04<00:06, 62.41it/s] 47%|████▋     | 346/729 [00:04<00:06, 62.75it/s] 48%|████▊     | 353/729 [00:05<00:06, 62.42it/s] 49%|████▉     | 360/729 [00:05<00:05, 64.09it/s] 50%|█████     | 367/729 [00:05<00:05, 64.12it/s] 51%|█████▏    | 375/729 [00:05<00:05, 67.02it/s] 53%|█████▎    | 383/729 [00:05<00:04, 69.30it/s] 54%|█████▎    | 391/729 [00:05<00:04, 70.98it/s] 55%|█████▍    | 399/729 [00:05<00:04, 72.00it/s] 56%|█████▌    | 407/729 [00:05<00:04, 70.94it/s] 57%|█████▋    | 415/729 [00:05<00:04, 72.07it/s] 58%|█████▊    | 423/729 [00:06<00:04, 72.39it/s] 59%|█████▉    | 431/729 [00:06<00:04, 73.02it/s] 60%|██████    | 439/729 [00:06<00:03, 72.58it/s] 61%|██████▏   | 447/729 [00:06<00:03, 72.92it/s] 62%|██████▏   | 455/729 [00:06<00:03, 73.17it/s] 64%|██████▎   | 463/729 [00:06<00:03, 73.28it/s] 65%|██████▍   | 471/729 [00:06<00:03, 73.40it/s] 66%|██████▌   | 479/729 [00:06<00:03, 73.26it/s] 67%|██████▋   | 487/729 [00:06<00:03, 73.08it/s] 68%|██████▊   | 495/729 [00:07<00:03, 73.35it/s] 69%|██████▉   | 503/729 [00:07<00:03, 72.32it/s] 70%|███████   | 511/729 [00:07<00:02, 72.69it/s] 71%|███████   | 519/729 [00:07<00:02, 72.78it/s] 72%|███████▏  | 527/729 [00:07<00:02, 73.63it/s] 73%|███████▎  | 535/729 [00:07<00:02, 73.36it/s] 74%|███████▍  | 543/729 [00:07<00:02, 73.46it/s] 76%|███████▌  | 551/729 [00:07<00:02, 73.59it/s] 77%|███████▋  | 559/729 [00:07<00:02, 62.53it/s] 78%|███████▊  | 566/729 [00:08<00:02, 62.97it/s] 79%|███████▊  | 574/729 [00:08<00:02, 65.90it/s] 80%|███████▉  | 582/729 [00:08<00:02, 67.64it/s] 81%|████████  | 590/729 [00:08<00:02, 68.74it/s] 82%|████████▏ | 598/729 [00:08<00:01, 69.61it/s] 83%|████████▎ | 606/729 [00:08<00:01, 70.03it/s] 84%|████████▍ | 614/729 [00:08<00:01, 70.07it/s] 85%|████████▌ | 622/729 [00:08<00:01, 70.36it/s] 86%|████████▋ | 630/729 [00:08<00:01, 71.20it/s] 88%|████████▊ | 638/729 [00:09<00:01, 71.35it/s] 89%|████████▊ | 646/729 [00:09<00:01, 72.40it/s] 90%|████████▉ | 654/729 [00:09<00:01, 72.71it/s] 91%|█████████ | 662/729 [00:09<00:00, 70.91it/s] 92%|█████████▏| 670/729 [00:09<00:00, 71.54it/s] 93%|█████████▎| 678/729 [00:09<00:00, 71.49it/s] 94%|█████████▍| 686/729 [00:09<00:00, 71.27it/s] 95%|█████████▌| 694/729 [00:09<00:00, 70.69it/s] 96%|█████████▋| 702/729 [00:09<00:00, 71.34it/s] 97%|█████████▋| 710/729 [00:10<00:00, 72.03it/s] 98%|█████████▊| 718/729 [00:10<00:00, 72.17it/s]100%|█████████▉| 726/729 [00:10<00:00, 71.48it/s]100%|██████████| 729/729 [00:10<00:00, 70.64it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'lm_head.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:06,  3.29it/s]  9%|▊         | 2/23 [00:00<00:05,  3.66it/s] 13%|█▎        | 3/23 [00:00<00:05,  3.86it/s] 17%|█▋        | 4/23 [00:01<00:05,  3.65it/s] 22%|██▏       | 5/23 [00:01<00:04,  3.82it/s] 26%|██▌       | 6/23 [00:01<00:04,  3.81it/s] 30%|███       | 7/23 [00:01<00:04,  3.88it/s] 35%|███▍      | 8/23 [00:02<00:03,  3.93it/s] 39%|███▉      | 9/23 [00:02<00:03,  3.86it/s] 43%|████▎     | 10/23 [00:02<00:04,  3.17it/s] 48%|████▊     | 11/23 [00:03<00:03,  3.15it/s] 52%|█████▏    | 12/23 [00:03<00:03,  3.31it/s] 57%|█████▋    | 13/23 [00:03<00:02,  3.34it/s] 61%|██████    | 14/23 [00:03<00:02,  3.59it/s] 65%|██████▌   | 15/23 [00:04<00:02,  3.59it/s] 70%|██████▉   | 16/23 [00:04<00:01,  3.70it/s] 74%|███████▍  | 17/23 [00:04<00:01,  3.89it/s] 78%|███████▊  | 18/23 [00:04<00:01,  3.93it/s] 83%|████████▎ | 19/23 [00:05<00:01,  3.86it/s] 87%|████████▋ | 20/23 [00:05<00:00,  3.81it/s] 91%|█████████▏| 21/23 [00:05<00:00,  3.39it/s] 96%|█████████▌| 22/23 [00:06<00:00,  3.51it/s]100%|██████████| 23/23 [00:06<00:00,  3.85it/s]100%|██████████| 23/23 [00:06<00:00,  3.65it/s]
{'bleu-1': 0.20345314517252255, 'bleu-2': 0.056077527636948746, 'bleu-3': 0.005935554698219797, 'bleu-4': 0.0003569011801146379}
./predict/train_sen_mse_add_mask_in_input_ablation_token_mean_one_stage_1122-s-74560/test_sen_mse_add_mask_in_input_ablation_token_mean.0
perplexity: 23.571957
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.644125 R=0.635226 F=0.639445
begin predicting
acc : 0.7352537722908093
{'distinct-1': 0.027168810553926264, 'distinct-2': 0.3419795687604788, 'distinct-3': 0.7118958560362855, 'distinct-4': 0.8948185503806981}
{'bleu-1': 0.1613573496602464, 'bleu-2': 0.04198558070632849, 'bleu-3': 0.005368454745247819, 'bleu-4': 0.0005226806496094985}
./predict/train_sen_mse_add_mask_in_input_ablation_token_mean_one_stage_1122-s-74560/test_sen_mse_add_mask_in_input_ablation_token_mean.1
perplexity: 17.612112
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.616777 R=0.651924 F=0.633672
begin predicting
acc : 0.9176954732510288
{'distinct-1': 0.014939593913394474, 'distinct-2': 0.2678814481300246, 'distinct-3': 0.6205671077504726, 'distinct-4': 0.8211276088224554}
