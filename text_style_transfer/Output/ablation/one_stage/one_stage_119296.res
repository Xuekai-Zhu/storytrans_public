0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 0.864 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:00,  1.15it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
32it [00:00, 44.62it/s]65it [00:01, 92.33it/s]95it [00:01, 132.24it/s]122it [00:01, 144.57it/s]149it [00:01, 170.00it/s]174it [00:01, 188.22it/s]199it [00:01, 201.80it/s]224it [00:01, 214.35it/s]251it [00:01, 228.33it/s]277it [00:01, 235.99it/s]305it [00:02, 248.42it/s]337it [00:02, 267.87it/s]370it [00:02, 283.46it/s]400it [00:02, 277.10it/s]432it [00:02, 287.52it/s]462it [00:02, 288.56it/s]493it [00:02, 294.74it/s]524it [00:02, 296.68it/s]556it [00:02, 300.01it/s]590it [00:02, 308.87it/s]623it [00:03, 313.51it/s]655it [00:03, 313.70it/s]688it [00:03, 317.92it/s]722it [00:03, 323.29it/s]729it [00:03, 214.09it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:02<33:33,  2.77s/it]  1%|          | 8/729 [00:02<03:11,  3.76it/s]  2%|▏         | 12/729 [00:04<04:22,  2.73it/s]  3%|▎         | 19/729 [00:04<02:11,  5.40it/s]  4%|▎         | 26/729 [00:05<01:19,  8.86it/s]  5%|▍         | 33/729 [00:05<00:52, 13.17it/s]  5%|▌         | 40/729 [00:05<00:37, 18.34it/s]  6%|▋         | 47/729 [00:05<00:28, 24.22it/s]  7%|▋         | 54/729 [00:05<00:22, 30.39it/s]  9%|▊         | 62/729 [00:05<00:17, 38.00it/s] 10%|▉         | 70/729 [00:05<00:14, 45.78it/s] 11%|█         | 78/729 [00:05<00:12, 52.64it/s] 12%|█▏        | 86/729 [00:05<00:11, 58.22it/s] 13%|█▎        | 94/729 [00:06<00:10, 62.87it/s] 14%|█▍        | 102/729 [00:07<00:44, 13.94it/s] 15%|█▍        | 109/729 [00:07<00:34, 17.86it/s] 16%|█▌        | 116/729 [00:07<00:27, 22.53it/s] 17%|█▋        | 123/729 [00:07<00:21, 27.66it/s] 18%|█▊        | 130/729 [00:08<00:17, 33.45it/s] 19%|█▉        | 137/729 [00:08<00:15, 39.04it/s] 20%|█▉        | 145/729 [00:08<00:12, 45.89it/s] 21%|██        | 153/729 [00:08<00:10, 52.50it/s] 22%|██▏       | 161/729 [00:08<00:09, 58.38it/s] 23%|██▎       | 169/729 [00:08<00:08, 62.95it/s] 24%|██▍       | 177/729 [00:08<00:08, 66.58it/s] 25%|██▌       | 185/729 [00:08<00:07, 69.48it/s] 26%|██▋       | 193/729 [00:08<00:07, 71.79it/s] 28%|██▊       | 201/729 [00:08<00:07, 73.04it/s] 29%|██▊       | 209/729 [00:09<00:06, 74.63it/s] 30%|██▉       | 217/729 [00:09<00:06, 75.08it/s] 31%|███       | 225/729 [00:09<00:06, 75.92it/s] 32%|███▏      | 233/729 [00:09<00:06, 76.42it/s] 33%|███▎      | 242/729 [00:09<00:06, 77.78it/s] 34%|███▍      | 250/729 [00:09<00:06, 77.95it/s] 35%|███▌      | 258/729 [00:09<00:06, 78.03it/s] 36%|███▋      | 266/729 [00:09<00:05, 78.11it/s] 38%|███▊      | 274/729 [00:09<00:05, 78.02it/s] 39%|███▊      | 282/729 [00:10<00:05, 77.86it/s] 40%|███▉      | 290/729 [00:10<00:05, 77.85it/s] 41%|████      | 298/729 [00:10<00:06, 71.33it/s] 42%|████▏     | 306/729 [00:10<00:05, 72.98it/s] 43%|████▎     | 314/729 [00:10<00:05, 73.94it/s] 44%|████▍     | 322/729 [00:10<00:05, 74.81it/s] 45%|████▌     | 330/729 [00:10<00:05, 75.47it/s] 46%|████▋     | 338/729 [00:10<00:05, 76.07it/s] 47%|████▋     | 346/729 [00:10<00:04, 76.72it/s] 49%|████▊     | 354/729 [00:10<00:04, 76.05it/s] 50%|████▉     | 362/729 [00:11<00:04, 76.29it/s] 51%|█████     | 370/729 [00:11<00:04, 76.34it/s] 52%|█████▏    | 378/729 [00:11<00:04, 76.53it/s] 53%|█████▎    | 386/729 [00:11<00:04, 76.53it/s] 54%|█████▍    | 394/729 [00:11<00:04, 76.72it/s] 55%|█████▌    | 402/729 [00:11<00:04, 77.25it/s] 56%|█████▌    | 410/729 [00:11<00:04, 77.17it/s] 57%|█████▋    | 418/729 [00:11<00:04, 77.32it/s] 58%|█████▊    | 426/729 [00:11<00:03, 76.60it/s] 60%|█████▉    | 434/729 [00:12<00:03, 77.25it/s] 61%|██████    | 442/729 [00:12<00:03, 77.22it/s] 62%|██████▏   | 450/729 [00:12<00:03, 77.30it/s] 63%|██████▎   | 458/729 [00:12<00:03, 77.34it/s] 64%|██████▍   | 466/729 [00:12<00:03, 77.94it/s] 65%|██████▌   | 474/729 [00:12<00:03, 77.98it/s] 66%|██████▌   | 482/729 [00:12<00:03, 77.77it/s] 67%|██████▋   | 490/729 [00:12<00:03, 77.63it/s] 68%|██████▊   | 498/729 [00:12<00:02, 77.86it/s] 69%|██████▉   | 506/729 [00:12<00:02, 76.87it/s] 71%|███████   | 514/729 [00:13<00:02, 77.50it/s] 72%|███████▏  | 522/729 [00:13<00:02, 77.71it/s] 73%|███████▎  | 530/729 [00:13<00:02, 77.67it/s] 74%|███████▍  | 538/729 [00:13<00:02, 77.51it/s] 75%|███████▍  | 546/729 [00:13<00:02, 77.85it/s] 76%|███████▌  | 554/729 [00:13<00:02, 78.13it/s] 77%|███████▋  | 562/729 [00:13<00:02, 78.01it/s] 78%|███████▊  | 570/729 [00:13<00:02, 78.09it/s] 79%|███████▉  | 578/729 [00:13<00:01, 77.80it/s] 80%|████████  | 586/729 [00:13<00:01, 77.62it/s] 81%|████████▏ | 594/729 [00:14<00:01, 78.21it/s] 83%|████████▎ | 602/729 [00:14<00:01, 78.47it/s] 84%|████████▎ | 610/729 [00:14<00:01, 78.88it/s] 85%|████████▍ | 618/729 [00:14<00:01, 78.36it/s] 86%|████████▌ | 626/729 [00:14<00:01, 78.32it/s] 87%|████████▋ | 634/729 [00:14<00:01, 78.38it/s] 88%|████████▊ | 642/729 [00:14<00:01, 78.70it/s] 89%|████████▉ | 650/729 [00:14<00:01, 78.33it/s] 90%|█████████ | 659/729 [00:14<00:00, 79.06it/s] 91%|█████████▏| 667/729 [00:15<00:00, 79.18it/s] 93%|█████████▎| 675/729 [00:15<00:00, 78.71it/s] 94%|█████████▎| 683/729 [00:15<00:00, 78.09it/s] 95%|█████████▍| 691/729 [00:15<00:00, 77.83it/s] 96%|█████████▌| 699/729 [00:15<00:00, 78.24it/s] 97%|█████████▋| 708/729 [00:15<00:00, 79.04it/s] 98%|█████████▊| 716/729 [00:15<00:00, 78.75it/s] 99%|█████████▉| 724/729 [00:15<00:00, 78.27it/s]100%|██████████| 729/729 [00:15<00:00, 46.11it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.embed_tokens.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'lm_head.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:03,  6.22it/s]  9%|▊         | 2/23 [00:00<00:03,  5.33it/s] 13%|█▎        | 3/23 [00:00<00:05,  3.36it/s] 17%|█▋        | 4/23 [00:00<00:04,  4.12it/s] 22%|██▏       | 5/23 [00:01<00:03,  4.92it/s] 26%|██▌       | 6/23 [00:01<00:03,  5.49it/s] 30%|███       | 7/23 [00:01<00:02,  5.86it/s] 35%|███▍      | 8/23 [00:01<00:02,  6.17it/s] 39%|███▉      | 9/23 [00:01<00:02,  5.74it/s] 43%|████▎     | 10/23 [00:01<00:02,  5.65it/s] 48%|████▊     | 11/23 [00:02<00:02,  5.73it/s] 52%|█████▏    | 12/23 [00:02<00:01,  6.08it/s] 57%|█████▋    | 13/23 [00:02<00:01,  5.80it/s] 61%|██████    | 14/23 [00:02<00:01,  6.10it/s] 65%|██████▌   | 15/23 [00:02<00:01,  6.46it/s] 70%|██████▉   | 16/23 [00:02<00:01,  6.37it/s] 74%|███████▍  | 17/23 [00:02<00:00,  6.46it/s] 78%|███████▊  | 18/23 [00:03<00:00,  6.18it/s] 83%|████████▎ | 19/23 [00:03<00:00,  6.19it/s] 87%|████████▋ | 20/23 [00:03<00:00,  6.48it/s] 91%|█████████▏| 21/23 [00:03<00:00,  6.21it/s] 96%|█████████▌| 22/23 [00:03<00:00,  6.27it/s]100%|██████████| 23/23 [00:03<00:00,  6.95it/s]100%|██████████| 23/23 [00:03<00:00,  5.88it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
20it [00:00, 193.91it/s]44it [00:00, 218.97it/s]70it [00:00, 234.79it/s]94it [00:00, 231.18it/s]118it [00:00, 225.92it/s]141it [00:00, 222.32it/s]165it [00:00, 225.77it/s]188it [00:00, 221.22it/s]212it [00:00, 224.21it/s]236it [00:01, 227.83it/s]262it [00:01, 236.27it/s]286it [00:01, 231.68it/s]310it [00:01, 228.04it/s]334it [00:01, 230.88it/s]359it [00:01, 234.96it/s]383it [00:01, 230.24it/s]407it [00:01, 228.98it/s]431it [00:01, 231.89it/s]455it [00:01, 228.87it/s]478it [00:02, 227.90it/s]501it [00:02, 226.64it/s]524it [00:02, 225.20it/s]549it [00:02, 230.94it/s]574it [00:02, 233.73it/s]600it [00:02, 238.79it/s]624it [00:02, 237.88it/s]648it [00:02, 236.18it/s]674it [00:02, 242.65it/s]699it [00:03, 238.98it/s]723it [00:03, 238.57it/s]729it [00:03, 231.01it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 7/729 [00:00<00:10, 68.72it/s]  2%|▏         | 15/729 [00:00<00:10, 71.17it/s]  3%|▎         | 23/729 [00:00<00:09, 71.63it/s]  4%|▍         | 31/729 [00:00<00:09, 72.23it/s]  5%|▌         | 39/729 [00:00<00:09, 72.32it/s]  6%|▋         | 47/729 [00:00<00:09, 73.06it/s]  8%|▊         | 55/729 [00:00<00:09, 70.59it/s]  9%|▊         | 63/729 [00:00<00:09, 70.69it/s] 10%|▉         | 71/729 [00:00<00:09, 70.78it/s] 11%|█         | 79/729 [00:01<00:09, 71.82it/s] 12%|█▏        | 87/729 [00:01<00:08, 72.22it/s] 13%|█▎        | 95/729 [00:01<00:08, 71.13it/s] 14%|█▍        | 103/729 [00:01<00:08, 71.92it/s] 15%|█▌        | 111/729 [00:01<00:08, 72.11it/s] 16%|█▋        | 119/729 [00:01<00:08, 70.66it/s] 17%|█▋        | 127/729 [00:01<00:08, 70.15it/s] 19%|█▊        | 135/729 [00:01<00:08, 70.60it/s] 20%|█▉        | 143/729 [00:02<00:08, 71.48it/s] 21%|██        | 151/729 [00:02<00:08, 71.97it/s] 22%|██▏       | 159/729 [00:02<00:07, 71.90it/s] 23%|██▎       | 167/729 [00:02<00:07, 72.44it/s] 24%|██▍       | 175/729 [00:02<00:07, 71.96it/s] 25%|██▌       | 183/729 [00:02<00:07, 71.58it/s] 26%|██▌       | 191/729 [00:02<00:07, 72.16it/s] 27%|██▋       | 199/729 [00:02<00:07, 72.33it/s] 28%|██▊       | 207/729 [00:02<00:07, 72.99it/s] 29%|██▉       | 215/729 [00:02<00:07, 72.53it/s] 31%|███       | 223/729 [00:03<00:06, 73.18it/s] 32%|███▏      | 231/729 [00:03<00:06, 73.50it/s] 33%|███▎      | 239/729 [00:03<00:06, 73.49it/s] 34%|███▍      | 247/729 [00:03<00:06, 74.11it/s] 35%|███▍      | 255/729 [00:03<00:06, 73.98it/s] 36%|███▌      | 263/729 [00:03<00:06, 73.70it/s] 37%|███▋      | 271/729 [00:03<00:06, 72.91it/s] 38%|███▊      | 279/729 [00:03<00:06, 72.93it/s] 39%|███▉      | 287/729 [00:03<00:06, 72.80it/s] 40%|████      | 295/729 [00:04<00:05, 72.68it/s] 42%|████▏     | 303/729 [00:04<00:05, 71.87it/s] 43%|████▎     | 311/729 [00:04<00:05, 71.91it/s] 44%|████▍     | 319/729 [00:04<00:05, 71.67it/s] 45%|████▍     | 327/729 [00:04<00:05, 72.56it/s] 46%|████▌     | 335/729 [00:04<00:05, 72.39it/s] 47%|████▋     | 343/729 [00:04<00:05, 73.18it/s] 48%|████▊     | 351/729 [00:04<00:05, 73.46it/s] 49%|████▉     | 359/729 [00:04<00:05, 72.69it/s] 50%|█████     | 367/729 [00:05<00:04, 72.85it/s] 51%|█████▏    | 375/729 [00:05<00:04, 72.56it/s] 53%|█████▎    | 383/729 [00:05<00:04, 72.33it/s] 54%|█████▎    | 391/729 [00:05<00:04, 72.15it/s] 55%|█████▍    | 399/729 [00:05<00:04, 72.19it/s] 56%|█████▌    | 407/729 [00:05<00:04, 72.82it/s] 57%|█████▋    | 415/729 [00:05<00:04, 73.23it/s] 58%|█████▊    | 423/729 [00:05<00:04, 72.46it/s] 59%|█████▉    | 431/729 [00:05<00:04, 72.64it/s] 60%|██████    | 439/729 [00:06<00:04, 72.38it/s] 61%|██████▏   | 447/729 [00:06<00:03, 72.45it/s] 62%|██████▏   | 455/729 [00:06<00:03, 72.34it/s] 64%|██████▎   | 463/729 [00:06<00:03, 72.57it/s] 65%|██████▍   | 471/729 [00:06<00:03, 72.81it/s] 66%|██████▌   | 479/729 [00:06<00:03, 72.53it/s] 67%|██████▋   | 487/729 [00:06<00:03, 72.84it/s] 68%|██████▊   | 495/729 [00:06<00:03, 73.06it/s] 69%|██████▉   | 503/729 [00:06<00:03, 72.29it/s] 70%|███████   | 511/729 [00:07<00:03, 72.61it/s] 71%|███████   | 519/729 [00:07<00:02, 72.66it/s] 72%|███████▏  | 527/729 [00:07<00:02, 72.38it/s] 73%|███████▎  | 535/729 [00:07<00:02, 72.47it/s] 74%|███████▍  | 543/729 [00:07<00:02, 72.75it/s] 76%|███████▌  | 551/729 [00:07<00:02, 72.91it/s] 77%|███████▋  | 559/729 [00:07<00:02, 72.94it/s] 78%|███████▊  | 567/729 [00:07<00:02, 73.70it/s] 79%|███████▉  | 575/729 [00:07<00:02, 73.61it/s] 80%|███████▉  | 583/729 [00:08<00:01, 74.07it/s] 81%|████████  | 591/729 [00:08<00:01, 73.88it/s] 82%|████████▏ | 599/729 [00:08<00:01, 74.27it/s] 83%|████████▎ | 607/729 [00:08<00:01, 73.92it/s] 84%|████████▍ | 615/729 [00:08<00:01, 73.64it/s] 85%|████████▌ | 623/729 [00:08<00:01, 73.63it/s] 87%|████████▋ | 631/729 [00:08<00:01, 73.65it/s] 88%|████████▊ | 639/729 [00:08<00:01, 73.04it/s] 89%|████████▉ | 647/729 [00:08<00:01, 70.59it/s] 90%|████████▉ | 655/729 [00:09<00:01, 71.75it/s] 91%|█████████ | 663/729 [00:09<00:00, 73.16it/s] 92%|█████████▏| 671/729 [00:09<00:00, 73.54it/s] 93%|█████████▎| 679/729 [00:09<00:00, 73.32it/s] 94%|█████████▍| 687/729 [00:09<00:00, 73.44it/s] 95%|█████████▌| 695/729 [00:09<00:00, 72.90it/s] 96%|█████████▋| 703/729 [00:09<00:00, 72.95it/s] 98%|█████████▊| 711/729 [00:09<00:00, 72.86it/s] 99%|█████████▊| 719/729 [00:09<00:00, 73.00it/s]100%|█████████▉| 727/729 [00:10<00:00, 72.89it/s]100%|██████████| 729/729 [00:10<00:00, 72.56it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.embed_tokens.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'lm_head.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  3.86it/s]  9%|▊         | 2/23 [00:00<00:05,  3.95it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.17it/s] 17%|█▋        | 4/23 [00:01<00:06,  3.15it/s] 22%|██▏       | 5/23 [00:01<00:05,  3.45it/s] 26%|██▌       | 6/23 [00:01<00:04,  3.60it/s] 30%|███       | 7/23 [00:01<00:04,  3.89it/s] 35%|███▍      | 8/23 [00:02<00:03,  4.00it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.12it/s] 43%|████▎     | 10/23 [00:02<00:03,  4.04it/s] 48%|████▊     | 11/23 [00:02<00:02,  4.17it/s] 52%|█████▏    | 12/23 [00:03<00:02,  4.24it/s] 57%|█████▋    | 13/23 [00:03<00:02,  4.35it/s] 61%|██████    | 14/23 [00:03<00:02,  4.36it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.40it/s] 70%|██████▉   | 16/23 [00:03<00:01,  4.40it/s] 74%|███████▍  | 17/23 [00:04<00:01,  4.40it/s] 78%|███████▊  | 18/23 [00:04<00:01,  4.42it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.28it/s] 87%|████████▋ | 20/23 [00:04<00:00,  4.19it/s] 91%|█████████▏| 21/23 [00:05<00:00,  4.13it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.13it/s]100%|██████████| 23/23 [00:05<00:00,  4.53it/s]100%|██████████| 23/23 [00:05<00:00,  4.14it/s]
{'bleu-1': 0.19414251536688665, 'bleu-2': 0.04524716998956477, 'bleu-3': 0.002651201152999734, 'bleu-4': 0.00023516743035096325}
./predict/train_sen_mse_one_stage_1122-s-119296/test_sen_mse.0
perplexity: 21.291712
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.622016 R=0.624941 F=0.623355
begin predicting
acc : 0.7914951989026063
{'distinct-1': 0.024700537643194482, 'distinct-2': 0.3204912171104006, 'distinct-3': 0.6875496163006086, 'distinct-4': 0.8831271474804283}
{'bleu-1': 0.15186178740510675, 'bleu-2': 0.03380071037250231, 'bleu-3': 0.002942350328580748, 'bleu-4': 0.00015825642746587063}
./predict/train_sen_mse_one_stage_1122-s-119296/test_sen_mse.1
perplexity: 16.431814
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.607916 R=0.640183 F=0.623481
begin predicting
acc : 0.8532235939643347
{'distinct-1': 0.015709774473481364, 'distinct-2': 0.26782327457126315, 'distinct-3': 0.615362651344758, 'distinct-4': 0.8164536596647528}
