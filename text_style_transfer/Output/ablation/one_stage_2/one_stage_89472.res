0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 0.801 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:00,  1.24it/s]31it [00:00, 46.13it/s]66it [00:01, 99.71it/s]98it [00:01, 143.74it/s]131it [00:01, 185.04it/s]164it [00:01, 219.59it/s]195it [00:01, 241.33it/s]228it [00:01, 263.41it/s]262it [00:01, 283.07it/s]295it [00:01, 293.87it/s]328it [00:01, 301.46it/s]363it [00:01, 312.71it/s]396it [00:02, 314.20it/s]430it [00:02, 320.36it/s]463it [00:02, 316.82it/s]496it [00:02, 317.02it/s]532it [00:02, 327.54it/s]567it [00:02, 333.94it/s]601it [00:02, 326.23it/s]634it [00:02, 321.96it/s]667it [00:02, 319.48it/s]700it [00:02, 319.19it/s]729it [00:03, 238.72it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:03<38:31,  3.18s/it]  1%|          | 9/729 [00:03<03:12,  3.74it/s]  2%|▏         | 14/729 [00:03<01:56,  6.16it/s]  3%|▎         | 22/729 [00:03<01:01, 11.55it/s]  4%|▍         | 28/729 [00:05<02:03,  5.69it/s]  5%|▍         | 34/729 [00:06<01:40,  6.89it/s]  6%|▌         | 41/729 [00:06<01:07, 10.13it/s]  7%|▋         | 48/729 [00:06<00:47, 14.25it/s]  8%|▊         | 55/729 [00:06<00:35, 19.19it/s]  9%|▊         | 63/729 [00:06<00:25, 25.67it/s] 10%|▉         | 71/729 [00:06<00:20, 32.45it/s] 11%|█         | 78/729 [00:06<00:16, 38.49it/s] 12%|█▏        | 85/729 [00:06<00:14, 44.23it/s] 13%|█▎        | 93/729 [00:06<00:12, 51.02it/s] 14%|█▍        | 101/729 [00:07<00:10, 57.14it/s] 15%|█▍        | 109/729 [00:07<00:10, 61.40it/s] 16%|█▌        | 118/729 [00:07<00:09, 66.76it/s] 17%|█▋        | 126/729 [00:07<00:08, 69.59it/s] 18%|█▊        | 134/729 [00:07<00:08, 72.03it/s] 19%|█▉        | 142/729 [00:07<00:07, 74.15it/s] 21%|██        | 150/729 [00:07<00:07, 75.64it/s] 22%|██▏       | 158/729 [00:07<00:07, 76.42it/s] 23%|██▎       | 166/729 [00:07<00:07, 77.10it/s] 24%|██▍       | 174/729 [00:07<00:07, 77.54it/s] 25%|██▍       | 182/729 [00:08<00:06, 78.20it/s] 26%|██▌       | 190/729 [00:08<00:06, 78.08it/s] 27%|██▋       | 199/729 [00:08<00:06, 78.35it/s] 28%|██▊       | 207/729 [00:08<00:06, 78.60it/s] 29%|██▉       | 215/729 [00:08<00:06, 77.50it/s] 31%|███       | 223/729 [00:08<00:06, 77.21it/s] 32%|███▏      | 231/729 [00:08<00:06, 77.49it/s] 33%|███▎      | 239/729 [00:08<00:06, 77.91it/s] 34%|███▍      | 247/729 [00:08<00:06, 77.63it/s] 35%|███▌      | 256/729 [00:09<00:06, 78.71it/s] 36%|███▌      | 264/729 [00:09<00:05, 79.06it/s] 37%|███▋      | 272/729 [00:09<00:05, 79.23it/s] 38%|███▊      | 280/729 [00:09<00:05, 79.29it/s] 40%|███▉      | 289/729 [00:09<00:05, 79.22it/s] 41%|████      | 297/729 [00:09<00:05, 79.39it/s] 42%|████▏     | 306/729 [00:09<00:05, 79.54it/s] 43%|████▎     | 314/729 [00:09<00:05, 79.51it/s] 44%|████▍     | 322/729 [00:09<00:05, 79.46it/s] 45%|████▌     | 331/729 [00:09<00:04, 80.09it/s] 47%|████▋     | 340/729 [00:10<00:04, 80.51it/s] 48%|████▊     | 349/729 [00:10<00:04, 80.04it/s] 49%|████▉     | 358/729 [00:11<00:13, 26.95it/s] 50%|█████     | 365/729 [00:11<00:11, 31.78it/s] 51%|█████     | 372/729 [00:11<00:09, 36.97it/s] 52%|█████▏    | 379/729 [00:11<00:08, 42.41it/s] 53%|█████▎    | 386/729 [00:11<00:07, 47.48it/s] 54%|█████▍    | 393/729 [00:11<00:06, 52.28it/s] 55%|█████▍    | 400/729 [00:11<00:05, 56.03it/s] 56%|█████▌    | 408/729 [00:11<00:05, 62.00it/s] 57%|█████▋    | 416/729 [00:11<00:04, 66.39it/s] 58%|█████▊    | 425/729 [00:11<00:04, 70.66it/s] 60%|█████▉    | 434/729 [00:12<00:04, 73.62it/s] 61%|██████    | 443/729 [00:12<00:03, 75.59it/s] 62%|██████▏   | 451/729 [00:12<00:03, 76.31it/s] 63%|██████▎   | 459/729 [00:12<00:03, 77.08it/s] 64%|██████▍   | 467/729 [00:12<00:03, 77.72it/s] 65%|██████▌   | 475/729 [00:12<00:03, 78.35it/s] 66%|██████▋   | 483/729 [00:12<00:03, 78.28it/s] 67%|██████▋   | 491/729 [00:12<00:03, 78.73it/s] 68%|██████▊   | 499/729 [00:12<00:02, 78.85it/s] 70%|██████▉   | 508/729 [00:13<00:02, 79.67it/s] 71%|███████   | 516/729 [00:13<00:02, 79.25it/s] 72%|███████▏  | 525/729 [00:13<00:02, 80.03it/s] 73%|███████▎  | 534/729 [00:13<00:02, 79.53it/s] 74%|███████▍  | 543/729 [00:13<00:02, 79.89it/s] 76%|███████▌  | 551/729 [00:13<00:02, 79.20it/s] 77%|███████▋  | 560/729 [00:13<00:02, 79.72it/s] 78%|███████▊  | 569/729 [00:13<00:01, 80.43it/s] 79%|███████▉  | 578/729 [00:13<00:01, 79.65it/s] 81%|████████  | 587/729 [00:13<00:01, 80.17it/s] 82%|████████▏ | 596/729 [00:14<00:01, 80.41it/s] 83%|████████▎ | 605/729 [00:14<00:01, 80.54it/s] 84%|████████▍ | 614/729 [00:14<00:01, 79.95it/s] 85%|████████▌ | 622/729 [00:14<00:01, 79.78it/s] 87%|████████▋ | 631/729 [00:14<00:01, 79.96it/s] 88%|████████▊ | 639/729 [00:14<00:01, 78.35it/s] 89%|████████▉ | 647/729 [00:14<00:01, 78.58it/s] 90%|████████▉ | 656/729 [00:14<00:00, 79.47it/s] 91%|█████████ | 665/729 [00:14<00:00, 80.31it/s] 92%|█████████▏| 674/729 [00:15<00:00, 79.53it/s] 94%|█████████▎| 682/729 [00:15<00:00, 79.33it/s] 95%|█████████▍| 691/729 [00:15<00:00, 79.91it/s] 96%|█████████▌| 700/729 [00:15<00:00, 79.93it/s] 97%|█████████▋| 709/729 [00:15<00:00, 80.22it/s] 98%|█████████▊| 718/729 [00:15<00:00, 80.63it/s]100%|█████████▉| 727/729 [00:15<00:00, 80.50it/s]100%|██████████| 729/729 [00:15<00:00, 46.22it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'lm_head.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:04,  5.23it/s]  9%|▊         | 2/23 [00:00<00:03,  5.51it/s] 13%|█▎        | 3/23 [00:00<00:03,  5.86it/s] 17%|█▋        | 4/23 [00:00<00:03,  6.19it/s] 22%|██▏       | 5/23 [00:00<00:02,  6.19it/s] 26%|██▌       | 6/23 [00:00<00:02,  6.24it/s] 30%|███       | 7/23 [00:01<00:02,  6.00it/s] 35%|███▍      | 8/23 [00:01<00:02,  6.42it/s] 39%|███▉      | 9/23 [00:01<00:02,  6.42it/s] 43%|████▎     | 10/23 [00:01<00:02,  6.37it/s] 48%|████▊     | 11/23 [00:01<00:01,  6.51it/s] 52%|█████▏    | 12/23 [00:01<00:01,  6.33it/s] 57%|█████▋    | 13/23 [00:02<00:01,  6.13it/s] 61%|██████    | 14/23 [00:02<00:01,  6.40it/s] 65%|██████▌   | 15/23 [00:02<00:01,  6.50it/s] 70%|██████▉   | 16/23 [00:02<00:01,  6.17it/s] 74%|███████▍  | 17/23 [00:02<00:00,  6.41it/s] 78%|███████▊  | 18/23 [00:02<00:00,  6.54it/s] 83%|████████▎ | 19/23 [00:03<00:00,  6.30it/s] 87%|████████▋ | 20/23 [00:03<00:00,  6.30it/s] 91%|█████████▏| 21/23 [00:03<00:00,  6.12it/s] 96%|█████████▌| 22/23 [00:03<00:00,  6.74it/s]100%|██████████| 23/23 [00:03<00:00,  6.44it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
21it [00:00, 204.30it/s]44it [00:00, 217.14it/s]69it [00:00, 231.26it/s]93it [00:00, 223.66it/s]117it [00:00, 227.67it/s]140it [00:00, 225.35it/s]163it [00:00, 224.86it/s]186it [00:00, 221.52it/s]210it [00:00, 223.62it/s]233it [00:01, 223.29it/s]258it [00:01, 229.38it/s]282it [00:01, 229.47it/s]305it [00:01, 229.28it/s]328it [00:01, 224.74it/s]352it [00:01, 228.87it/s]375it [00:01, 226.30it/s]398it [00:01, 224.14it/s]421it [00:01, 224.61it/s]445it [00:01, 226.16it/s]468it [00:02, 221.67it/s]492it [00:02, 224.02it/s]515it [00:02, 222.32it/s]540it [00:02, 230.02it/s]564it [00:02, 230.16it/s]589it [00:02, 235.49it/s]613it [00:02, 229.05it/s]636it [00:02, 226.83it/s]660it [00:02, 226.54it/s]685it [00:03, 231.53it/s]709it [00:03, 229.76it/s]729it [00:03, 226.09it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 8/729 [00:00<00:09, 74.04it/s]  2%|▏         | 16/729 [00:00<00:09, 74.58it/s]  3%|▎         | 24/729 [00:00<00:09, 73.77it/s]  4%|▍         | 32/729 [00:00<00:09, 73.98it/s]  5%|▌         | 40/729 [00:00<00:09, 75.39it/s]  7%|▋         | 48/729 [00:00<00:08, 76.40it/s]  8%|▊         | 56/729 [00:00<00:08, 75.34it/s]  9%|▉         | 64/729 [00:00<00:08, 75.26it/s] 10%|▉         | 72/729 [00:00<00:08, 75.85it/s] 11%|█         | 80/729 [00:01<00:08, 76.14it/s] 12%|█▏        | 88/729 [00:01<00:08, 75.18it/s] 13%|█▎        | 96/729 [00:01<00:08, 74.81it/s] 14%|█▍        | 104/729 [00:01<00:08, 75.44it/s] 15%|█▌        | 112/729 [00:01<00:08, 74.51it/s] 16%|█▋        | 120/729 [00:01<00:08, 73.57it/s] 18%|█▊        | 128/729 [00:01<00:08, 73.88it/s] 19%|█▊        | 136/729 [00:01<00:08, 73.96it/s] 20%|█▉        | 144/729 [00:01<00:08, 73.07it/s] 21%|██        | 152/729 [00:02<00:07, 72.20it/s] 22%|██▏       | 160/729 [00:02<00:07, 73.11it/s] 23%|██▎       | 168/729 [00:02<00:07, 73.69it/s] 24%|██▍       | 176/729 [00:02<00:07, 73.57it/s] 25%|██▌       | 184/729 [00:02<00:07, 74.18it/s] 26%|██▋       | 192/729 [00:02<00:07, 74.62it/s] 27%|██▋       | 200/729 [00:02<00:07, 74.59it/s] 29%|██▊       | 208/729 [00:02<00:06, 75.16it/s] 30%|██▉       | 216/729 [00:02<00:06, 74.48it/s] 31%|███       | 224/729 [00:03<00:06, 75.59it/s] 32%|███▏      | 232/729 [00:03<00:06, 74.79it/s] 33%|███▎      | 240/729 [00:03<00:06, 74.75it/s] 34%|███▍      | 248/729 [00:03<00:06, 76.01it/s] 35%|███▌      | 256/729 [00:03<00:06, 76.21it/s] 36%|███▌      | 264/729 [00:03<00:06, 75.96it/s] 37%|███▋      | 272/729 [00:03<00:06, 75.70it/s] 38%|███▊      | 280/729 [00:03<00:05, 75.61it/s] 40%|███▉      | 288/729 [00:03<00:05, 75.12it/s] 41%|████      | 296/729 [00:03<00:05, 74.71it/s] 42%|████▏     | 304/729 [00:04<00:05, 75.64it/s] 43%|████▎     | 312/729 [00:04<00:05, 74.94it/s] 44%|████▍     | 320/729 [00:04<00:05, 74.88it/s] 45%|████▍     | 328/729 [00:04<00:05, 75.14it/s] 46%|████▌     | 336/729 [00:04<00:05, 75.32it/s] 47%|████▋     | 344/729 [00:04<00:05, 75.33it/s] 48%|████▊     | 352/729 [00:04<00:05, 75.01it/s] 49%|████▉     | 360/729 [00:04<00:04, 75.11it/s] 50%|█████     | 368/729 [00:04<00:04, 74.92it/s] 52%|█████▏    | 376/729 [00:05<00:04, 74.97it/s] 53%|█████▎    | 384/729 [00:05<00:04, 74.83it/s] 54%|█████▍    | 392/729 [00:05<00:04, 74.89it/s] 55%|█████▍    | 400/729 [00:05<00:04, 75.18it/s] 56%|█████▌    | 408/729 [00:05<00:04, 75.38it/s] 57%|█████▋    | 416/729 [00:05<00:04, 74.80it/s] 58%|█████▊    | 424/729 [00:05<00:04, 74.96it/s] 59%|█████▉    | 432/729 [00:05<00:03, 75.14it/s] 60%|██████    | 440/729 [00:05<00:03, 75.19it/s] 61%|██████▏   | 448/729 [00:05<00:03, 74.84it/s] 63%|██████▎   | 456/729 [00:06<00:03, 75.01it/s] 64%|██████▎   | 464/729 [00:06<00:03, 74.85it/s] 65%|██████▍   | 472/729 [00:06<00:03, 74.96it/s] 66%|██████▌   | 480/729 [00:06<00:03, 75.41it/s] 67%|██████▋   | 488/729 [00:06<00:03, 75.23it/s] 68%|██████▊   | 496/729 [00:06<00:03, 75.72it/s] 69%|██████▉   | 504/729 [00:06<00:02, 75.70it/s] 70%|███████   | 512/729 [00:06<00:02, 75.68it/s] 71%|███████▏  | 520/729 [00:06<00:02, 75.18it/s] 72%|███████▏  | 528/729 [00:07<00:02, 75.02it/s] 74%|███████▎  | 536/729 [00:07<00:02, 74.53it/s] 75%|███████▍  | 544/729 [00:07<00:02, 74.91it/s] 76%|███████▌  | 552/729 [00:07<00:02, 75.37it/s] 77%|███████▋  | 560/729 [00:07<00:02, 75.23it/s] 78%|███████▊  | 568/729 [00:07<00:02, 75.90it/s] 79%|███████▉  | 576/729 [00:07<00:02, 76.32it/s] 80%|████████  | 584/729 [00:07<00:01, 76.15it/s] 81%|████████  | 592/729 [00:07<00:01, 76.48it/s] 82%|████████▏ | 600/729 [00:07<00:01, 76.51it/s] 83%|████████▎ | 608/729 [00:08<00:01, 75.99it/s] 84%|████████▍ | 616/729 [00:08<00:01, 76.02it/s] 86%|████████▌ | 624/729 [00:08<00:01, 75.89it/s] 87%|████████▋ | 632/729 [00:08<00:01, 75.31it/s] 88%|████████▊ | 640/729 [00:08<00:01, 75.57it/s] 89%|████████▉ | 648/729 [00:08<00:01, 74.81it/s] 90%|████████▉ | 656/729 [00:08<00:00, 74.53it/s] 91%|█████████ | 664/729 [00:08<00:00, 75.35it/s] 92%|█████████▏| 672/729 [00:08<00:00, 75.53it/s] 93%|█████████▎| 680/729 [00:09<00:00, 75.94it/s] 94%|█████████▍| 688/729 [00:09<00:00, 76.40it/s] 95%|█████████▌| 696/729 [00:09<00:00, 76.23it/s] 97%|█████████▋| 704/729 [00:09<00:00, 75.50it/s] 98%|█████████▊| 712/729 [00:09<00:00, 75.14it/s] 99%|█████████▉| 720/729 [00:09<00:00, 74.63it/s]100%|█████████▉| 728/729 [00:09<00:00, 74.26it/s]100%|██████████| 729/729 [00:09<00:00, 75.07it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'lm_head.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  3.89it/s]  9%|▊         | 2/23 [00:00<00:04,  4.36it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.24it/s] 17%|█▋        | 4/23 [00:00<00:04,  4.35it/s] 22%|██▏       | 5/23 [00:01<00:04,  4.41it/s] 26%|██▌       | 6/23 [00:01<00:03,  4.41it/s] 30%|███       | 7/23 [00:01<00:03,  4.43it/s] 35%|███▍      | 8/23 [00:01<00:03,  4.26it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.37it/s] 43%|████▎     | 10/23 [00:02<00:02,  4.38it/s] 48%|████▊     | 11/23 [00:02<00:02,  4.39it/s] 52%|█████▏    | 12/23 [00:02<00:02,  4.46it/s] 57%|█████▋    | 13/23 [00:02<00:02,  4.36it/s] 61%|██████    | 14/23 [00:03<00:02,  4.37it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.39it/s] 70%|██████▉   | 16/23 [00:03<00:01,  4.40it/s] 74%|███████▍  | 17/23 [00:03<00:01,  4.55it/s] 78%|███████▊  | 18/23 [00:04<00:01,  4.45it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.46it/s] 87%|████████▋ | 20/23 [00:04<00:00,  4.28it/s] 91%|█████████▏| 21/23 [00:04<00:00,  4.17it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.24it/s]100%|██████████| 23/23 [00:05<00:00,  4.57it/s]100%|██████████| 23/23 [00:05<00:00,  4.39it/s]
{'bleu-1': 0.19453010609901686, 'bleu-2': 0.043775698280343896, 'bleu-3': 0.0037932874154237455, 'bleu-4': 4.481065968018075e-05}
./predict/train_sen_mse_one_stage_1122-s-89472/test_sen_mse.0
perplexity: 21.143852
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.629901 R=0.629779 F=0.629719
begin predicting
acc : 0.6625514403292181
{'distinct-1': 0.025455881598960915, 'distinct-2': 0.3308769787698737, 'distinct-3': 0.7040616779731413, 'distinct-4': 0.8970820017589843}
{'bleu-1': 0.14777941700978833, 'bleu-2': 0.02851814824722255, 'bleu-3': 0.0019006697333054184, 'bleu-4': 0.00017338272554564147}
./predict/train_sen_mse_one_stage_1122-s-89472/test_sen_mse.1
perplexity: 16.87039
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.608091 R=0.637923 F=0.622501
begin predicting
acc : 0.8724279835390947
{'distinct-1': 0.015781279130114935, 'distinct-2': 0.27011540756774255, 'distinct-3': 0.6226288672876323, 'distinct-4': 0.8275285822005747}
