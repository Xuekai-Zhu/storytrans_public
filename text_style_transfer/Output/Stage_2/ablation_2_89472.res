0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 1.020 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:01,  1.03s/it]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
28it [00:01, 33.73it/s]55it [00:01, 68.63it/s]90it [00:01, 117.28it/s]119it [00:01, 151.30it/s]146it [00:01, 171.68it/s]175it [00:01, 198.17it/s]204it [00:01, 220.72it/s]233it [00:01, 237.36it/s]263it [00:01, 253.80it/s]293it [00:02, 265.23it/s]322it [00:02, 271.53it/s]355it [00:02, 287.19it/s]390it [00:02, 303.97it/s]429it [00:02, 328.52it/s]465it [00:02, 336.05it/s]503it [00:02, 346.58it/s]542it [00:02, 359.30it/s]582it [00:02, 371.15it/s]620it [00:02, 372.22it/s]660it [00:03, 379.87it/s]699it [00:03, 376.17it/s]729it [00:03, 223.57it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:05<1:06:04,  5.45s/it]  1%|          | 7/729 [00:05<07:02,  1.71it/s]    2%|▏         | 14/729 [00:05<02:54,  4.10it/s]  3%|▎         | 21/729 [00:05<01:37,  7.23it/s]  4%|▍         | 29/729 [00:05<00:59, 11.77it/s]  5%|▍         | 36/729 [00:05<00:41, 16.57it/s]  6%|▌         | 44/729 [00:06<00:29, 22.93it/s]  7%|▋         | 51/729 [00:06<00:23, 28.98it/s]  8%|▊         | 60/729 [00:06<00:17, 38.01it/s]  9%|▉         | 69/729 [00:06<00:14, 46.77it/s] 11%|█         | 78/729 [00:06<00:11, 54.83it/s] 12%|█▏        | 87/729 [00:06<00:10, 61.96it/s] 13%|█▎        | 96/729 [00:06<00:10, 58.52it/s] 14%|█▍        | 104/729 [00:06<00:10, 61.54it/s] 15%|█▌        | 112/729 [00:07<00:09, 64.16it/s] 16%|█▋        | 120/729 [00:07<00:09, 67.55it/s] 18%|█▊        | 129/729 [00:07<00:08, 71.70it/s] 19%|█▉        | 138/729 [00:07<00:07, 76.06it/s] 20%|██        | 147/729 [00:07<00:07, 78.74it/s] 21%|██▏       | 156/729 [00:09<00:37, 15.19it/s] 22%|██▏       | 164/729 [00:09<00:28, 19.60it/s] 24%|██▎       | 173/729 [00:09<00:21, 25.74it/s] 25%|██▍       | 182/729 [00:09<00:16, 32.74it/s] 26%|██▌       | 191/729 [00:09<00:13, 40.07it/s] 27%|██▋       | 200/729 [00:09<00:11, 47.49it/s] 29%|██▊       | 209/729 [00:09<00:09, 54.94it/s] 30%|██▉       | 218/729 [00:09<00:08, 61.28it/s] 31%|███       | 227/729 [00:09<00:07, 66.44it/s] 32%|███▏      | 236/729 [00:10<00:07, 70.42it/s] 34%|███▎      | 245/729 [00:10<00:06, 74.26it/s] 35%|███▍      | 254/729 [00:10<00:06, 77.34it/s] 36%|███▌      | 263/729 [00:10<00:05, 79.72it/s] 37%|███▋      | 272/729 [00:10<00:05, 81.45it/s] 39%|███▊      | 281/729 [00:10<00:05, 81.62it/s] 40%|███▉      | 290/729 [00:10<00:05, 81.35it/s] 41%|████      | 299/729 [00:10<00:05, 81.54it/s] 42%|████▏     | 308/729 [00:10<00:05, 81.23it/s] 43%|████▎     | 317/729 [00:11<00:05, 81.48it/s] 45%|████▍     | 326/729 [00:11<00:04, 81.65it/s] 46%|████▌     | 335/729 [00:11<00:04, 82.20it/s] 47%|████▋     | 344/729 [00:11<00:04, 83.94it/s] 48%|████▊     | 353/729 [00:11<00:04, 84.19it/s] 50%|████▉     | 362/729 [00:11<00:04, 83.91it/s] 51%|█████     | 371/729 [00:11<00:04, 83.37it/s] 52%|█████▏    | 380/729 [00:11<00:04, 83.73it/s] 53%|█████▎    | 389/729 [00:11<00:04, 82.69it/s] 55%|█████▍    | 398/729 [00:12<00:03, 83.20it/s] 56%|█████▌    | 407/729 [00:12<00:03, 83.65it/s] 57%|█████▋    | 416/729 [00:12<00:03, 84.28it/s] 58%|█████▊    | 425/729 [00:12<00:03, 84.47it/s] 60%|█████▉    | 434/729 [00:12<00:03, 84.47it/s] 61%|██████    | 443/729 [00:12<00:03, 83.64it/s] 62%|██████▏   | 452/729 [00:12<00:03, 83.34it/s] 63%|██████▎   | 461/729 [00:12<00:03, 83.13it/s] 64%|██████▍   | 470/729 [00:12<00:03, 84.72it/s] 66%|██████▌   | 479/729 [00:12<00:02, 84.47it/s] 67%|██████▋   | 488/729 [00:13<00:02, 84.90it/s] 68%|██████▊   | 497/729 [00:13<00:02, 84.35it/s] 69%|██████▉   | 506/729 [00:13<00:02, 84.25it/s] 71%|███████   | 515/729 [00:13<00:02, 85.64it/s] 72%|███████▏  | 524/729 [00:13<00:02, 85.63it/s] 73%|███████▎  | 533/729 [00:13<00:02, 85.98it/s] 74%|███████▍  | 542/729 [00:13<00:02, 86.79it/s] 76%|███████▌  | 551/729 [00:13<00:02, 85.75it/s] 77%|███████▋  | 560/729 [00:13<00:01, 85.49it/s] 78%|███████▊  | 569/729 [00:14<00:01, 86.71it/s] 79%|███████▉  | 578/729 [00:14<00:01, 86.72it/s] 81%|████████  | 587/729 [00:14<00:01, 83.75it/s] 82%|████████▏ | 596/729 [00:14<00:01, 84.26it/s] 83%|████████▎ | 605/729 [00:14<00:01, 83.91it/s] 84%|████████▍ | 614/729 [00:14<00:01, 82.89it/s] 85%|████████▌ | 623/729 [00:14<00:01, 82.48it/s] 87%|████████▋ | 632/729 [00:14<00:01, 82.75it/s] 88%|████████▊ | 641/729 [00:14<00:01, 83.01it/s] 89%|████████▉ | 650/729 [00:15<00:00, 82.65it/s] 90%|█████████ | 659/729 [00:15<00:00, 83.48it/s] 92%|█████████▏| 668/729 [00:15<00:00, 82.75it/s] 93%|█████████▎| 677/729 [00:15<00:00, 82.40it/s] 94%|█████████▍| 686/729 [00:15<00:00, 81.89it/s] 95%|█████████▌| 695/729 [00:15<00:00, 81.42it/s] 97%|█████████▋| 704/729 [00:15<00:00, 82.26it/s] 98%|█████████▊| 713/729 [00:15<00:00, 82.71it/s] 99%|█████████▉| 722/729 [00:15<00:00, 83.45it/s]100%|██████████| 729/729 [00:15<00:00, 45.66it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.embed_tokens.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'lm_head.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:03,  6.73it/s] 13%|█▎        | 3/23 [00:00<00:02,  8.14it/s] 17%|█▋        | 4/23 [00:00<00:02,  7.42it/s] 22%|██▏       | 5/23 [00:00<00:02,  7.65it/s] 30%|███       | 7/23 [00:00<00:01,  8.51it/s] 35%|███▍      | 8/23 [00:01<00:01,  7.91it/s] 39%|███▉      | 9/23 [00:01<00:01,  7.86it/s] 48%|████▊     | 11/23 [00:01<00:01,  8.82it/s] 52%|█████▏    | 12/23 [00:01<00:01,  8.65it/s] 57%|█████▋    | 13/23 [00:01<00:01,  8.79it/s] 61%|██████    | 14/23 [00:01<00:01,  8.29it/s] 65%|██████▌   | 15/23 [00:01<00:00,  8.69it/s] 70%|██████▉   | 16/23 [00:01<00:00,  8.09it/s] 78%|███████▊  | 18/23 [00:02<00:00,  8.33it/s] 83%|████████▎ | 19/23 [00:02<00:00,  7.90it/s] 87%|████████▋ | 20/23 [00:02<00:00,  8.29it/s] 91%|█████████▏| 21/23 [00:02<00:00,  8.24it/s] 96%|█████████▌| 22/23 [00:02<00:00,  7.54it/s]100%|██████████| 23/23 [00:02<00:00,  8.20it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
17it [00:00, 161.78it/s]35it [00:00, 172.26it/s]53it [00:00, 147.97it/s]71it [00:00, 157.14it/s]89it [00:00, 164.64it/s]110it [00:00, 178.13it/s]129it [00:00, 177.09it/s]150it [00:00, 185.23it/s]169it [00:00, 184.56it/s]188it [00:01, 182.19it/s]211it [00:01, 194.86it/s]234it [00:01, 204.00it/s]259it [00:01, 215.57it/s]283it [00:01, 222.70it/s]307it [00:01, 224.75it/s]332it [00:01, 229.92it/s]356it [00:01, 227.13it/s]381it [00:01, 232.79it/s]405it [00:02, 228.78it/s]429it [00:02, 230.41it/s]453it [00:02, 228.60it/s]476it [00:02, 228.85it/s]500it [00:02, 229.99it/s]524it [00:02, 225.05it/s]548it [00:02, 227.19it/s]575it [00:02, 236.96it/s]599it [00:02, 233.34it/s]623it [00:02, 231.11it/s]647it [00:03, 231.50it/s]671it [00:03, 232.92it/s]695it [00:03, 234.38it/s]721it [00:03, 239.69it/s]729it [00:03, 214.28it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 8/729 [00:00<00:09, 73.30it/s]  2%|▏         | 16/729 [00:00<00:09, 74.10it/s]  3%|▎         | 24/729 [00:00<00:09, 74.04it/s]  4%|▍         | 32/729 [00:00<00:09, 75.77it/s]  6%|▌         | 41/729 [00:00<00:08, 77.01it/s]  7%|▋         | 49/729 [00:00<00:08, 77.62it/s]  8%|▊         | 57/729 [00:00<00:08, 76.85it/s]  9%|▉         | 65/729 [00:00<00:08, 77.46it/s] 10%|█         | 73/729 [00:00<00:08, 78.10it/s] 11%|█         | 81/729 [00:01<00:08, 77.01it/s] 12%|█▏        | 89/729 [00:01<00:08, 76.97it/s] 13%|█▎        | 97/729 [00:01<00:08, 71.94it/s] 14%|█▍        | 105/729 [00:01<00:09, 68.79it/s] 16%|█▌        | 113/729 [00:01<00:08, 70.33it/s] 17%|█▋        | 121/729 [00:01<00:09, 66.41it/s] 18%|█▊        | 128/729 [00:01<00:10, 58.66it/s] 19%|█▊        | 135/729 [00:01<00:10, 58.50it/s] 19%|█▉        | 142/729 [00:02<00:09, 60.19it/s] 20%|██        | 149/729 [00:02<00:09, 60.97it/s] 21%|██▏       | 156/729 [00:02<00:09, 61.41it/s] 22%|██▏       | 163/729 [00:02<00:09, 62.23it/s] 23%|██▎       | 170/729 [00:02<00:08, 62.64it/s] 24%|██▍       | 178/729 [00:02<00:08, 65.18it/s] 26%|██▌       | 186/729 [00:02<00:07, 68.31it/s] 27%|██▋       | 194/729 [00:02<00:07, 70.70it/s] 28%|██▊       | 202/729 [00:02<00:07, 72.81it/s] 29%|██▉       | 211/729 [00:03<00:06, 74.90it/s] 30%|███       | 219/729 [00:03<00:06, 75.07it/s] 31%|███       | 227/729 [00:03<00:06, 76.43it/s] 32%|███▏      | 235/729 [00:03<00:06, 76.41it/s] 33%|███▎      | 243/729 [00:03<00:06, 77.43it/s] 34%|███▍      | 251/729 [00:03<00:06, 77.54it/s] 36%|███▌      | 259/729 [00:03<00:06, 78.19it/s] 37%|███▋      | 267/729 [00:03<00:05, 78.06it/s] 38%|███▊      | 275/729 [00:03<00:06, 75.40it/s] 39%|███▉      | 283/729 [00:03<00:05, 75.34it/s] 40%|███▉      | 291/729 [00:04<00:05, 75.92it/s] 41%|████      | 299/729 [00:04<00:05, 75.72it/s] 42%|████▏     | 307/729 [00:04<00:05, 76.79it/s] 43%|████▎     | 315/729 [00:04<00:05, 76.69it/s] 44%|████▍     | 323/729 [00:04<00:05, 76.86it/s] 45%|████▌     | 331/729 [00:04<00:05, 76.44it/s] 47%|████▋     | 339/729 [00:04<00:05, 76.27it/s] 48%|████▊     | 347/729 [00:04<00:05, 75.88it/s] 49%|████▊     | 355/729 [00:04<00:04, 75.47it/s] 50%|████▉     | 363/729 [00:05<00:04, 75.77it/s] 51%|█████     | 371/729 [00:05<00:04, 76.71it/s] 52%|█████▏    | 380/729 [00:05<00:04, 78.05it/s] 53%|█████▎    | 388/729 [00:05<00:04, 78.39it/s] 54%|█████▍    | 396/729 [00:05<00:04, 77.92it/s] 55%|█████▌    | 404/729 [00:05<00:04, 78.45it/s] 57%|█████▋    | 412/729 [00:05<00:04, 78.55it/s] 58%|█████▊    | 420/729 [00:05<00:03, 78.61it/s] 59%|█████▊    | 428/729 [00:05<00:03, 78.53it/s] 60%|█████▉    | 436/729 [00:05<00:03, 78.58it/s] 61%|██████    | 445/729 [00:06<00:03, 79.32it/s] 62%|██████▏   | 453/729 [00:06<00:03, 78.48it/s] 63%|██████▎   | 461/729 [00:06<00:03, 78.43it/s] 64%|██████▍   | 469/729 [00:06<00:03, 78.62it/s] 65%|██████▌   | 477/729 [00:06<00:03, 77.68it/s] 67%|██████▋   | 485/729 [00:06<00:03, 77.97it/s] 68%|██████▊   | 493/729 [00:06<00:03, 77.41it/s] 69%|██████▊   | 501/729 [00:06<00:02, 76.90it/s] 70%|██████▉   | 509/729 [00:06<00:02, 77.36it/s] 71%|███████   | 517/729 [00:06<00:02, 77.32it/s] 72%|███████▏  | 525/729 [00:07<00:02, 75.24it/s] 73%|███████▎  | 533/729 [00:07<00:02, 73.99it/s] 74%|███████▍  | 541/729 [00:07<00:02, 62.94it/s] 75%|███████▌  | 548/729 [00:07<00:02, 61.91it/s] 76%|███████▌  | 555/729 [00:07<00:02, 61.76it/s] 77%|███████▋  | 562/729 [00:07<00:02, 61.93it/s] 78%|███████▊  | 569/729 [00:07<00:02, 62.59it/s] 79%|███████▉  | 576/729 [00:07<00:02, 62.73it/s] 80%|███████▉  | 583/729 [00:08<00:02, 55.43it/s] 81%|████████  | 589/729 [00:08<00:02, 54.73it/s] 82%|████████▏ | 595/729 [00:08<00:02, 51.38it/s] 82%|████████▏ | 601/729 [00:08<00:02, 49.73it/s] 84%|████████▎ | 609/729 [00:08<00:02, 56.85it/s] 85%|████████▍ | 618/729 [00:08<00:01, 63.56it/s] 86%|████████▌ | 627/729 [00:08<00:01, 68.90it/s] 87%|████████▋ | 635/729 [00:08<00:01, 70.98it/s] 88%|████████▊ | 644/729 [00:09<00:01, 74.21it/s] 89%|████████▉ | 652/729 [00:09<00:01, 75.26it/s] 91%|█████████ | 661/729 [00:09<00:00, 77.00it/s] 92%|█████████▏| 669/729 [00:09<00:00, 77.83it/s] 93%|█████████▎| 677/729 [00:09<00:00, 77.67it/s] 94%|█████████▍| 685/729 [00:09<00:00, 78.22it/s] 95%|█████████▌| 694/729 [00:09<00:00, 79.15it/s] 96%|█████████▋| 703/729 [00:09<00:00, 79.60it/s] 98%|█████████▊| 711/729 [00:09<00:00, 79.53it/s] 99%|█████████▊| 719/729 [00:09<00:00, 79.54it/s]100%|█████████▉| 727/729 [00:10<00:00, 78.73it/s]100%|██████████| 729/729 [00:10<00:00, 72.23it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.embed_tokens.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'lm_head.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  4.36it/s]  9%|▊         | 2/23 [00:00<00:04,  4.53it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.76it/s] 17%|█▋        | 4/23 [00:00<00:03,  4.84it/s] 22%|██▏       | 5/23 [00:01<00:03,  5.00it/s] 26%|██▌       | 6/23 [00:01<00:03,  4.96it/s] 30%|███       | 7/23 [00:01<00:03,  4.84it/s] 35%|███▍      | 8/23 [00:01<00:03,  4.75it/s] 39%|███▉      | 9/23 [00:01<00:02,  4.81it/s] 43%|████▎     | 10/23 [00:02<00:02,  4.89it/s] 48%|████▊     | 11/23 [00:02<00:02,  4.70it/s] 52%|█████▏    | 12/23 [00:02<00:02,  4.87it/s] 57%|█████▋    | 13/23 [00:02<00:02,  4.54it/s] 61%|██████    | 14/23 [00:02<00:01,  4.63it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.58it/s] 70%|██████▉   | 16/23 [00:03<00:01,  4.68it/s] 74%|███████▍  | 17/23 [00:03<00:01,  3.83it/s] 78%|███████▊  | 18/23 [00:03<00:01,  4.21it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.32it/s] 87%|████████▋ | 20/23 [00:04<00:00,  4.40it/s] 91%|█████████▏| 21/23 [00:04<00:00,  4.57it/s] 96%|█████████▌| 22/23 [00:04<00:00,  4.69it/s]100%|██████████| 23/23 [00:04<00:00,  5.11it/s]100%|██████████| 23/23 [00:04<00:00,  4.67it/s]
{'bleu-1': 0.25380209577330687, 'bleu-2': 0.07970124080421707, 'bleu-3': 0.009423651720450626, 'bleu-4': 0.0003729463780859298}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-89472-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 32.866737
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.663726 R=0.656314 F=0.659850
begin predicting
acc : 0.7544581618655692
{'distinct-1': 0.02927946517002311, 'distinct-2': 0.36367321905633115, 'distinct-3': 0.7518277993086834, 'distinct-4': 0.9319407648219884}
{'bleu-1': 0.22715292222763933, 'bleu-2': 0.06855211667966167, 'bleu-3': 0.008008173961747493, 'bleu-4': 0.00043348081785793207}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-89472-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 25.91017
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.639637 R=0.674509 F=0.656461
begin predicting
acc : 0.9245541838134431
{'distinct-1': 0.016928724346873744, 'distinct-2': 0.29464453457402723, 'distinct-3': 0.671535964936614, 'distinct-4': 0.8680396176385392}
