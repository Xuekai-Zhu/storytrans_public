0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 0.785 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:00,  1.27it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
31it [00:00, 46.94it/s]68it [00:00, 104.48it/s]102it [00:01, 152.24it/s]133it [00:01, 186.92it/s]168it [00:01, 225.76it/s]203it [00:01, 255.23it/s]237it [00:01, 277.14it/s]271it [00:01, 293.38it/s]304it [00:01, 289.62it/s]336it [00:01, 295.19it/s]370it [00:01, 307.53it/s]403it [00:02, 307.49it/s]437it [00:02, 316.57it/s]471it [00:02, 322.76it/s]504it [00:02, 319.13it/s]538it [00:02, 323.12it/s]571it [00:02, 324.40it/s]604it [00:02, 325.73it/s]637it [00:02, 326.06it/s]672it [00:02, 329.63it/s]709it [00:02, 340.71it/s]729it [00:03, 242.36it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:00<12:05,  1.00it/s]  1%|          | 7/729 [00:01<01:27,  8.24it/s]  2%|▏         | 11/729 [00:01<01:16,  9.42it/s]  2%|▏         | 16/729 [00:01<00:48, 14.71it/s]  3%|▎         | 23/729 [00:01<00:30, 23.03it/s]  4%|▍         | 30/729 [00:01<00:22, 31.20it/s]  5%|▌         | 37/729 [00:01<00:17, 38.66it/s]  6%|▌         | 44/729 [00:02<00:15, 45.32it/s]  7%|▋         | 51/729 [00:02<00:13, 50.93it/s]  8%|▊         | 58/729 [00:02<00:12, 55.30it/s]  9%|▉         | 66/729 [00:02<00:10, 60.93it/s] 10%|█         | 73/729 [00:02<00:10, 59.87it/s] 11%|█         | 80/729 [00:02<00:11, 56.08it/s] 12%|█▏        | 86/729 [00:02<00:11, 56.01it/s] 13%|█▎        | 92/729 [00:02<00:11, 55.61it/s] 13%|█▎        | 98/729 [00:02<00:11, 55.64it/s] 14%|█▍        | 104/729 [00:03<00:11, 56.07it/s] 15%|█▌        | 110/729 [00:03<00:11, 56.24it/s] 16%|█▌        | 117/729 [00:03<00:10, 58.49it/s] 17%|█▋        | 124/729 [00:03<00:10, 60.08it/s] 18%|█▊        | 131/729 [00:03<00:09, 61.61it/s] 19%|█▉        | 138/729 [00:03<00:09, 63.09it/s] 20%|█▉        | 145/729 [00:03<00:09, 64.00it/s] 21%|██        | 152/729 [00:03<00:08, 64.44it/s] 22%|██▏       | 159/729 [00:03<00:08, 65.44it/s] 23%|██▎       | 166/729 [00:03<00:08, 65.93it/s] 24%|██▎       | 173/729 [00:04<00:08, 65.19it/s] 25%|██▍       | 180/729 [00:04<00:08, 66.27it/s] 26%|██▌       | 187/729 [00:04<00:08, 66.49it/s] 27%|██▋       | 194/729 [00:04<00:07, 66.89it/s] 28%|██▊       | 201/729 [00:04<00:07, 66.77it/s] 29%|██▊       | 208/729 [00:04<00:07, 67.02it/s] 29%|██▉       | 215/729 [00:04<00:07, 66.42it/s] 30%|███       | 222/729 [00:04<00:07, 66.38it/s] 31%|███▏      | 229/729 [00:04<00:07, 66.86it/s] 32%|███▏      | 236/729 [00:05<00:07, 66.93it/s] 33%|███▎      | 243/729 [00:05<00:07, 66.59it/s] 34%|███▍      | 250/729 [00:05<00:07, 66.52it/s] 35%|███▌      | 257/729 [00:05<00:08, 53.46it/s] 36%|███▌      | 264/729 [00:05<00:08, 56.16it/s] 37%|███▋      | 271/729 [00:05<00:07, 58.24it/s] 38%|███▊      | 278/729 [00:05<00:07, 59.94it/s] 39%|███▉      | 285/729 [00:05<00:07, 61.95it/s] 40%|████      | 292/729 [00:06<00:08, 49.38it/s] 41%|████      | 299/729 [00:06<00:08, 53.25it/s] 42%|████▏     | 306/729 [00:06<00:07, 57.26it/s] 43%|████▎     | 313/729 [00:06<00:06, 59.49it/s] 44%|████▍     | 320/729 [00:06<00:06, 61.88it/s] 45%|████▍     | 327/729 [00:06<00:06, 63.72it/s] 46%|████▌     | 334/729 [00:06<00:06, 64.81it/s] 47%|████▋     | 342/729 [00:06<00:05, 68.53it/s] 48%|████▊     | 350/729 [00:06<00:05, 71.59it/s] 49%|████▉     | 358/729 [00:06<00:05, 73.17it/s] 50%|█████     | 366/729 [00:07<00:04, 74.56it/s] 51%|█████▏    | 375/729 [00:07<00:04, 76.51it/s] 53%|█████▎    | 383/729 [00:07<00:04, 77.36it/s] 54%|█████▍    | 392/729 [00:07<00:04, 78.71it/s] 55%|█████▍    | 400/729 [00:07<00:04, 78.95it/s] 56%|█████▌    | 409/729 [00:07<00:04, 79.96it/s] 57%|█████▋    | 418/729 [00:07<00:03, 80.75it/s] 59%|█████▊    | 427/729 [00:07<00:03, 80.21it/s] 60%|█████▉    | 436/729 [00:07<00:03, 79.78it/s] 61%|██████    | 445/729 [00:08<00:03, 79.87it/s] 62%|██████▏   | 454/729 [00:08<00:03, 80.32it/s] 64%|██████▎   | 463/729 [00:08<00:03, 80.05it/s] 65%|██████▍   | 472/729 [00:08<00:03, 81.46it/s] 66%|██████▌   | 481/729 [00:08<00:03, 81.06it/s] 67%|██████▋   | 490/729 [00:08<00:02, 81.56it/s] 68%|██████▊   | 499/729 [00:08<00:02, 81.23it/s] 70%|██████▉   | 508/729 [00:08<00:02, 81.74it/s] 71%|███████   | 517/729 [00:08<00:02, 80.77it/s] 72%|███████▏  | 526/729 [00:09<00:02, 81.30it/s] 73%|███████▎  | 535/729 [00:09<00:02, 81.60it/s] 75%|███████▍  | 544/729 [00:09<00:02, 80.95it/s] 76%|███████▌  | 553/729 [00:09<00:02, 80.31it/s] 77%|███████▋  | 562/729 [00:09<00:02, 81.32it/s] 78%|███████▊  | 571/729 [00:09<00:01, 82.17it/s] 80%|███████▉  | 580/729 [00:09<00:01, 81.20it/s] 81%|████████  | 589/729 [00:09<00:01, 80.77it/s] 82%|████████▏ | 598/729 [00:09<00:01, 80.91it/s] 83%|████████▎ | 607/729 [00:10<00:01, 81.39it/s] 84%|████████▍ | 616/729 [00:10<00:01, 81.10it/s] 86%|████████▌ | 625/729 [00:10<00:01, 81.50it/s] 87%|████████▋ | 634/729 [00:10<00:01, 82.38it/s] 88%|████████▊ | 643/729 [00:10<00:01, 81.51it/s] 89%|████████▉ | 652/729 [00:10<00:00, 80.91it/s] 91%|█████████ | 661/729 [00:10<00:00, 81.80it/s] 92%|█████████▏| 670/729 [00:10<00:00, 80.95it/s] 93%|█████████▎| 679/729 [00:10<00:00, 79.37it/s] 94%|█████████▍| 688/729 [00:11<00:00, 80.91it/s] 96%|█████████▌| 697/729 [00:11<00:00, 80.63it/s] 97%|█████████▋| 706/729 [00:11<00:00, 80.66it/s] 98%|█████████▊| 715/729 [00:11<00:00, 80.97it/s] 99%|█████████▉| 724/729 [00:11<00:00, 81.65it/s]100%|██████████| 729/729 [00:11<00:00, 62.99it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'lm_head.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:04,  5.12it/s]  9%|▊         | 2/23 [00:00<00:03,  6.72it/s] 13%|█▎        | 3/23 [00:00<00:02,  6.76it/s] 17%|█▋        | 4/23 [00:00<00:03,  6.31it/s] 22%|██▏       | 5/23 [00:00<00:02,  6.68it/s] 26%|██▌       | 6/23 [00:00<00:02,  6.34it/s] 30%|███       | 7/23 [00:01<00:02,  6.91it/s] 35%|███▍      | 8/23 [00:01<00:02,  6.59it/s] 39%|███▉      | 9/23 [00:01<00:02,  6.50it/s] 43%|████▎     | 10/23 [00:01<00:03,  4.22it/s] 48%|████▊     | 11/23 [00:01<00:02,  4.78it/s] 52%|█████▏    | 12/23 [00:02<00:02,  5.41it/s] 57%|█████▋    | 13/23 [00:02<00:01,  5.90it/s] 61%|██████    | 14/23 [00:02<00:01,  6.29it/s] 65%|██████▌   | 15/23 [00:02<00:01,  6.27it/s] 70%|██████▉   | 16/23 [00:02<00:01,  6.49it/s] 74%|███████▍  | 17/23 [00:02<00:00,  7.02it/s] 78%|███████▊  | 18/23 [00:02<00:00,  7.01it/s] 83%|████████▎ | 19/23 [00:03<00:00,  6.57it/s] 87%|████████▋ | 20/23 [00:03<00:00,  6.65it/s] 91%|█████████▏| 21/23 [00:03<00:00,  6.96it/s] 96%|█████████▌| 22/23 [00:03<00:00,  6.81it/s]100%|██████████| 23/23 [00:03<00:00,  7.13it/s]100%|██████████| 23/23 [00:03<00:00,  6.32it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
22it [00:00, 215.80it/s]44it [00:00, 214.56it/s]68it [00:00, 224.70it/s]92it [00:00, 226.76it/s]117it [00:00, 234.66it/s]141it [00:00, 235.52it/s]166it [00:00, 239.19it/s]190it [00:00, 235.86it/s]214it [00:00, 235.79it/s]238it [00:01, 236.00it/s]263it [00:01, 237.54it/s]287it [00:01, 235.59it/s]311it [00:01, 232.77it/s]336it [00:01, 236.87it/s]361it [00:01, 240.59it/s]386it [00:01, 237.71it/s]410it [00:01, 232.85it/s]435it [00:01, 234.39it/s]459it [00:01, 231.81it/s]483it [00:02, 233.91it/s]507it [00:02, 230.98it/s]532it [00:02, 234.52it/s]558it [00:02, 240.78it/s]585it [00:02, 247.30it/s]611it [00:02, 250.72it/s]637it [00:02, 241.48it/s]662it [00:02, 242.40it/s]687it [00:02, 240.33it/s]713it [00:03, 243.75it/s]729it [00:03, 237.04it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 7/729 [00:00<00:11, 61.66it/s]  2%|▏         | 14/729 [00:00<00:12, 59.45it/s]  3%|▎         | 20/729 [00:00<00:11, 59.22it/s]  4%|▎         | 27/729 [00:00<00:11, 60.36it/s]  5%|▍         | 34/729 [00:00<00:11, 61.42it/s]  6%|▌         | 41/729 [00:00<00:11, 62.45it/s]  7%|▋         | 48/729 [00:00<00:10, 63.23it/s]  8%|▊         | 55/729 [00:00<00:10, 62.40it/s]  9%|▊         | 62/729 [00:01<00:10, 61.93it/s]  9%|▉         | 69/729 [00:01<00:10, 62.36it/s] 10%|█         | 76/729 [00:01<00:10, 62.79it/s] 11%|█▏        | 83/729 [00:01<00:10, 61.95it/s] 12%|█▏        | 90/729 [00:01<00:10, 61.96it/s] 13%|█▎        | 97/729 [00:01<00:10, 62.63it/s] 14%|█▍        | 104/729 [00:01<00:10, 62.46it/s] 15%|█▌        | 111/729 [00:01<00:09, 62.95it/s] 16%|█▌        | 118/729 [00:01<00:09, 63.16it/s] 17%|█▋        | 125/729 [00:02<00:09, 62.27it/s] 18%|█▊        | 132/729 [00:02<00:09, 62.50it/s] 19%|█▉        | 139/729 [00:02<00:09, 63.31it/s] 20%|██        | 146/729 [00:02<00:09, 63.55it/s] 21%|██        | 153/729 [00:02<00:09, 63.11it/s] 22%|██▏       | 160/729 [00:02<00:09, 62.53it/s] 23%|██▎       | 167/729 [00:02<00:09, 61.47it/s] 24%|██▍       | 174/729 [00:02<00:08, 61.97it/s] 25%|██▍       | 181/729 [00:02<00:08, 62.07it/s] 26%|██▌       | 188/729 [00:03<00:08, 62.47it/s] 27%|██▋       | 195/729 [00:03<00:08, 62.47it/s] 28%|██▊       | 202/729 [00:03<00:08, 61.10it/s] 29%|██▊       | 209/729 [00:03<00:08, 61.31it/s] 30%|██▉       | 216/729 [00:03<00:08, 61.64it/s] 31%|███       | 223/729 [00:03<00:08, 61.96it/s] 32%|███▏      | 230/729 [00:03<00:08, 61.50it/s] 33%|███▎      | 237/729 [00:03<00:08, 61.12it/s] 33%|███▎      | 244/729 [00:03<00:07, 61.88it/s] 34%|███▍      | 251/729 [00:04<00:07, 61.87it/s] 35%|███▌      | 258/729 [00:04<00:07, 61.97it/s] 36%|███▋      | 265/729 [00:04<00:07, 61.75it/s] 37%|███▋      | 272/729 [00:04<00:07, 62.05it/s] 38%|███▊      | 279/729 [00:04<00:07, 62.32it/s] 39%|███▉      | 286/729 [00:04<00:07, 62.02it/s] 40%|████      | 293/729 [00:04<00:07, 62.22it/s] 41%|████      | 300/729 [00:04<00:07, 60.74it/s] 42%|████▏     | 307/729 [00:04<00:06, 60.83it/s] 43%|████▎     | 314/729 [00:05<00:06, 60.99it/s] 44%|████▍     | 321/729 [00:05<00:06, 61.59it/s] 45%|████▍     | 328/729 [00:05<00:06, 61.57it/s] 46%|████▌     | 335/729 [00:05<00:06, 61.25it/s] 47%|████▋     | 342/729 [00:05<00:06, 60.39it/s] 48%|████▊     | 349/729 [00:05<00:06, 61.02it/s] 49%|████▉     | 356/729 [00:05<00:06, 61.75it/s] 50%|████▉     | 363/729 [00:05<00:05, 61.94it/s] 51%|█████     | 370/729 [00:05<00:05, 61.99it/s] 52%|█████▏    | 377/729 [00:06<00:05, 61.68it/s] 53%|█████▎    | 384/729 [00:06<00:05, 61.46it/s] 54%|█████▎    | 391/729 [00:06<00:05, 61.48it/s] 55%|█████▍    | 398/729 [00:06<00:05, 61.10it/s] 56%|█████▌    | 405/729 [00:06<00:05, 61.07it/s] 57%|█████▋    | 412/729 [00:06<00:05, 61.61it/s] 57%|█████▋    | 419/729 [00:06<00:04, 62.05it/s] 58%|█████▊    | 426/729 [00:06<00:04, 62.09it/s] 59%|█████▉    | 433/729 [00:06<00:04, 62.33it/s] 60%|██████    | 440/729 [00:07<00:04, 62.21it/s] 61%|██████▏   | 447/729 [00:07<00:04, 62.37it/s] 62%|██████▏   | 454/729 [00:07<00:04, 61.81it/s] 63%|██████▎   | 461/729 [00:07<00:04, 61.76it/s] 64%|██████▍   | 468/729 [00:07<00:04, 62.51it/s] 65%|██████▌   | 475/729 [00:07<00:04, 63.40it/s] 66%|██████▌   | 482/729 [00:07<00:03, 63.61it/s] 67%|██████▋   | 489/729 [00:07<00:03, 63.79it/s] 68%|██████▊   | 496/729 [00:07<00:03, 63.12it/s] 69%|██████▉   | 503/729 [00:08<00:03, 61.92it/s] 70%|██████▉   | 510/729 [00:08<00:03, 62.38it/s] 71%|███████   | 517/729 [00:08<00:03, 62.22it/s] 72%|███████▏  | 524/729 [00:08<00:03, 62.10it/s] 73%|███████▎  | 531/729 [00:08<00:03, 62.06it/s] 74%|███████▍  | 538/729 [00:08<00:03, 62.49it/s] 75%|███████▍  | 545/729 [00:08<00:02, 62.72it/s] 76%|███████▌  | 552/729 [00:08<00:02, 62.84it/s] 77%|███████▋  | 559/729 [00:09<00:02, 63.62it/s] 78%|███████▊  | 566/729 [00:09<00:02, 63.80it/s] 79%|███████▊  | 573/729 [00:09<00:02, 63.51it/s] 80%|███████▉  | 580/729 [00:09<00:02, 63.58it/s] 81%|████████  | 587/729 [00:09<00:02, 63.53it/s] 81%|████████▏ | 594/729 [00:09<00:02, 63.65it/s] 82%|████████▏ | 601/729 [00:09<00:02, 63.24it/s] 83%|████████▎ | 608/729 [00:09<00:01, 62.63it/s] 84%|████████▍ | 615/729 [00:09<00:01, 62.55it/s] 85%|████████▌ | 622/729 [00:10<00:01, 62.35it/s] 86%|████████▋ | 629/729 [00:10<00:01, 62.92it/s] 87%|████████▋ | 636/729 [00:10<00:01, 62.36it/s] 88%|████████▊ | 643/729 [00:10<00:01, 61.91it/s] 89%|████████▉ | 650/729 [00:10<00:01, 62.23it/s] 90%|█████████ | 657/729 [00:10<00:01, 62.19it/s] 91%|█████████ | 664/729 [00:10<00:01, 61.26it/s] 92%|█████████▏| 671/729 [00:10<00:00, 61.21it/s] 93%|█████████▎| 678/729 [00:10<00:00, 61.38it/s] 94%|█████████▍| 685/729 [00:11<00:00, 61.67it/s] 95%|█████████▍| 692/729 [00:11<00:00, 61.34it/s] 96%|█████████▌| 699/729 [00:11<00:00, 62.19it/s] 97%|█████████▋| 706/729 [00:11<00:00, 62.08it/s] 98%|█████████▊| 713/729 [00:11<00:00, 61.97it/s] 99%|█████████▉| 720/729 [00:11<00:00, 62.64it/s]100%|█████████▉| 727/729 [00:11<00:00, 63.04it/s]100%|██████████| 729/729 [00:11<00:00, 62.15it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'lm_head.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:07,  3.14it/s]  9%|▊         | 2/23 [00:00<00:06,  3.35it/s] 13%|█▎        | 3/23 [00:00<00:06,  3.33it/s] 17%|█▋        | 4/23 [00:01<00:05,  3.31it/s] 22%|██▏       | 5/23 [00:01<00:04,  3.66it/s] 26%|██▌       | 6/23 [00:01<00:04,  3.93it/s] 30%|███       | 7/23 [00:01<00:03,  4.10it/s] 35%|███▍      | 8/23 [00:02<00:04,  3.73it/s] 39%|███▉      | 9/23 [00:02<00:03,  3.94it/s] 43%|████▎     | 10/23 [00:02<00:04,  3.22it/s] 48%|████▊     | 11/23 [00:03<00:03,  3.23it/s] 52%|█████▏    | 12/23 [00:03<00:03,  3.43it/s] 57%|█████▋    | 13/23 [00:03<00:02,  3.63it/s] 61%|██████    | 14/23 [00:03<00:02,  3.58it/s] 65%|██████▌   | 15/23 [00:04<00:02,  3.57it/s] 70%|██████▉   | 16/23 [00:04<00:01,  3.66it/s] 74%|███████▍  | 17/23 [00:04<00:01,  3.91it/s] 78%|███████▊  | 18/23 [00:04<00:01,  3.98it/s] 83%|████████▎ | 19/23 [00:05<00:01,  3.93it/s] 87%|████████▋ | 20/23 [00:05<00:00,  3.62it/s] 91%|█████████▏| 21/23 [00:05<00:00,  3.32it/s] 96%|█████████▌| 22/23 [00:06<00:00,  3.46it/s]100%|██████████| 23/23 [00:06<00:00,  3.65it/s]100%|██████████| 23/23 [00:06<00:00,  3.60it/s]
{'bleu-1': 0.25346214329435784, 'bleu-2': 0.07551735527038807, 'bleu-3': 0.007863347510933457, 'bleu-4': 0.0003156188719292537}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-46600-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 31.53258
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.655119 R=0.655359 F=0.655075
begin predicting
acc : 0.6927297668038409
{'distinct-1': 0.025022573060728238, 'distinct-2': 0.3529380005399082, 'distinct-3': 0.7586525484370613, 'distinct-4': 0.9416806477496535}
{'bleu-1': 0.1952919384891595, 'bleu-2': 0.05185687191623723, 'bleu-3': 0.0039683851185952, 'bleu-4': 0.00021759501498339805}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-46600-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 26.417488
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.627999 R=0.661575 F=0.644158
begin predicting
acc : 0.9657064471879286
{'distinct-1': 0.016059831789630576, 'distinct-2': 0.28770219792551743, 'distinct-3': 0.6629918133925602, 'distinct-4': 0.8606851830907025}
