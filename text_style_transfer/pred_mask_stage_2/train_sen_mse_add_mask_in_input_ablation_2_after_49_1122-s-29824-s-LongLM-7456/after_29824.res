0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 1.022 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:01,  1.03s/it]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
25it [00:01, 30.13it/s]53it [00:01, 66.76it/s]81it [00:01, 103.56it/s]111it [00:01, 142.12it/s]147it [00:01, 189.89it/s]183it [00:01, 229.04it/s]218it [00:01, 259.32it/s]254it [00:01, 285.00it/s]288it [00:02, 255.28it/s]318it [00:02, 255.50it/s]348it [00:02, 264.29it/s]377it [00:02, 264.77it/s]405it [00:02, 261.16it/s]433it [00:02, 262.65it/s]460it [00:02, 263.87it/s]495it [00:02, 287.95it/s]525it [00:02, 290.68it/s]555it [00:02, 278.15it/s]589it [00:03, 293.19it/s]622it [00:03, 302.90it/s]658it [00:03, 318.99it/s]694it [00:03, 329.51it/s]729it [00:03, 209.69it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:05<1:08:38,  5.66s/it]  1%|          | 7/729 [00:05<07:19,  1.64it/s]    2%|▏         | 13/729 [00:05<03:18,  3.61it/s]  3%|▎         | 20/729 [00:05<01:47,  6.62it/s]  3%|▎         | 25/729 [00:06<01:25,  8.26it/s]  4%|▍         | 31/729 [00:06<00:58, 11.95it/s]  5%|▌         | 38/729 [00:06<00:40, 17.08it/s]  6%|▋         | 46/729 [00:06<00:28, 24.14it/s]  7%|▋         | 54/729 [00:06<00:21, 31.63it/s]  9%|▊         | 62/729 [00:06<00:17, 39.17it/s] 10%|▉         | 70/729 [00:06<00:14, 46.65it/s] 11%|█         | 78/729 [00:07<00:12, 53.43it/s] 12%|█▏        | 86/729 [00:07<00:10, 59.14it/s] 13%|█▎        | 94/729 [00:07<00:10, 63.11it/s] 14%|█▍        | 102/729 [00:07<00:09, 65.97it/s] 15%|█▌        | 110/729 [00:07<00:08, 68.90it/s] 16%|█▌        | 118/729 [00:07<00:08, 70.93it/s] 17%|█▋        | 126/729 [00:07<00:08, 72.12it/s] 18%|█▊        | 134/729 [00:07<00:08, 72.66it/s] 19%|█▉        | 142/729 [00:07<00:07, 74.08it/s] 21%|██        | 150/729 [00:09<00:34, 16.76it/s] 22%|██▏       | 158/729 [00:09<00:26, 21.88it/s] 23%|██▎       | 166/729 [00:09<00:20, 27.90it/s] 24%|██▍       | 174/729 [00:09<00:16, 34.43it/s] 25%|██▍       | 182/729 [00:09<00:13, 41.31it/s] 26%|██▌       | 190/729 [00:09<00:12, 43.51it/s] 27%|██▋       | 197/729 [00:09<00:11, 44.88it/s] 28%|██▊       | 204/729 [00:10<00:11, 47.43it/s] 29%|██▉       | 210/729 [00:10<00:10, 49.37it/s] 30%|██▉       | 216/729 [00:10<00:10, 50.79it/s] 30%|███       | 222/729 [00:10<00:09, 51.61it/s] 31%|███▏      | 228/729 [00:10<00:09, 52.83it/s] 32%|███▏      | 234/729 [00:10<00:16, 29.96it/s] 33%|███▎      | 240/729 [00:11<00:14, 34.28it/s] 34%|███▎      | 246/729 [00:11<00:12, 38.64it/s] 35%|███▍      | 252/729 [00:11<00:11, 41.77it/s] 35%|███▌      | 258/729 [00:11<00:10, 45.11it/s] 36%|███▌      | 264/729 [00:11<00:09, 47.60it/s] 37%|███▋      | 271/729 [00:11<00:08, 51.38it/s] 38%|███▊      | 278/729 [00:11<00:08, 55.40it/s] 39%|███▉      | 285/729 [00:11<00:07, 58.57it/s] 40%|████      | 292/729 [00:11<00:07, 60.35it/s] 41%|████      | 299/729 [00:12<00:06, 62.25it/s] 42%|████▏     | 306/729 [00:12<00:06, 62.26it/s] 43%|████▎     | 313/729 [00:12<00:06, 63.01it/s] 44%|████▍     | 320/729 [00:12<00:06, 64.06it/s] 45%|████▍     | 327/729 [00:12<00:06, 65.24it/s] 46%|████▌     | 334/729 [00:12<00:06, 64.28it/s] 47%|████▋     | 341/729 [00:12<00:05, 64.88it/s] 48%|████▊     | 348/729 [00:12<00:05, 64.92it/s] 49%|████▊     | 355/729 [00:12<00:05, 65.03it/s] 50%|████▉     | 362/729 [00:12<00:05, 65.17it/s] 51%|█████     | 369/729 [00:13<00:05, 66.23it/s] 52%|█████▏    | 376/729 [00:13<00:05, 65.94it/s] 53%|█████▎    | 383/729 [00:13<00:05, 65.42it/s] 53%|█████▎    | 390/729 [00:13<00:05, 64.83it/s] 54%|█████▍    | 397/729 [00:13<00:05, 65.54it/s] 55%|█████▌    | 404/729 [00:13<00:04, 66.03it/s] 56%|█████▋    | 411/729 [00:13<00:04, 66.31it/s] 57%|█████▋    | 418/729 [00:13<00:04, 66.22it/s] 58%|█████▊    | 425/729 [00:13<00:04, 66.82it/s] 59%|█████▉    | 433/729 [00:14<00:04, 67.74it/s] 60%|██████    | 440/729 [00:14<00:04, 67.72it/s] 61%|██████▏   | 447/729 [00:14<00:04, 67.75it/s] 62%|██████▏   | 454/729 [00:14<00:04, 67.72it/s] 63%|██████▎   | 461/729 [00:14<00:03, 67.32it/s] 64%|██████▍   | 468/729 [00:14<00:03, 67.31it/s] 65%|██████▌   | 475/729 [00:14<00:03, 66.54it/s] 66%|██████▌   | 482/729 [00:14<00:03, 66.33it/s] 67%|██████▋   | 489/729 [00:14<00:04, 59.91it/s] 68%|██████▊   | 496/729 [00:15<00:04, 58.17it/s] 69%|██████▉   | 503/729 [00:15<00:03, 59.63it/s] 70%|██████▉   | 510/729 [00:15<00:03, 60.53it/s] 71%|███████   | 517/729 [00:15<00:03, 53.98it/s] 72%|███████▏  | 524/729 [00:15<00:03, 57.25it/s] 73%|███████▎  | 531/729 [00:15<00:03, 60.27it/s] 74%|███████▍  | 538/729 [00:15<00:03, 62.28it/s] 75%|███████▍  | 545/729 [00:15<00:02, 63.95it/s] 76%|███████▌  | 552/729 [00:15<00:02, 65.09it/s] 77%|███████▋  | 559/729 [00:16<00:02, 65.86it/s] 78%|███████▊  | 566/729 [00:16<00:02, 64.52it/s] 79%|███████▊  | 573/729 [00:16<00:02, 65.45it/s] 80%|███████▉  | 580/729 [00:16<00:02, 66.37it/s] 81%|████████  | 588/729 [00:16<00:02, 67.81it/s] 82%|████████▏ | 595/729 [00:16<00:01, 68.42it/s] 83%|████████▎ | 602/729 [00:16<00:01, 68.14it/s] 84%|████████▎ | 609/729 [00:16<00:01, 68.00it/s] 84%|████████▍ | 616/729 [00:16<00:01, 68.16it/s] 85%|████████▌ | 623/729 [00:17<00:01, 68.58it/s] 87%|████████▋ | 631/729 [00:17<00:01, 68.99it/s] 88%|████████▊ | 638/729 [00:17<00:01, 68.91it/s] 88%|████████▊ | 645/729 [00:17<00:01, 68.47it/s] 89%|████████▉ | 652/729 [00:17<00:01, 61.98it/s] 90%|█████████ | 659/729 [00:17<00:01, 59.64it/s] 91%|█████████▏| 666/729 [00:17<00:01, 61.75it/s] 92%|█████████▏| 673/729 [00:17<00:00, 62.58it/s] 93%|█████████▎| 680/729 [00:17<00:00, 64.24it/s] 94%|█████████▍| 688/729 [00:18<00:00, 66.19it/s] 95%|█████████▌| 695/729 [00:18<00:00, 66.93it/s] 96%|█████████▋| 702/729 [00:18<00:00, 67.74it/s] 97%|█████████▋| 710/729 [00:18<00:00, 68.22it/s] 98%|█████████▊| 718/729 [00:18<00:00, 68.92it/s] 99%|█████████▉| 725/729 [00:18<00:00, 69.01it/s]100%|██████████| 729/729 [00:18<00:00, 39.17it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'lm_head.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:03,  6.28it/s]  9%|▊         | 2/23 [00:00<00:03,  6.92it/s] 13%|█▎        | 3/23 [00:00<00:02,  6.84it/s] 17%|█▋        | 4/23 [00:00<00:02,  6.99it/s] 22%|██▏       | 5/23 [00:00<00:02,  6.78it/s] 26%|██▌       | 6/23 [00:00<00:02,  6.29it/s] 30%|███       | 7/23 [00:01<00:02,  6.50it/s] 35%|███▍      | 8/23 [00:01<00:02,  6.54it/s] 39%|███▉      | 9/23 [00:01<00:02,  6.34it/s] 43%|████▎     | 10/23 [00:01<00:01,  6.74it/s] 48%|████▊     | 11/23 [00:01<00:01,  6.96it/s] 52%|█████▏    | 12/23 [00:01<00:01,  6.61it/s] 57%|█████▋    | 13/23 [00:01<00:01,  6.93it/s] 61%|██████    | 14/23 [00:02<00:01,  7.08it/s] 65%|██████▌   | 15/23 [00:02<00:01,  7.11it/s] 70%|██████▉   | 16/23 [00:02<00:01,  6.86it/s] 74%|███████▍  | 17/23 [00:02<00:00,  7.10it/s] 78%|███████▊  | 18/23 [00:02<00:00,  6.38it/s] 83%|████████▎ | 19/23 [00:02<00:00,  6.30it/s] 87%|████████▋ | 20/23 [00:02<00:00,  6.55it/s] 91%|█████████▏| 21/23 [00:03<00:00,  6.69it/s] 96%|█████████▌| 22/23 [00:03<00:00,  6.98it/s]100%|██████████| 23/23 [00:03<00:00,  7.25it/s]100%|██████████| 23/23 [00:03<00:00,  6.79it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
21it [00:00, 204.40it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
44it [00:00, 218.21it/s]69it [00:00, 229.80it/s]92it [00:00, 223.46it/s]117it [00:00, 231.18it/s]141it [00:00, 228.41it/s]166it [00:00, 234.74it/s]190it [00:00, 231.10it/s]215it [00:00, 236.06it/s]242it [00:01, 244.58it/s]268it [00:01, 246.95it/s]294it [00:01, 247.22it/s]319it [00:01, 247.69it/s]348it [00:01, 259.62it/s]375it [00:01, 259.46it/s]401it [00:01, 254.03it/s]427it [00:01, 253.82it/s]454it [00:01, 256.48it/s]480it [00:01, 255.22it/s]506it [00:02, 249.51it/s]532it [00:02, 250.10it/s]558it [00:02, 250.61it/s]586it [00:02, 256.26it/s]612it [00:02, 257.27it/s]638it [00:02, 255.26it/s]665it [00:02, 257.45it/s]691it [00:02, 250.11it/s]717it [00:02, 252.94it/s]729it [00:02, 246.91it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 5/729 [00:00<00:14, 48.63it/s]  1%|▏         | 10/729 [00:00<00:14, 48.93it/s]  2%|▏         | 15/729 [00:00<00:14, 49.32it/s]  3%|▎         | 21/729 [00:00<00:13, 51.23it/s]  4%|▍         | 28/729 [00:00<00:12, 56.19it/s]  5%|▍         | 35/729 [00:00<00:11, 59.40it/s]  6%|▌         | 42/729 [00:00<00:11, 61.24it/s]  7%|▋         | 49/729 [00:00<00:10, 62.78it/s]  8%|▊         | 56/729 [00:00<00:10, 63.27it/s]  9%|▊         | 63/729 [00:01<00:10, 63.99it/s] 10%|▉         | 70/729 [00:01<00:10, 64.87it/s] 11%|█         | 77/729 [00:01<00:10, 64.44it/s] 12%|█▏        | 84/729 [00:01<00:09, 64.77it/s] 12%|█▏        | 91/729 [00:01<00:09, 64.30it/s] 13%|█▎        | 98/729 [00:01<00:09, 64.34it/s] 14%|█▍        | 105/729 [00:01<00:09, 65.26it/s] 15%|█▌        | 112/729 [00:01<00:09, 65.37it/s] 16%|█▋        | 119/729 [00:01<00:09, 64.95it/s] 17%|█▋        | 126/729 [00:02<00:09, 64.66it/s] 18%|█▊        | 133/729 [00:02<00:09, 64.74it/s] 19%|█▉        | 140/729 [00:02<00:09, 65.08it/s] 20%|██        | 147/729 [00:02<00:08, 65.57it/s] 21%|██        | 154/729 [00:02<00:08, 65.41it/s] 22%|██▏       | 161/729 [00:02<00:08, 65.21it/s] 23%|██▎       | 168/729 [00:02<00:08, 65.18it/s] 24%|██▍       | 175/729 [00:02<00:08, 64.38it/s] 25%|██▍       | 182/729 [00:02<00:08, 64.52it/s] 26%|██▌       | 189/729 [00:03<00:08, 64.31it/s] 27%|██▋       | 196/729 [00:03<00:08, 63.17it/s] 28%|██▊       | 203/729 [00:03<00:08, 62.83it/s] 29%|██▉       | 210/729 [00:03<00:08, 63.18it/s] 30%|██▉       | 217/729 [00:03<00:08, 63.11it/s] 31%|███       | 224/729 [00:03<00:07, 63.73it/s] 32%|███▏      | 231/729 [00:03<00:07, 63.86it/s] 33%|███▎      | 238/729 [00:03<00:07, 64.36it/s] 34%|███▎      | 245/729 [00:03<00:07, 64.26it/s] 35%|███▍      | 252/729 [00:03<00:07, 64.72it/s] 36%|███▌      | 259/729 [00:04<00:07, 64.50it/s] 36%|███▋      | 266/729 [00:04<00:07, 64.68it/s] 37%|███▋      | 273/729 [00:04<00:07, 64.07it/s] 38%|███▊      | 280/729 [00:04<00:06, 64.18it/s] 39%|███▉      | 287/729 [00:04<00:06, 63.75it/s] 40%|████      | 294/729 [00:04<00:06, 64.38it/s] 41%|████▏     | 301/729 [00:04<00:06, 64.10it/s] 42%|████▏     | 308/729 [00:04<00:06, 64.56it/s] 43%|████▎     | 315/729 [00:04<00:06, 64.40it/s] 44%|████▍     | 322/729 [00:05<00:06, 64.71it/s] 45%|████▌     | 329/729 [00:05<00:06, 64.93it/s] 46%|████▌     | 336/729 [00:05<00:06, 65.38it/s] 47%|████▋     | 343/729 [00:05<00:05, 64.71it/s] 48%|████▊     | 350/729 [00:05<00:05, 63.95it/s] 49%|████▉     | 357/729 [00:05<00:05, 63.74it/s] 50%|████▉     | 364/729 [00:05<00:05, 63.89it/s] 51%|█████     | 371/729 [00:05<00:05, 64.01it/s] 52%|█████▏    | 378/729 [00:05<00:05, 64.20it/s] 53%|█████▎    | 385/729 [00:06<00:05, 64.16it/s] 54%|█████▍    | 392/729 [00:06<00:05, 64.22it/s] 55%|█████▍    | 399/729 [00:06<00:05, 64.25it/s] 56%|█████▌    | 406/729 [00:06<00:04, 65.08it/s] 57%|█████▋    | 413/729 [00:06<00:04, 65.17it/s] 58%|█████▊    | 420/729 [00:06<00:04, 65.07it/s] 59%|█████▊    | 427/729 [00:06<00:04, 65.16it/s] 60%|█████▉    | 434/729 [00:06<00:04, 65.28it/s] 60%|██████    | 441/729 [00:06<00:04, 65.15it/s] 61%|██████▏   | 448/729 [00:07<00:04, 64.68it/s] 62%|██████▏   | 455/729 [00:07<00:04, 64.09it/s] 63%|██████▎   | 462/729 [00:07<00:04, 63.19it/s] 64%|██████▍   | 469/729 [00:07<00:04, 64.04it/s] 65%|██████▌   | 476/729 [00:07<00:03, 64.38it/s] 66%|██████▋   | 483/729 [00:07<00:03, 64.33it/s] 67%|██████▋   | 490/729 [00:07<00:03, 64.29it/s] 68%|██████▊   | 497/729 [00:07<00:03, 64.36it/s] 69%|██████▉   | 504/729 [00:07<00:03, 64.44it/s] 70%|███████   | 511/729 [00:08<00:03, 64.66it/s] 71%|███████   | 518/729 [00:08<00:03, 64.28it/s] 72%|███████▏  | 525/729 [00:08<00:03, 64.15it/s] 73%|███████▎  | 532/729 [00:08<00:03, 64.22it/s] 74%|███████▍  | 539/729 [00:08<00:02, 64.49it/s] 75%|███████▍  | 546/729 [00:08<00:02, 64.27it/s] 76%|███████▌  | 553/729 [00:08<00:02, 64.69it/s] 77%|███████▋  | 560/729 [00:08<00:02, 64.36it/s] 78%|███████▊  | 567/729 [00:08<00:02, 65.14it/s] 79%|███████▊  | 574/729 [00:08<00:02, 64.97it/s] 80%|███████▉  | 581/729 [00:09<00:02, 65.01it/s] 81%|████████  | 588/729 [00:09<00:02, 64.99it/s] 82%|████████▏ | 595/729 [00:09<00:02, 65.18it/s] 83%|████████▎ | 602/729 [00:09<00:01, 64.48it/s] 84%|████████▎ | 609/729 [00:09<00:01, 64.88it/s] 84%|████████▍ | 616/729 [00:09<00:01, 64.77it/s] 85%|████████▌ | 623/729 [00:09<00:01, 64.77it/s] 86%|████████▋ | 630/729 [00:09<00:01, 64.73it/s] 87%|████████▋ | 637/729 [00:09<00:01, 64.33it/s] 88%|████████▊ | 644/729 [00:10<00:01, 64.64it/s] 89%|████████▉ | 651/729 [00:10<00:01, 64.61it/s] 90%|█████████ | 658/729 [00:10<00:01, 64.84it/s] 91%|█████████ | 665/729 [00:10<00:00, 64.88it/s] 92%|█████████▏| 672/729 [00:10<00:00, 65.26it/s] 93%|█████████▎| 679/729 [00:10<00:00, 64.30it/s] 94%|█████████▍| 686/729 [00:10<00:00, 64.23it/s] 95%|█████████▌| 693/729 [00:10<00:00, 62.45it/s] 96%|█████████▌| 700/729 [00:10<00:00, 62.06it/s] 97%|█████████▋| 707/729 [00:11<00:00, 62.49it/s] 98%|█████████▊| 714/729 [00:11<00:00, 62.94it/s] 99%|█████████▉| 721/729 [00:11<00:00, 62.46it/s]100%|█████████▉| 728/729 [00:11<00:00, 62.55it/s]100%|██████████| 729/729 [00:11<00:00, 63.86it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'lm_head.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:06,  3.53it/s]  9%|▊         | 2/23 [00:00<00:05,  3.78it/s] 13%|█▎        | 3/23 [00:00<00:05,  3.77it/s] 17%|█▋        | 4/23 [00:01<00:04,  4.04it/s] 22%|██▏       | 5/23 [00:01<00:04,  4.25it/s] 26%|██▌       | 6/23 [00:01<00:03,  4.33it/s] 30%|███       | 7/23 [00:01<00:03,  4.41it/s] 35%|███▍      | 8/23 [00:01<00:03,  4.46it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.15it/s] 43%|████▎     | 10/23 [00:02<00:02,  4.35it/s] 48%|████▊     | 11/23 [00:02<00:02,  4.42it/s] 52%|█████▏    | 12/23 [00:02<00:02,  4.50it/s] 57%|█████▋    | 13/23 [00:03<00:02,  4.33it/s] 61%|██████    | 14/23 [00:03<00:02,  4.42it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.45it/s] 70%|██████▉   | 16/23 [00:03<00:01,  4.48it/s] 74%|███████▍  | 17/23 [00:03<00:01,  4.59it/s] 78%|███████▊  | 18/23 [00:04<00:01,  4.61it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.40it/s] 87%|████████▋ | 20/23 [00:04<00:00,  4.53it/s] 91%|█████████▏| 21/23 [00:04<00:00,  4.52it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.52it/s]100%|██████████| 23/23 [00:05<00:00,  4.88it/s]100%|██████████| 23/23 [00:05<00:00,  4.42it/s]
{'bleu-1': 0.27582813016322233, 'bleu-2': 0.08867001593871632, 'bleu-3': 0.011655041942546487, 'bleu-4': 0.0009723467135717454}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-29824-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 31.083302
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.662380 R=0.663577 F=0.662850
begin predicting
acc : 0.7434842249657064
{'distinct-1': 0.02590687361419069, 'distinct-2': 0.3511038108926005, 'distinct-3': 0.7489936383567553, 'distinct-4': 0.9333954397040601}
{'bleu-1': 0.22158566338537727, 'bleu-2': 0.06964014197082687, 'bleu-3': 0.009209052983963894, 'bleu-4': 0.0007148572562449591}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-29824-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 25.789995
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.643084 R=0.677922 F=0.659883
begin predicting
acc : 0.906721536351166
{'distinct-1': 0.016141611082606463, 'distinct-2': 0.2973049528508377, 'distinct-3': 0.6861229722604192, 'distinct-4': 0.8836560066004888}
