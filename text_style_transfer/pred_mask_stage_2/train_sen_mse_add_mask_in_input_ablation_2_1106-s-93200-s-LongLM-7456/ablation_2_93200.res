0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 0.866 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:00,  1.15it/s]30it [00:00, 41.89it/s]65it [00:01, 93.30it/s]96it [00:01, 134.35it/s]129it [00:01, 175.88it/s]162it [00:01, 211.09it/s]194it [00:01, 236.85it/s]226it [00:01, 258.52it/s]259it [00:01, 275.02it/s]291it [00:01, 248.47it/s]319it [00:01, 249.40it/s]348it [00:02, 257.42it/s]376it [00:02, 259.78it/s]404it [00:02, 259.57it/s]432it [00:02, 262.93it/s]459it [00:02, 261.35it/s]488it [00:02, 268.67it/s]516it [00:02, 262.43it/s]543it [00:02, 245.98it/s]569it [00:02, 249.45it/s]596it [00:03, 252.95it/s]622it [00:03, 251.39it/s]655it [00:03, 270.98it/s]685it [00:03, 279.07it/s]719it [00:03, 294.01it/s]729it [00:03, 210.24it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:00<07:19,  1.66it/s]  1%|          | 8/729 [00:00<00:49, 14.55it/s]  2%|▏         | 15/729 [00:00<00:27, 25.97it/s]  3%|▎         | 21/729 [00:01<00:27, 25.88it/s]  4%|▍         | 28/729 [00:01<00:20, 34.33it/s]  5%|▍         | 35/729 [00:01<00:16, 41.88it/s]  6%|▌         | 42/729 [00:01<00:14, 48.56it/s]  7%|▋         | 49/729 [00:01<00:12, 53.60it/s]  8%|▊         | 56/729 [00:01<00:11, 57.65it/s]  9%|▊         | 63/729 [00:01<00:10, 60.80it/s] 10%|▉         | 71/729 [00:01<00:10, 64.02it/s] 11%|█         | 80/729 [00:01<00:09, 69.00it/s] 12%|█▏        | 88/729 [00:02<00:09, 69.85it/s] 13%|█▎        | 96/729 [00:02<00:08, 71.82it/s] 14%|█▍        | 104/729 [00:02<00:13, 47.59it/s] 15%|█▌        | 110/729 [00:02<00:12, 49.75it/s] 16%|█▌        | 118/729 [00:02<00:10, 56.14it/s] 17%|█▋        | 126/729 [00:02<00:09, 61.51it/s] 18%|█▊        | 134/729 [00:02<00:09, 65.99it/s] 19%|█▉        | 142/729 [00:02<00:10, 58.68it/s] 20%|██        | 149/729 [00:03<00:09, 60.82it/s] 22%|██▏       | 157/729 [00:03<00:08, 65.02it/s] 23%|██▎       | 165/729 [00:03<00:08, 68.91it/s] 24%|██▎       | 173/729 [00:03<00:07, 71.92it/s] 25%|██▍       | 182/729 [00:03<00:07, 74.38it/s] 26%|██▌       | 190/729 [00:03<00:07, 75.53it/s] 27%|██▋       | 198/729 [00:03<00:06, 76.31it/s] 28%|██▊       | 206/729 [00:03<00:06, 77.25it/s] 29%|██▉       | 214/729 [00:03<00:06, 77.70it/s] 30%|███       | 222/729 [00:04<00:06, 77.84it/s] 32%|███▏      | 230/729 [00:04<00:06, 78.42it/s] 33%|███▎      | 238/729 [00:04<00:06, 78.45it/s] 34%|███▍      | 247/729 [00:04<00:06, 78.77it/s] 35%|███▍      | 255/729 [00:04<00:06, 78.84it/s] 36%|███▌      | 263/729 [00:04<00:05, 78.75it/s] 37%|███▋      | 271/729 [00:04<00:05, 78.81it/s] 38%|███▊      | 279/729 [00:04<00:05, 78.27it/s] 39%|███▉      | 287/729 [00:04<00:05, 78.06it/s] 40%|████      | 295/729 [00:04<00:05, 77.93it/s] 42%|████▏     | 303/729 [00:05<00:05, 78.38it/s] 43%|████▎     | 311/729 [00:05<00:05, 78.63it/s] 44%|████▍     | 319/729 [00:05<00:05, 77.53it/s] 45%|████▍     | 328/729 [00:05<00:05, 78.44it/s] 46%|████▌     | 336/729 [00:05<00:05, 78.45it/s] 47%|████▋     | 344/729 [00:05<00:04, 78.87it/s] 48%|████▊     | 352/729 [00:05<00:04, 78.57it/s] 49%|████▉     | 360/729 [00:05<00:04, 78.55it/s] 50%|█████     | 368/729 [00:05<00:04, 78.39it/s] 52%|█████▏    | 376/729 [00:05<00:04, 78.43it/s] 53%|█████▎    | 384/729 [00:06<00:04, 78.22it/s] 54%|█████▍    | 392/729 [00:06<00:04, 77.95it/s] 55%|█████▍    | 400/729 [00:06<00:04, 77.68it/s] 56%|█████▌    | 408/729 [00:06<00:04, 78.04it/s] 57%|█████▋    | 416/729 [00:06<00:03, 78.30it/s] 58%|█████▊    | 424/729 [00:06<00:03, 78.33it/s] 59%|█████▉    | 432/729 [00:06<00:03, 78.27it/s] 60%|██████    | 440/729 [00:06<00:03, 78.22it/s] 61%|██████▏   | 448/729 [00:06<00:03, 78.02it/s] 63%|██████▎   | 456/729 [00:07<00:03, 78.23it/s] 64%|██████▎   | 464/729 [00:07<00:03, 78.10it/s] 65%|██████▍   | 472/729 [00:07<00:03, 78.00it/s] 66%|██████▌   | 480/729 [00:07<00:03, 76.56it/s] 67%|██████▋   | 488/729 [00:07<00:03, 76.62it/s] 68%|██████▊   | 496/729 [00:07<00:03, 76.73it/s] 69%|██████▉   | 504/729 [00:07<00:02, 76.57it/s] 70%|███████   | 512/729 [00:07<00:02, 77.19it/s] 71%|███████▏  | 521/729 [00:07<00:02, 77.94it/s] 73%|███████▎  | 529/729 [00:07<00:02, 77.74it/s] 74%|███████▍  | 538/729 [00:08<00:02, 78.58it/s] 75%|███████▍  | 546/729 [00:08<00:02, 78.10it/s] 76%|███████▌  | 554/729 [00:08<00:02, 77.85it/s] 77%|███████▋  | 562/729 [00:08<00:02, 77.77it/s] 78%|███████▊  | 570/729 [00:08<00:02, 77.94it/s] 79%|███████▉  | 578/729 [00:08<00:01, 77.07it/s] 80%|████████  | 586/729 [00:08<00:01, 77.51it/s] 81%|████████▏ | 594/729 [00:08<00:01, 77.38it/s] 83%|████████▎ | 602/729 [00:08<00:01, 77.30it/s] 84%|████████▎ | 610/729 [00:08<00:01, 76.44it/s] 85%|████████▍ | 618/729 [00:09<00:01, 76.12it/s] 86%|████████▌ | 626/729 [00:09<00:01, 76.63it/s] 87%|████████▋ | 634/729 [00:09<00:01, 76.58it/s] 88%|████████▊ | 642/729 [00:09<00:01, 77.00it/s] 89%|████████▉ | 651/729 [00:09<00:01, 77.99it/s] 90%|█████████ | 659/729 [00:09<00:00, 78.46it/s] 91%|█████████▏| 667/729 [00:09<00:00, 78.77it/s] 93%|█████████▎| 675/729 [00:09<00:00, 77.89it/s] 94%|█████████▎| 683/729 [00:09<00:00, 78.05it/s] 95%|█████████▍| 691/729 [00:10<00:00, 78.20it/s] 96%|█████████▌| 699/729 [00:10<00:00, 78.48it/s] 97%|█████████▋| 707/729 [00:10<00:00, 78.75it/s] 98%|█████████▊| 715/729 [00:10<00:00, 78.56it/s] 99%|█████████▉| 723/729 [00:10<00:00, 78.70it/s]100%|██████████| 729/729 [00:10<00:00, 69.33it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'lm_head.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:03,  5.61it/s]  9%|▊         | 2/23 [00:00<00:02,  7.20it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.74it/s] 17%|█▋        | 4/23 [00:00<00:03,  5.38it/s] 22%|██▏       | 5/23 [00:00<00:03,  5.72it/s] 26%|██▌       | 6/23 [00:01<00:02,  6.43it/s] 30%|███       | 7/23 [00:01<00:02,  6.64it/s] 35%|███▍      | 8/23 [00:01<00:02,  6.41it/s] 39%|███▉      | 9/23 [00:01<00:02,  6.61it/s] 43%|████▎     | 10/23 [00:01<00:01,  6.76it/s] 48%|████▊     | 11/23 [00:01<00:01,  7.13it/s] 52%|█████▏    | 12/23 [00:01<00:01,  6.89it/s] 57%|█████▋    | 13/23 [00:02<00:01,  6.59it/s] 61%|██████    | 14/23 [00:02<00:01,  6.64it/s] 65%|██████▌   | 15/23 [00:02<00:01,  6.63it/s] 70%|██████▉   | 16/23 [00:02<00:01,  6.56it/s] 74%|███████▍  | 17/23 [00:02<00:00,  6.90it/s] 78%|███████▊  | 18/23 [00:02<00:00,  6.47it/s] 83%|████████▎ | 19/23 [00:03<00:00,  5.95it/s] 87%|████████▋ | 20/23 [00:03<00:00,  6.41it/s] 91%|█████████▏| 21/23 [00:03<00:00,  6.37it/s] 96%|█████████▌| 22/23 [00:03<00:00,  6.42it/s]100%|██████████| 23/23 [00:03<00:00,  6.51it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
20it [00:00, 191.00it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
43it [00:00, 210.60it/s]66it [00:00, 215.94it/s]88it [00:00, 216.92it/s]111it [00:00, 218.82it/s]133it [00:00, 215.32it/s]156it [00:00, 219.00it/s]178it [00:00, 216.53it/s]200it [00:00, 216.37it/s]222it [00:01, 217.16it/s]246it [00:01, 222.01it/s]269it [00:01, 223.05it/s]292it [00:01, 221.39it/s]315it [00:01, 220.08it/s]338it [00:01, 221.48it/s]362it [00:01, 225.38it/s]385it [00:01, 222.24it/s]408it [00:01, 218.55it/s]431it [00:01, 221.62it/s]454it [00:02, 219.47it/s]476it [00:02, 218.68it/s]498it [00:02, 216.00it/s]520it [00:02, 214.33it/s]543it [00:02, 215.56it/s]567it [00:02, 220.00it/s]590it [00:02, 221.81it/s]613it [00:02, 222.24it/s]636it [00:02, 222.93it/s]660it [00:03, 226.30it/s]683it [00:03, 225.82it/s]706it [00:03, 226.40it/s]729it [00:03, 221.81it/s]729it [00:03, 219.89it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 6/729 [00:00<00:12, 59.81it/s]  2%|▏         | 14/729 [00:00<00:10, 66.46it/s]  3%|▎         | 21/729 [00:00<00:10, 67.36it/s]  4%|▍         | 29/729 [00:00<00:10, 69.57it/s]  5%|▌         | 37/729 [00:00<00:09, 71.07it/s]  6%|▌         | 45/729 [00:00<00:09, 72.03it/s]  7%|▋         | 53/729 [00:00<00:09, 71.42it/s]  8%|▊         | 61/729 [00:00<00:09, 71.71it/s]  9%|▉         | 69/729 [00:00<00:09, 72.11it/s] 11%|█         | 77/729 [00:01<00:09, 72.34it/s] 12%|█▏        | 85/729 [00:01<00:08, 71.58it/s] 13%|█▎        | 93/729 [00:01<00:08, 71.02it/s] 14%|█▍        | 101/729 [00:01<00:08, 71.90it/s] 15%|█▍        | 109/729 [00:01<00:08, 72.26it/s] 16%|█▌        | 117/729 [00:01<00:08, 72.08it/s] 17%|█▋        | 125/729 [00:01<00:08, 71.98it/s] 18%|█▊        | 133/729 [00:01<00:08, 71.26it/s] 19%|█▉        | 141/729 [00:01<00:08, 71.04it/s] 20%|██        | 149/729 [00:02<00:08, 71.50it/s] 22%|██▏       | 157/729 [00:02<00:08, 71.35it/s] 23%|██▎       | 165/729 [00:02<00:07, 71.21it/s] 24%|██▎       | 173/729 [00:02<00:07, 71.74it/s] 25%|██▍       | 181/729 [00:02<00:07, 72.39it/s] 26%|██▌       | 189/729 [00:02<00:08, 63.09it/s] 27%|██▋       | 196/729 [00:02<00:08, 62.79it/s] 28%|██▊       | 203/729 [00:02<00:08, 62.95it/s] 29%|██▉       | 210/729 [00:03<00:08, 62.90it/s] 30%|██▉       | 217/729 [00:03<00:08, 62.40it/s] 31%|███       | 224/729 [00:03<00:08, 62.65it/s] 32%|███▏      | 231/729 [00:03<00:07, 63.45it/s] 33%|███▎      | 239/729 [00:03<00:07, 66.62it/s] 34%|███▍      | 247/729 [00:03<00:07, 68.46it/s] 35%|███▍      | 255/729 [00:03<00:06, 70.05it/s] 36%|███▌      | 263/729 [00:03<00:06, 70.68it/s] 37%|███▋      | 271/729 [00:03<00:06, 70.82it/s] 38%|███▊      | 279/729 [00:04<00:06, 70.96it/s] 39%|███▉      | 287/729 [00:04<00:06, 71.12it/s] 40%|████      | 295/729 [00:04<00:06, 71.86it/s] 42%|████▏     | 303/729 [00:04<00:05, 72.05it/s] 43%|████▎     | 311/729 [00:04<00:05, 71.33it/s] 44%|████▍     | 319/729 [00:04<00:05, 70.62it/s] 45%|████▍     | 327/729 [00:04<00:05, 70.34it/s] 46%|████▌     | 335/729 [00:04<00:05, 69.93it/s] 47%|████▋     | 343/729 [00:04<00:05, 70.77it/s] 48%|████▊     | 351/729 [00:05<00:05, 71.16it/s] 49%|████▉     | 359/729 [00:05<00:05, 71.77it/s] 50%|█████     | 367/729 [00:05<00:05, 72.40it/s] 51%|█████▏    | 375/729 [00:05<00:04, 72.60it/s] 53%|█████▎    | 383/729 [00:05<00:04, 72.55it/s] 54%|█████▎    | 391/729 [00:05<00:04, 71.49it/s] 55%|█████▍    | 399/729 [00:05<00:04, 71.53it/s] 56%|█████▌    | 407/729 [00:05<00:04, 71.41it/s] 57%|█████▋    | 415/729 [00:05<00:04, 70.12it/s] 58%|█████▊    | 423/729 [00:06<00:04, 70.67it/s] 59%|█████▉    | 431/729 [00:06<00:04, 71.05it/s] 60%|██████    | 439/729 [00:06<00:04, 71.14it/s] 61%|██████▏   | 447/729 [00:06<00:03, 71.26it/s] 62%|██████▏   | 455/729 [00:06<00:03, 71.36it/s] 64%|██████▎   | 463/729 [00:06<00:03, 71.79it/s] 65%|██████▍   | 471/729 [00:06<00:03, 72.17it/s] 66%|██████▌   | 479/729 [00:06<00:03, 72.07it/s] 67%|██████▋   | 487/729 [00:06<00:03, 72.31it/s] 68%|██████▊   | 495/729 [00:07<00:03, 72.22it/s] 69%|██████▉   | 503/729 [00:07<00:03, 72.14it/s] 70%|███████   | 511/729 [00:07<00:03, 71.69it/s] 71%|███████   | 519/729 [00:07<00:02, 71.39it/s] 72%|███████▏  | 527/729 [00:07<00:02, 71.30it/s] 73%|███████▎  | 535/729 [00:07<00:02, 71.77it/s] 74%|███████▍  | 543/729 [00:07<00:02, 72.15it/s] 76%|███████▌  | 551/729 [00:07<00:02, 72.44it/s] 77%|███████▋  | 559/729 [00:07<00:02, 72.56it/s] 78%|███████▊  | 567/729 [00:08<00:02, 73.24it/s] 79%|███████▉  | 575/729 [00:08<00:02, 73.41it/s] 80%|███████▉  | 583/729 [00:08<00:01, 73.22it/s] 81%|████████  | 591/729 [00:08<00:01, 73.42it/s] 82%|████████▏ | 599/729 [00:08<00:01, 73.35it/s] 83%|████████▎ | 607/729 [00:08<00:01, 73.15it/s] 84%|████████▍ | 615/729 [00:08<00:01, 73.00it/s] 85%|████████▌ | 623/729 [00:08<00:01, 73.01it/s] 87%|████████▋ | 631/729 [00:08<00:01, 72.96it/s] 88%|████████▊ | 639/729 [00:09<00:01, 72.60it/s] 89%|████████▉ | 647/729 [00:09<00:01, 72.60it/s] 90%|████████▉ | 655/729 [00:09<00:01, 72.77it/s] 91%|█████████ | 663/729 [00:09<00:00, 73.16it/s] 92%|█████████▏| 671/729 [00:09<00:00, 73.22it/s] 93%|█████████▎| 679/729 [00:09<00:00, 72.83it/s] 94%|█████████▍| 687/729 [00:09<00:00, 72.79it/s] 95%|█████████▌| 695/729 [00:09<00:00, 72.39it/s] 96%|█████████▋| 703/729 [00:09<00:00, 67.37it/s] 97%|█████████▋| 710/729 [00:10<00:00, 64.37it/s] 98%|█████████▊| 717/729 [00:10<00:00, 62.87it/s] 99%|█████████▉| 724/729 [00:10<00:00, 60.67it/s]100%|██████████| 729/729 [00:10<00:00, 70.13it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'lm_head.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  3.85it/s]  9%|▊         | 2/23 [00:00<00:04,  4.22it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.26it/s] 17%|█▋        | 4/23 [00:00<00:04,  4.49it/s] 22%|██▏       | 5/23 [00:01<00:04,  4.15it/s] 26%|██▌       | 6/23 [00:01<00:04,  4.02it/s] 30%|███       | 7/23 [00:01<00:03,  4.18it/s] 35%|███▍      | 8/23 [00:01<00:03,  4.35it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.28it/s] 43%|████▎     | 10/23 [00:02<00:03,  4.31it/s] 48%|████▊     | 11/23 [00:02<00:02,  4.34it/s] 52%|█████▏    | 12/23 [00:02<00:02,  4.47it/s] 57%|█████▋    | 13/23 [00:03<00:02,  4.27it/s] 61%|██████    | 14/23 [00:03<00:02,  4.36it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.42it/s] 70%|██████▉   | 16/23 [00:03<00:01,  4.44it/s] 74%|███████▍  | 17/23 [00:03<00:01,  4.54it/s] 78%|███████▊  | 18/23 [00:04<00:01,  4.62it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.56it/s] 87%|████████▋ | 20/23 [00:04<00:00,  4.52it/s] 91%|█████████▏| 21/23 [00:04<00:00,  4.60it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.52it/s]100%|██████████| 23/23 [00:05<00:00,  4.66it/s]100%|██████████| 23/23 [00:05<00:00,  4.41it/s]
{'bleu-1': 0.2626964515557728, 'bleu-2': 0.08202463124051518, 'bleu-3': 0.009420332121632107, 'bleu-4': 0.0004981164039273111}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-93200-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 30.627369
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.656662 R=0.661407 F=0.658890
begin predicting
acc : 0.7832647462277091
{'distinct-1': 0.0250786408102596, 'distinct-2': 0.34752624209693267, 'distinct-3': 0.7451907087468711, 'distinct-4': 0.9329422438319724}
{'bleu-1': 0.20140290603384584, 'bleu-2': 0.058788831061622006, 'bleu-3': 0.004982021463971082, 'bleu-4': 0.00027876642067165957}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-93200-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 25.442047
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.630426 R=0.671794 F=0.650314
begin predicting
acc : 0.9807956104252401
{'distinct-1': 0.015195089915675064, 'distinct-2': 0.28232895910307043, 'distinct-3': 0.6579360200644225, 'distinct-4': 0.8605934378331425}
