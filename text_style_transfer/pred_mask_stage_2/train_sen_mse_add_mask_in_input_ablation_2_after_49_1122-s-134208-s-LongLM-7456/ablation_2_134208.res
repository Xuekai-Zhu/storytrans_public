0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 0.944 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:00,  1.06it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
32it [00:01, 41.42it/s]67it [00:01, 89.75it/s]98it [00:01, 129.72it/s]130it [00:01, 168.59it/s]164it [00:01, 206.34it/s]198it [00:01, 237.36it/s]233it [00:01, 264.90it/s]268it [00:01, 286.14it/s]301it [00:01, 293.58it/s]337it [00:01, 310.96it/s]373it [00:02, 324.23it/s]408it [00:02, 325.46it/s]445it [00:02, 335.34it/s]480it [00:02, 336.98it/s]515it [00:02, 329.82it/s]552it [00:02, 338.66it/s]589it [00:02, 346.96it/s]624it [00:02, 343.64it/s]659it [00:02, 342.55it/s]694it [00:03, 335.86it/s]729it [00:03, 234.35it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:05<1:06:14,  5.46s/it]  0%|          | 2/729 [00:05<28:42,  2.37s/it]    1%|          | 4/729 [00:05<11:33,  1.05it/s]  1%|          | 6/729 [00:06<06:29,  1.86it/s]  2%|▏         | 12/729 [00:06<02:16,  5.26it/s]  3%|▎         | 19/729 [00:06<01:09, 10.21it/s]  4%|▎         | 26/729 [00:06<00:43, 16.05it/s]  5%|▍         | 33/729 [00:06<00:30, 22.56it/s]  5%|▌         | 40/729 [00:06<00:23, 29.40it/s]  6%|▋         | 47/729 [00:06<00:18, 36.17it/s]  8%|▊         | 55/729 [00:06<00:15, 44.52it/s]  9%|▊         | 63/729 [00:06<00:12, 52.26it/s] 10%|▉         | 71/729 [00:06<00:11, 58.87it/s] 11%|█         | 80/729 [00:07<00:09, 65.56it/s] 12%|█▏        | 88/729 [00:07<00:09, 69.11it/s] 13%|█▎        | 96/729 [00:07<00:08, 71.15it/s] 14%|█▍        | 105/729 [00:07<00:08, 73.98it/s] 16%|█▌        | 114/729 [00:07<00:08, 75.96it/s] 17%|█▋        | 122/729 [00:07<00:07, 76.72it/s] 18%|█▊        | 130/729 [00:07<00:07, 77.41it/s] 19%|█▉        | 138/729 [00:07<00:07, 77.43it/s] 20%|██        | 147/729 [00:07<00:07, 78.49it/s] 21%|██▏       | 155/729 [00:08<00:07, 74.23it/s] 22%|██▏       | 163/729 [00:08<00:07, 71.43it/s] 23%|██▎       | 171/729 [00:08<00:18, 30.76it/s] 24%|██▍       | 178/729 [00:08<00:15, 35.86it/s] 25%|██▌       | 185/729 [00:09<00:13, 41.25it/s] 26%|██▋       | 192/729 [00:09<00:11, 46.35it/s] 28%|██▊       | 201/729 [00:09<00:09, 54.55it/s] 29%|██▉       | 210/729 [00:09<00:08, 61.25it/s] 30%|██▉       | 218/729 [00:09<00:07, 65.75it/s] 31%|███       | 226/729 [00:09<00:07, 68.93it/s] 32%|███▏      | 234/729 [00:09<00:06, 71.58it/s] 33%|███▎      | 243/729 [00:09<00:06, 74.54it/s] 35%|███▍      | 252/729 [00:09<00:06, 76.94it/s] 36%|███▌      | 260/729 [00:09<00:06, 77.05it/s] 37%|███▋      | 269/729 [00:10<00:05, 78.39it/s] 38%|███▊      | 278/729 [00:10<00:05, 79.98it/s] 39%|███▉      | 287/729 [00:10<00:05, 79.48it/s] 41%|████      | 296/729 [00:10<00:05, 77.86it/s] 42%|████▏     | 304/729 [00:10<00:05, 77.30it/s] 43%|████▎     | 312/729 [00:10<00:05, 77.62it/s] 44%|████▍     | 321/729 [00:10<00:05, 78.24it/s] 45%|████▌     | 329/729 [00:10<00:05, 78.46it/s] 46%|████▋     | 338/729 [00:10<00:04, 78.87it/s] 48%|████▊     | 347/729 [00:11<00:04, 80.07it/s] 49%|████▉     | 356/729 [00:11<00:04, 79.61it/s] 50%|█████     | 365/729 [00:11<00:04, 79.91it/s] 51%|█████     | 373/729 [00:11<00:04, 79.06it/s] 52%|█████▏    | 381/729 [00:11<00:04, 78.61it/s] 53%|█████▎    | 389/729 [00:11<00:04, 78.11it/s] 55%|█████▍    | 398/729 [00:11<00:04, 78.83it/s] 56%|█████▌    | 407/729 [00:11<00:04, 79.58it/s] 57%|█████▋    | 416/729 [00:11<00:03, 80.31it/s] 58%|█████▊    | 425/729 [00:12<00:03, 80.07it/s] 60%|█████▉    | 434/729 [00:12<00:03, 81.00it/s] 61%|██████    | 443/729 [00:12<00:07, 39.46it/s] 62%|██████▏   | 452/729 [00:12<00:05, 46.57it/s] 63%|██████▎   | 461/729 [00:12<00:05, 53.38it/s] 64%|██████▍   | 470/729 [00:12<00:04, 59.79it/s] 66%|██████▌   | 479/729 [00:13<00:03, 64.76it/s] 67%|██████▋   | 487/729 [00:13<00:05, 41.02it/s] 68%|██████▊   | 495/729 [00:13<00:04, 47.39it/s] 69%|██████▉   | 503/729 [00:13<00:04, 53.31it/s] 70%|███████   | 512/729 [00:13<00:03, 59.72it/s] 71%|███████▏  | 520/729 [00:13<00:03, 64.10it/s] 72%|███████▏  | 528/729 [00:13<00:02, 67.98it/s] 74%|███████▎  | 537/729 [00:14<00:02, 71.66it/s] 75%|███████▍  | 545/729 [00:14<00:02, 73.68it/s] 76%|███████▌  | 553/729 [00:14<00:02, 75.27it/s] 77%|███████▋  | 562/729 [00:14<00:02, 77.48it/s] 78%|███████▊  | 571/729 [00:14<00:01, 79.30it/s] 80%|███████▉  | 580/729 [00:14<00:01, 79.68it/s] 81%|████████  | 589/729 [00:14<00:01, 79.82it/s] 82%|████████▏ | 598/729 [00:14<00:01, 81.03it/s] 83%|████████▎ | 607/729 [00:14<00:01, 80.16it/s] 84%|████████▍ | 616/729 [00:15<00:01, 80.32it/s] 86%|████████▌ | 625/729 [00:15<00:01, 80.96it/s] 87%|████████▋ | 634/729 [00:15<00:01, 80.37it/s] 88%|████████▊ | 643/729 [00:15<00:01, 80.45it/s] 89%|████████▉ | 652/729 [00:15<00:00, 79.68it/s] 91%|█████████ | 660/729 [00:15<00:00, 78.47it/s] 92%|█████████▏| 668/729 [00:15<00:00, 78.73it/s] 93%|█████████▎| 677/729 [00:15<00:00, 79.76it/s] 94%|█████████▍| 686/729 [00:15<00:00, 79.93it/s] 95%|█████████▌| 694/729 [00:16<00:00, 79.54it/s] 96%|█████████▋| 702/729 [00:16<00:00, 79.58it/s] 97%|█████████▋| 710/729 [00:16<00:00, 75.05it/s] 98%|█████████▊| 718/729 [00:16<00:00, 72.63it/s]100%|█████████▉| 726/729 [00:16<00:00, 69.81it/s]100%|██████████| 729/729 [00:16<00:00, 44.00it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'lm_head.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:04,  4.83it/s]  9%|▊         | 2/23 [00:00<00:03,  5.76it/s] 13%|█▎        | 3/23 [00:00<00:03,  5.70it/s] 17%|█▋        | 4/23 [00:00<00:03,  6.14it/s] 22%|██▏       | 5/23 [00:00<00:02,  6.33it/s] 26%|██▌       | 6/23 [00:00<00:02,  6.58it/s] 30%|███       | 7/23 [00:01<00:02,  6.21it/s] 35%|███▍      | 8/23 [00:01<00:02,  6.26it/s] 39%|███▉      | 9/23 [00:01<00:02,  6.58it/s] 43%|████▎     | 10/23 [00:01<00:01,  6.67it/s] 48%|████▊     | 11/23 [00:01<00:01,  6.68it/s] 52%|█████▏    | 12/23 [00:01<00:01,  6.85it/s] 57%|█████▋    | 13/23 [00:02<00:01,  6.55it/s] 61%|██████    | 14/23 [00:02<00:01,  6.54it/s] 65%|██████▌   | 15/23 [00:02<00:01,  6.43it/s] 70%|██████▉   | 16/23 [00:02<00:01,  5.48it/s] 74%|███████▍  | 17/23 [00:02<00:01,  5.72it/s] 78%|███████▊  | 18/23 [00:02<00:00,  6.02it/s] 83%|████████▎ | 19/23 [00:03<00:00,  5.89it/s] 87%|████████▋ | 20/23 [00:03<00:00,  6.20it/s] 91%|█████████▏| 21/23 [00:03<00:00,  6.02it/s] 96%|█████████▌| 22/23 [00:03<00:00,  6.31it/s]100%|██████████| 23/23 [00:03<00:00,  6.97it/s]100%|██████████| 23/23 [00:03<00:00,  6.30it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
19it [00:00, 189.16it/s]44it [00:00, 221.24it/s]70it [00:00, 237.26it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
94it [00:00, 231.42it/s]119it [00:00, 234.22it/s]143it [00:00, 231.72it/s]167it [00:00, 230.42it/s]191it [00:00, 225.15it/s]214it [00:00, 223.88it/s]238it [00:01, 228.12it/s]262it [00:01, 231.19it/s]286it [00:01, 227.03it/s]309it [00:01, 224.06it/s]334it [00:01, 230.40it/s]358it [00:01, 232.76it/s]382it [00:01, 230.35it/s]406it [00:01, 223.86it/s]431it [00:01, 229.83it/s]455it [00:01, 230.35it/s]479it [00:02, 227.46it/s]502it [00:02, 222.22it/s]525it [00:02, 223.44it/s]550it [00:02, 229.83it/s]574it [00:02, 232.28it/s]598it [00:02, 233.89it/s]622it [00:02, 228.53it/s]645it [00:02, 227.94it/s]671it [00:02, 234.50it/s]695it [00:03, 232.53it/s]719it [00:03, 232.92it/s]729it [00:03, 228.88it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 6/729 [00:00<00:12, 59.53it/s]  2%|▏         | 13/729 [00:00<00:12, 59.57it/s]  3%|▎         | 20/729 [00:00<00:11, 63.12it/s]  4%|▍         | 28/729 [00:00<00:10, 67.43it/s]  5%|▍         | 36/729 [00:00<00:09, 70.20it/s]  6%|▌         | 44/729 [00:00<00:09, 70.83it/s]  7%|▋         | 52/729 [00:00<00:09, 71.51it/s]  8%|▊         | 60/729 [00:00<00:09, 69.32it/s]  9%|▉         | 67/729 [00:00<00:09, 67.90it/s] 10%|█         | 74/729 [00:01<00:09, 67.31it/s] 11%|█         | 81/729 [00:01<00:10, 64.51it/s] 12%|█▏        | 88/729 [00:01<00:09, 64.60it/s] 13%|█▎        | 95/729 [00:01<00:10, 62.15it/s] 14%|█▍        | 102/729 [00:01<00:09, 62.91it/s] 15%|█▍        | 109/729 [00:01<00:09, 63.04it/s] 16%|█▌        | 116/729 [00:01<00:09, 63.07it/s] 17%|█▋        | 123/729 [00:01<00:09, 61.70it/s] 18%|█▊        | 130/729 [00:02<00:09, 60.77it/s] 19%|█▉        | 137/729 [00:02<00:09, 60.11it/s] 20%|█▉        | 144/729 [00:02<00:09, 61.15it/s] 21%|██        | 151/729 [00:02<00:09, 63.18it/s] 22%|██▏       | 158/729 [00:02<00:09, 62.01it/s] 23%|██▎       | 165/729 [00:02<00:09, 61.37it/s] 24%|██▎       | 172/729 [00:02<00:08, 62.05it/s] 25%|██▍       | 179/729 [00:02<00:08, 61.33it/s] 26%|██▌       | 186/729 [00:02<00:09, 60.14it/s] 26%|██▋       | 193/729 [00:03<00:08, 60.10it/s] 27%|██▋       | 200/729 [00:03<00:08, 61.51it/s] 28%|██▊       | 207/729 [00:03<00:08, 62.13it/s] 29%|██▉       | 214/729 [00:03<00:08, 61.78it/s] 30%|███       | 221/729 [00:03<00:08, 61.06it/s] 31%|███▏      | 228/729 [00:03<00:08, 61.24it/s] 32%|███▏      | 235/729 [00:03<00:08, 60.59it/s] 33%|███▎      | 242/729 [00:03<00:07, 61.64it/s] 34%|███▍      | 249/729 [00:03<00:08, 57.52it/s] 35%|███▍      | 255/729 [00:04<00:08, 53.65it/s] 36%|███▌      | 261/729 [00:04<00:08, 53.48it/s] 37%|███▋      | 268/729 [00:04<00:08, 55.50it/s] 38%|███▊      | 275/729 [00:04<00:07, 56.79it/s] 39%|███▊      | 281/729 [00:04<00:07, 56.34it/s] 39%|███▉      | 287/729 [00:04<00:07, 56.88it/s] 40%|████      | 294/729 [00:04<00:07, 57.55it/s] 41%|████▏     | 301/729 [00:04<00:07, 58.93it/s] 42%|████▏     | 308/729 [00:05<00:06, 61.37it/s] 43%|████▎     | 315/729 [00:05<00:06, 61.43it/s] 44%|████▍     | 322/729 [00:05<00:06, 61.88it/s] 45%|████▌     | 329/729 [00:05<00:06, 60.96it/s] 46%|████▌     | 336/729 [00:05<00:06, 62.22it/s] 47%|████▋     | 343/729 [00:05<00:06, 63.07it/s] 48%|████▊     | 350/729 [00:05<00:06, 62.27it/s] 49%|████▉     | 357/729 [00:05<00:06, 60.90it/s] 50%|████▉     | 364/729 [00:05<00:05, 61.75it/s] 51%|█████     | 371/729 [00:06<00:05, 62.93it/s] 52%|█████▏    | 378/729 [00:06<00:05, 62.74it/s] 53%|█████▎    | 385/729 [00:06<00:05, 59.55it/s] 54%|█████▍    | 392/729 [00:06<00:05, 60.14it/s] 55%|█████▍    | 399/729 [00:06<00:05, 57.47it/s] 56%|█████▌    | 406/729 [00:06<00:05, 60.48it/s] 57%|█████▋    | 413/729 [00:06<00:05, 60.88it/s] 58%|█████▊    | 420/729 [00:06<00:05, 61.71it/s] 59%|█████▊    | 428/729 [00:06<00:04, 64.12it/s] 60%|█████▉    | 436/729 [00:07<00:04, 65.82it/s] 61%|██████    | 443/729 [00:07<00:04, 65.00it/s] 62%|██████▏   | 450/729 [00:07<00:04, 63.73it/s] 63%|██████▎   | 457/729 [00:07<00:04, 61.97it/s] 64%|██████▎   | 464/729 [00:07<00:04, 59.57it/s] 65%|██████▍   | 471/729 [00:07<00:04, 61.57it/s] 66%|██████▌   | 478/729 [00:07<00:04, 61.30it/s] 67%|██████▋   | 485/729 [00:07<00:04, 60.87it/s] 67%|██████▋   | 492/729 [00:07<00:03, 60.09it/s] 68%|██████▊   | 499/729 [00:08<00:03, 59.52it/s] 69%|██████▉   | 506/729 [00:08<00:03, 59.68it/s] 70%|███████   | 513/729 [00:08<00:03, 60.12it/s] 71%|███████▏  | 520/729 [00:08<00:03, 61.58it/s] 72%|███████▏  | 527/729 [00:08<00:03, 61.05it/s] 73%|███████▎  | 534/729 [00:08<00:03, 61.84it/s] 74%|███████▍  | 541/729 [00:08<00:02, 63.33it/s] 75%|███████▌  | 548/729 [00:08<00:02, 62.46it/s] 76%|███████▌  | 555/729 [00:09<00:02, 61.80it/s] 77%|███████▋  | 562/729 [00:09<00:02, 61.56it/s] 78%|███████▊  | 569/729 [00:09<00:02, 62.33it/s] 79%|███████▉  | 576/729 [00:09<00:02, 61.65it/s] 80%|███████▉  | 583/729 [00:09<00:02, 62.73it/s] 81%|████████  | 590/729 [00:09<00:02, 62.34it/s] 82%|████████▏ | 597/729 [00:09<00:02, 64.06it/s] 83%|████████▎ | 604/729 [00:09<00:02, 61.71it/s] 84%|████████▍ | 611/729 [00:09<00:01, 62.35it/s] 85%|████████▍ | 619/729 [00:10<00:01, 65.08it/s] 86%|████████▌ | 627/729 [00:10<00:01, 68.12it/s] 87%|████████▋ | 635/729 [00:10<00:01, 69.84it/s] 88%|████████▊ | 643/729 [00:10<00:01, 70.40it/s] 89%|████████▉ | 651/729 [00:10<00:01, 71.20it/s] 90%|█████████ | 659/729 [00:10<00:00, 72.29it/s] 91%|█████████▏| 667/729 [00:10<00:00, 73.55it/s] 93%|█████████▎| 675/729 [00:10<00:00, 73.41it/s] 94%|█████████▎| 683/729 [00:10<00:00, 73.46it/s] 95%|█████████▍| 691/729 [00:10<00:00, 73.98it/s] 96%|█████████▌| 699/729 [00:11<00:00, 73.62it/s] 97%|█████████▋| 707/729 [00:11<00:00, 73.35it/s] 98%|█████████▊| 715/729 [00:11<00:00, 73.39it/s] 99%|█████████▉| 723/729 [00:11<00:00, 71.97it/s]100%|██████████| 729/729 [00:11<00:00, 63.29it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'lm_head.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  4.04it/s]  9%|▊         | 2/23 [00:00<00:05,  3.94it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.03it/s] 17%|█▋        | 4/23 [00:01<00:04,  3.98it/s] 22%|██▏       | 5/23 [00:01<00:04,  4.02it/s] 26%|██▌       | 6/23 [00:01<00:04,  4.13it/s] 30%|███       | 7/23 [00:01<00:03,  4.22it/s] 35%|███▍      | 8/23 [00:01<00:03,  4.20it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.01it/s] 43%|████▎     | 10/23 [00:02<00:03,  3.89it/s] 48%|████▊     | 11/23 [00:02<00:03,  3.95it/s] 52%|█████▏    | 12/23 [00:02<00:02,  3.92it/s] 57%|█████▋    | 13/23 [00:03<00:02,  3.90it/s] 61%|██████    | 14/23 [00:03<00:02,  4.04it/s] 65%|██████▌   | 15/23 [00:03<00:02,  3.25it/s] 70%|██████▉   | 16/23 [00:04<00:01,  3.58it/s] 74%|███████▍  | 17/23 [00:04<00:01,  3.85it/s] 78%|███████▊  | 18/23 [00:04<00:01,  3.99it/s] 83%|████████▎ | 19/23 [00:04<00:01,  3.98it/s] 87%|████████▋ | 20/23 [00:05<00:00,  4.01it/s] 91%|█████████▏| 21/23 [00:05<00:00,  4.05it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.04it/s]100%|██████████| 23/23 [00:05<00:00,  3.94it/s]100%|██████████| 23/23 [00:05<00:00,  3.94it/s]
{'bleu-1': 0.2935736510800327, 'bleu-2': 0.10212146831204635, 'bleu-3': 0.016091811687057624, 'bleu-4': 0.000796089177522558}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-134208-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 30.907408
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.672302 R=0.670602 F=0.671300
begin predicting
acc : 0.6323731138545954
{'distinct-1': 0.02512286490016153, 'distinct-2': 0.34369136859215454, 'distinct-3': 0.7415296267293134, 'distinct-4': 0.9307886971217415}
{'bleu-1': 0.21772626254246308, 'bleu-2': 0.07004013436244322, 'bleu-3': 0.00885082170570249, 'bleu-4': 0.0008601580422254995}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-134208-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 25.214365
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.638256 R=0.679910 F=0.658288
begin predicting
acc : 0.9327846364883402
{'distinct-1': 0.015073924375051185, 'distinct-2': 0.2835512066831683, 'distinct-3': 0.6606377869095037, 'distinct-4': 0.8638151808754078}
