0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 0.784 seconds.
Prefix dict has been built successfully.
1it [00:00,  1.27it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
34it [00:00, 51.67it/s]73it [00:00, 112.17it/s]107it [00:01, 158.31it/s]143it [00:01, 203.04it/s]178it [00:01, 238.64it/s]212it [00:01, 260.56it/s]248it [00:01, 286.56it/s]283it [00:01, 303.13it/s]318it [00:01, 310.53it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
356it [00:01, 326.45it/s]391it [00:01, 328.83it/s]426it [00:02, 332.88it/s]461it [00:02, 333.86it/s]496it [00:02, 335.10it/s]530it [00:02, 334.75it/s]567it [00:02, 344.09it/s]604it [00:02, 349.48it/s]640it [00:02, 343.22it/s]675it [00:02, 341.77it/s]710it [00:02, 336.13it/s]729it [00:02, 250.50it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:06<1:21:34,  6.72s/it]  0%|          | 2/729 [00:07<40:05,  3.31s/it]    1%|          | 6/729 [00:07<09:23,  1.28it/s]  2%|▏         | 11/729 [00:07<04:14,  2.83it/s]  2%|▏         | 16/729 [00:08<02:25,  4.91it/s]  3%|▎         | 23/729 [00:08<01:20,  8.72it/s]  4%|▍         | 30/729 [00:08<00:52, 13.38it/s]  5%|▌         | 37/729 [00:08<00:36, 18.85it/s]  6%|▌         | 44/729 [00:08<00:27, 24.83it/s]  7%|▋         | 51/729 [00:08<00:21, 31.19it/s]  8%|▊         | 58/729 [00:08<00:17, 37.40it/s]  9%|▉         | 65/729 [00:08<00:15, 42.90it/s] 10%|▉         | 72/729 [00:10<00:44, 14.71it/s] 11%|█         | 79/729 [00:10<00:33, 19.24it/s] 12%|█▏        | 86/729 [00:10<00:26, 24.53it/s] 13%|█▎        | 93/729 [00:10<00:21, 30.11it/s] 14%|█▎        | 100/729 [00:10<00:17, 36.00it/s] 15%|█▍        | 107/729 [00:10<00:14, 41.53it/s] 16%|█▌        | 114/729 [00:10<00:13, 46.99it/s] 17%|█▋        | 122/729 [00:10<00:11, 54.29it/s] 18%|█▊        | 131/729 [00:10<00:09, 61.39it/s] 19%|█▉        | 140/729 [00:10<00:08, 67.09it/s] 20%|██        | 149/729 [00:11<00:08, 71.73it/s] 22%|██▏       | 158/729 [00:11<00:07, 74.35it/s] 23%|██▎       | 166/729 [00:11<00:12, 43.50it/s] 24%|██▎       | 173/729 [00:11<00:12, 44.14it/s] 25%|██▍       | 180/729 [00:11<00:11, 48.08it/s] 26%|██▌       | 187/729 [00:11<00:10, 51.44it/s] 27%|██▋       | 194/729 [00:12<00:09, 54.73it/s] 28%|██▊       | 201/729 [00:12<00:09, 57.14it/s] 29%|██▊       | 208/729 [00:12<00:08, 59.49it/s] 29%|██▉       | 215/729 [00:12<00:08, 61.89it/s] 31%|███       | 224/729 [00:12<00:07, 68.24it/s] 32%|███▏      | 233/729 [00:12<00:06, 72.52it/s] 33%|███▎      | 242/729 [00:12<00:06, 76.05it/s] 34%|███▍      | 251/729 [00:12<00:06, 78.24it/s] 36%|███▌      | 260/729 [00:12<00:05, 79.92it/s] 37%|███▋      | 269/729 [00:13<00:05, 81.03it/s] 38%|███▊      | 278/729 [00:13<00:05, 81.95it/s] 39%|███▉      | 287/729 [00:13<00:05, 81.89it/s] 41%|████      | 296/729 [00:13<00:05, 82.65it/s] 42%|████▏     | 305/729 [00:13<00:05, 82.60it/s] 43%|████▎     | 314/729 [00:13<00:05, 82.70it/s] 44%|████▍     | 323/729 [00:13<00:04, 82.58it/s] 46%|████▌     | 332/729 [00:13<00:04, 83.32it/s] 47%|████▋     | 341/729 [00:13<00:04, 83.48it/s] 48%|████▊     | 350/729 [00:14<00:04, 82.40it/s] 49%|████▉     | 359/729 [00:14<00:04, 80.87it/s] 50%|█████     | 368/729 [00:14<00:04, 80.64it/s] 52%|█████▏    | 377/729 [00:14<00:04, 81.47it/s] 53%|█████▎    | 386/729 [00:14<00:04, 81.75it/s] 54%|█████▍    | 395/729 [00:14<00:04, 81.68it/s] 55%|█████▌    | 404/729 [00:14<00:03, 81.58it/s] 57%|█████▋    | 413/729 [00:14<00:03, 82.18it/s] 58%|█████▊    | 422/729 [00:14<00:03, 82.32it/s] 59%|█████▉    | 431/729 [00:15<00:03, 82.80it/s] 60%|██████    | 440/729 [00:15<00:03, 81.40it/s] 62%|██████▏   | 449/729 [00:15<00:03, 79.73it/s] 63%|██████▎   | 457/729 [00:15<00:03, 79.54it/s] 64%|██████▍   | 465/729 [00:15<00:03, 79.08it/s] 65%|██████▌   | 474/729 [00:15<00:03, 79.66it/s] 66%|██████▋   | 483/729 [00:15<00:03, 80.16it/s] 67%|██████▋   | 492/729 [00:15<00:02, 80.80it/s] 69%|██████▊   | 501/729 [00:15<00:02, 80.92it/s] 70%|██████▉   | 510/729 [00:15<00:02, 81.11it/s] 71%|███████   | 519/729 [00:16<00:02, 80.77it/s] 72%|███████▏  | 528/729 [00:16<00:02, 81.13it/s] 74%|███████▎  | 537/729 [00:16<00:02, 81.58it/s] 75%|███████▍  | 546/729 [00:16<00:02, 81.81it/s] 76%|███████▌  | 555/729 [00:16<00:02, 83.05it/s] 77%|███████▋  | 564/729 [00:16<00:01, 83.34it/s] 79%|███████▊  | 573/729 [00:16<00:01, 83.33it/s] 80%|███████▉  | 582/729 [00:16<00:01, 83.07it/s] 81%|████████  | 591/729 [00:16<00:01, 83.52it/s] 82%|████████▏ | 600/729 [00:17<00:01, 83.84it/s] 84%|████████▎ | 609/729 [00:17<00:01, 82.75it/s] 85%|████████▍ | 618/729 [00:17<00:01, 82.57it/s] 86%|████████▌ | 627/729 [00:17<00:01, 82.43it/s] 87%|████████▋ | 636/729 [00:17<00:01, 81.99it/s] 88%|████████▊ | 645/729 [00:17<00:01, 82.47it/s] 90%|████████▉ | 654/729 [00:17<00:00, 82.74it/s] 91%|█████████ | 663/729 [00:17<00:00, 83.42it/s] 92%|█████████▏| 672/729 [00:17<00:00, 83.04it/s] 93%|█████████▎| 681/729 [00:18<00:00, 82.59it/s] 95%|█████████▍| 690/729 [00:18<00:00, 82.22it/s] 96%|█████████▌| 699/729 [00:18<00:00, 82.49it/s] 97%|█████████▋| 708/729 [00:18<00:00, 82.65it/s] 98%|█████████▊| 717/729 [00:18<00:00, 82.73it/s]100%|█████████▉| 726/729 [00:18<00:00, 82.72it/s]100%|██████████| 729/729 [00:18<00:00, 39.11it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.final_layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.embed_tokens.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'lm_head.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:09,  2.43it/s]  9%|▊         | 2/23 [00:00<00:04,  4.23it/s] 13%|█▎        | 3/23 [00:00<00:03,  5.22it/s] 17%|█▋        | 4/23 [00:00<00:03,  5.75it/s] 22%|██▏       | 5/23 [00:00<00:02,  6.27it/s] 26%|██▌       | 6/23 [00:01<00:02,  6.73it/s] 30%|███       | 7/23 [00:01<00:02,  6.64it/s] 35%|███▍      | 8/23 [00:01<00:02,  7.15it/s] 39%|███▉      | 9/23 [00:01<00:01,  7.42it/s] 43%|████▎     | 10/23 [00:01<00:01,  6.87it/s] 48%|████▊     | 11/23 [00:01<00:01,  7.11it/s] 52%|█████▏    | 12/23 [00:01<00:01,  6.83it/s] 57%|█████▋    | 13/23 [00:02<00:01,  6.86it/s] 61%|██████    | 14/23 [00:02<00:01,  7.06it/s] 65%|██████▌   | 15/23 [00:02<00:01,  7.13it/s] 70%|██████▉   | 16/23 [00:05<00:06,  1.08it/s] 74%|███████▍  | 17/23 [00:05<00:04,  1.45it/s] 78%|███████▊  | 18/23 [00:05<00:02,  1.87it/s] 83%|████████▎ | 19/23 [00:05<00:01,  2.37it/s] 87%|████████▋ | 20/23 [00:05<00:00,  3.02it/s] 91%|█████████▏| 21/23 [00:05<00:00,  3.63it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.28it/s]100%|██████████| 23/23 [00:06<00:00,  3.80it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
25it [00:00, 246.78it/s]56it [00:00, 283.03it/s]87it [00:00, 293.36it/s]117it [00:00, 285.24it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
146it [00:00, 278.38it/s]174it [00:00, 277.90it/s]202it [00:00, 274.75it/s]230it [00:00, 265.08it/s]259it [00:00, 272.48it/s]288it [00:01, 277.20it/s]316it [00:01, 273.28it/s]347it [00:01, 281.88it/s]376it [00:01, 281.89it/s]405it [00:01, 280.51it/s]435it [00:01, 285.02it/s]464it [00:01, 282.53it/s]493it [00:01, 282.32it/s]522it [00:01, 279.87it/s]552it [00:01, 284.99it/s]583it [00:02, 288.70it/s]615it [00:02, 294.42it/s]645it [00:02, 295.93it/s]675it [00:02, 296.98it/s]705it [00:02, 295.20it/s]729it [00:02, 283.36it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 7/729 [00:00<00:11, 65.11it/s]  2%|▏         | 14/729 [00:00<00:10, 65.80it/s]  3%|▎         | 21/729 [00:00<00:12, 56.62it/s]  4%|▍         | 28/729 [00:00<00:11, 59.05it/s]  5%|▍         | 35/729 [00:00<00:11, 61.54it/s]  6%|▌         | 42/729 [00:00<00:10, 63.52it/s]  7%|▋         | 49/729 [00:00<00:10, 65.07it/s]  8%|▊         | 56/729 [00:00<00:10, 65.81it/s]  9%|▊         | 63/729 [00:00<00:10, 65.73it/s] 10%|▉         | 70/729 [00:01<00:09, 66.03it/s] 11%|█         | 77/729 [00:01<00:09, 66.53it/s] 12%|█▏        | 84/729 [00:01<00:09, 66.25it/s] 12%|█▏        | 91/729 [00:01<00:09, 65.00it/s] 13%|█▎        | 98/729 [00:01<00:09, 64.68it/s] 14%|█▍        | 105/729 [00:01<00:09, 64.42it/s] 15%|█▌        | 112/729 [00:01<00:09, 64.23it/s] 16%|█▋        | 119/729 [00:01<00:09, 62.96it/s] 17%|█▋        | 126/729 [00:01<00:09, 62.77it/s] 18%|█▊        | 133/729 [00:02<00:09, 62.56it/s] 19%|█▉        | 140/729 [00:02<00:09, 63.11it/s] 20%|██        | 147/729 [00:02<00:09, 62.95it/s] 21%|██        | 154/729 [00:02<00:09, 63.24it/s] 22%|██▏       | 161/729 [00:02<00:08, 63.67it/s] 23%|██▎       | 168/729 [00:02<00:08, 64.87it/s] 24%|██▍       | 175/729 [00:02<00:08, 64.79it/s] 25%|██▍       | 182/729 [00:02<00:08, 64.61it/s] 26%|██▌       | 189/729 [00:02<00:08, 64.34it/s] 27%|██▋       | 196/729 [00:03<00:08, 63.77it/s] 28%|██▊       | 203/729 [00:03<00:08, 62.99it/s] 29%|██▉       | 210/729 [00:03<00:08, 63.25it/s] 30%|██▉       | 217/729 [00:03<00:08, 63.90it/s] 31%|███       | 224/729 [00:03<00:07, 64.60it/s] 32%|███▏      | 231/729 [00:03<00:07, 64.85it/s] 33%|███▎      | 238/729 [00:03<00:07, 65.25it/s] 34%|███▎      | 245/729 [00:03<00:07, 65.07it/s] 35%|███▍      | 252/729 [00:03<00:07, 64.17it/s] 36%|███▌      | 259/729 [00:04<00:07, 64.80it/s] 36%|███▋      | 266/729 [00:04<00:07, 64.71it/s] 37%|███▋      | 273/729 [00:04<00:07, 64.67it/s] 38%|███▊      | 280/729 [00:04<00:06, 64.94it/s] 39%|███▉      | 287/729 [00:04<00:06, 64.37it/s] 40%|████      | 294/729 [00:04<00:06, 64.51it/s] 41%|████▏     | 301/729 [00:04<00:06, 64.75it/s] 42%|████▏     | 308/729 [00:04<00:06, 64.87it/s] 43%|████▎     | 315/729 [00:04<00:06, 64.75it/s] 44%|████▍     | 322/729 [00:05<00:06, 63.68it/s] 45%|████▌     | 329/729 [00:05<00:06, 63.57it/s] 46%|████▌     | 336/729 [00:05<00:06, 63.78it/s] 47%|████▋     | 343/729 [00:05<00:05, 64.79it/s] 48%|████▊     | 350/729 [00:05<00:05, 65.42it/s] 49%|████▉     | 357/729 [00:05<00:06, 60.23it/s] 50%|████▉     | 364/729 [00:05<00:06, 59.82it/s] 51%|█████     | 371/729 [00:05<00:05, 61.35it/s] 52%|█████▏    | 378/729 [00:05<00:05, 62.44it/s] 53%|█████▎    | 385/729 [00:06<00:05, 57.75it/s] 54%|█████▎    | 391/729 [00:06<00:05, 57.60it/s] 55%|█████▍    | 398/729 [00:06<00:05, 58.78it/s] 56%|█████▌    | 405/729 [00:06<00:05, 60.16it/s] 57%|█████▋    | 412/729 [00:06<00:05, 60.88it/s] 57%|█████▋    | 419/729 [00:06<00:05, 56.87it/s] 58%|█████▊    | 426/729 [00:06<00:05, 58.76it/s] 59%|█████▉    | 433/729 [00:06<00:04, 61.21it/s] 60%|██████    | 440/729 [00:06<00:04, 62.32it/s] 61%|██████▏   | 447/729 [00:07<00:04, 58.03it/s] 62%|██████▏   | 454/729 [00:07<00:04, 59.34it/s] 63%|██████▎   | 461/729 [00:07<00:04, 58.70it/s] 64%|██████▍   | 468/729 [00:07<00:04, 59.45it/s] 65%|██████▌   | 474/729 [00:07<00:04, 59.55it/s] 66%|██████▌   | 481/729 [00:07<00:04, 59.25it/s] 67%|██████▋   | 488/729 [00:07<00:03, 60.30it/s] 68%|██████▊   | 495/729 [00:07<00:03, 60.63it/s] 69%|██████▉   | 502/729 [00:08<00:03, 61.40it/s] 70%|██████▉   | 509/729 [00:08<00:03, 61.72it/s] 71%|███████   | 516/729 [00:08<00:03, 61.34it/s] 72%|███████▏  | 523/729 [00:08<00:03, 60.98it/s] 73%|███████▎  | 530/729 [00:08<00:03, 60.83it/s] 74%|███████▎  | 537/729 [00:08<00:03, 61.10it/s] 75%|███████▍  | 544/729 [00:08<00:03, 60.83it/s] 76%|███████▌  | 551/729 [00:08<00:02, 61.18it/s] 77%|███████▋  | 558/729 [00:08<00:02, 61.33it/s] 78%|███████▊  | 565/729 [00:09<00:02, 61.22it/s] 78%|███████▊  | 572/729 [00:09<00:02, 62.20it/s] 79%|███████▉  | 579/729 [00:09<00:02, 61.28it/s] 80%|████████  | 586/729 [00:09<00:02, 61.01it/s] 81%|████████▏ | 593/729 [00:09<00:02, 61.44it/s] 82%|████████▏ | 600/729 [00:09<00:02, 61.95it/s] 83%|████████▎ | 607/729 [00:09<00:01, 61.65it/s] 84%|████████▍ | 614/729 [00:09<00:01, 61.66it/s] 85%|████████▌ | 621/729 [00:09<00:01, 62.04it/s] 86%|████████▌ | 628/729 [00:10<00:01, 61.68it/s] 87%|████████▋ | 635/729 [00:10<00:01, 59.01it/s] 88%|████████▊ | 641/729 [00:10<00:01, 56.83it/s] 89%|████████▉ | 648/729 [00:10<00:01, 58.35it/s] 90%|████████▉ | 655/729 [00:10<00:01, 59.02it/s] 91%|█████████ | 662/729 [00:10<00:01, 59.77it/s] 92%|█████████▏| 669/729 [00:10<00:00, 60.29it/s] 93%|█████████▎| 676/729 [00:10<00:00, 60.46it/s] 94%|█████████▎| 683/729 [00:11<00:00, 54.10it/s] 95%|█████████▍| 689/729 [00:11<00:00, 54.81it/s] 95%|█████████▌| 696/729 [00:11<00:00, 57.17it/s] 96%|█████████▋| 703/729 [00:11<00:00, 59.00it/s] 97%|█████████▋| 710/729 [00:11<00:00, 60.97it/s] 98%|█████████▊| 717/729 [00:11<00:00, 62.39it/s] 99%|█████████▉| 724/729 [00:11<00:00, 62.48it/s]100%|██████████| 729/729 [00:11<00:00, 61.97it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.final_layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.embed_tokens.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'lm_head.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  3.90it/s]  9%|▊         | 2/23 [00:00<00:05,  3.77it/s] 13%|█▎        | 3/23 [00:00<00:05,  3.87it/s] 17%|█▋        | 4/23 [00:03<00:24,  1.27s/it] 22%|██▏       | 5/23 [00:03<00:15,  1.14it/s] 26%|██▌       | 6/23 [00:03<00:10,  1.56it/s] 30%|███       | 7/23 [00:04<00:07,  2.05it/s] 35%|███▍      | 8/23 [00:04<00:05,  2.58it/s] 39%|███▉      | 9/23 [00:04<00:04,  3.07it/s] 43%|████▎     | 10/23 [00:04<00:03,  3.57it/s] 48%|████▊     | 11/23 [00:04<00:02,  4.16it/s] 52%|█████▏    | 12/23 [00:05<00:02,  4.36it/s] 57%|█████▋    | 13/23 [00:05<00:02,  4.67it/s] 61%|██████    | 14/23 [00:05<00:01,  4.75it/s] 65%|██████▌   | 15/23 [00:05<00:01,  4.99it/s] 70%|██████▉   | 16/23 [00:05<00:01,  5.18it/s] 74%|███████▍  | 17/23 [00:05<00:01,  5.18it/s] 78%|███████▊  | 18/23 [00:06<00:00,  5.09it/s] 83%|████████▎ | 19/23 [00:06<00:00,  5.02it/s] 87%|████████▋ | 20/23 [00:06<00:00,  5.22it/s] 91%|█████████▏| 21/23 [00:06<00:00,  5.34it/s] 96%|█████████▌| 22/23 [00:06<00:00,  5.32it/s]100%|██████████| 23/23 [00:07<00:00,  5.72it/s]100%|██████████| 23/23 [00:07<00:00,  3.26it/s]
{'bleu-1': 0.396797461805203, 'bleu-2': 0.18025478824445654, 'bleu-3': 0.06702074582500094, 'bleu-4': 0.01872448713676624}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_style_classifier_1119-s-104384-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 26.662151
begin predicting
acc : 0.06995884773662552
{'distinct-1': 0.025250266607038427, 'distinct-2': 0.3515106406916666, 'distinct-3': 0.7453622254350363, 'distinct-4': 0.9290904469317813}
{'bleu-1': 0.3730788343692234, 'bleu-2': 0.16942932421767198, 'bleu-3': 0.0649424055693506, 'bleu-4': 0.017690608947933836}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_style_classifier_1119-s-104384-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 25.309278
begin predicting
acc : 0.23868312757201646
{'distinct-1': 0.020839036636909333, 'distinct-2': 0.3363303318616729, 'distinct-3': 0.735295902473889, 'distinct-4': 0.9235939829245156}
