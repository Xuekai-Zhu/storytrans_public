0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 0.934 seconds.
Prefix dict has been built successfully.
1it [00:00,  1.07it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
33it [00:01, 43.20it/s]68it [00:01, 91.64it/s]100it [00:01, 133.40it/s]131it [00:01, 169.10it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
163it [00:01, 203.31it/s]195it [00:01, 231.21it/s]230it [00:01, 259.61it/s]266it [00:01, 284.63it/s]300it [00:01, 299.81it/s]335it [00:01, 312.74it/s]371it [00:02, 325.73it/s]406it [00:02, 326.10it/s]440it [00:02, 328.35it/s]474it [00:02, 323.54it/s]508it [00:02, 325.94it/s]545it [00:02, 335.09it/s]579it [00:02, 334.09it/s]613it [00:02, 335.40it/s]647it [00:02, 336.67it/s]682it [00:02, 340.54it/s]718it [00:03, 344.39it/s]729it [00:03, 233.80it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:06<1:17:07,  6.36s/it]  1%|          | 6/729 [00:06<09:39,  1.25it/s]    2%|▏         | 11/729 [00:07<05:00,  2.39it/s]  2%|▏         | 16/729 [00:07<02:53,  4.12it/s]  3%|▎         | 23/729 [00:07<01:36,  7.32it/s]  4%|▍         | 30/729 [00:07<01:01, 11.33it/s]  5%|▌         | 37/729 [00:07<00:42, 16.22it/s]  6%|▌         | 44/729 [00:07<00:31, 21.82it/s]  7%|▋         | 51/729 [00:07<00:24, 27.55it/s]  8%|▊         | 57/729 [00:07<00:21, 31.34it/s]  9%|▉         | 64/729 [00:07<00:17, 37.45it/s] 10%|▉         | 71/729 [00:08<00:15, 43.46it/s] 11%|█         | 78/729 [00:08<00:13, 48.26it/s] 12%|█▏        | 85/729 [00:08<00:12, 52.17it/s] 13%|█▎        | 92/729 [00:08<00:11, 55.17it/s] 14%|█▎        | 99/729 [00:08<00:10, 58.34it/s] 15%|█▍        | 108/729 [00:08<00:09, 64.84it/s] 16%|█▌        | 117/729 [00:08<00:08, 69.69it/s] 17%|█▋        | 125/729 [00:08<00:08, 72.46it/s] 18%|█▊        | 134/729 [00:08<00:07, 75.32it/s] 19%|█▉        | 142/729 [00:09<00:07, 76.14it/s] 21%|██        | 151/729 [00:09<00:07, 77.53it/s] 22%|██▏       | 160/729 [00:09<00:07, 78.54it/s] 23%|██▎       | 169/729 [00:09<00:06, 80.07it/s] 24%|██▍       | 178/729 [00:09<00:06, 80.60it/s] 26%|██▌       | 187/729 [00:11<00:36, 14.92it/s] 26%|██▋       | 193/729 [00:11<00:30, 17.74it/s] 27%|██▋       | 200/729 [00:11<00:23, 22.11it/s] 28%|██▊       | 207/729 [00:11<00:19, 27.06it/s] 29%|██▉       | 214/729 [00:11<00:15, 32.42it/s] 30%|███       | 221/729 [00:11<00:13, 37.84it/s] 31%|███▏      | 228/729 [00:11<00:11, 43.06it/s] 32%|███▏      | 236/729 [00:11<00:09, 49.57it/s] 33%|███▎      | 244/729 [00:12<00:08, 56.32it/s] 35%|███▍      | 253/729 [00:12<00:07, 62.63it/s] 36%|███▌      | 262/729 [00:12<00:06, 67.79it/s] 37%|███▋      | 271/729 [00:12<00:06, 71.69it/s] 38%|███▊      | 280/729 [00:12<00:06, 74.25it/s] 40%|███▉      | 289/729 [00:12<00:05, 76.78it/s] 41%|████      | 298/729 [00:12<00:05, 78.17it/s] 42%|████▏     | 307/729 [00:12<00:05, 79.33it/s] 43%|████▎     | 316/729 [00:12<00:05, 79.19it/s] 45%|████▍     | 325/729 [00:13<00:05, 80.77it/s] 46%|████▌     | 334/729 [00:13<00:04, 80.79it/s] 47%|████▋     | 343/729 [00:13<00:04, 80.96it/s] 48%|████▊     | 352/729 [00:13<00:04, 79.96it/s] 50%|████▉     | 361/729 [00:13<00:04, 79.99it/s] 51%|█████     | 370/729 [00:13<00:04, 80.00it/s] 52%|█████▏    | 379/729 [00:13<00:04, 80.34it/s] 53%|█████▎    | 388/729 [00:13<00:04, 79.53it/s] 54%|█████▍    | 397/729 [00:13<00:04, 79.79it/s] 56%|█████▌    | 406/729 [00:14<00:03, 81.17it/s] 57%|█████▋    | 415/729 [00:14<00:04, 69.03it/s] 58%|█████▊    | 423/729 [00:14<00:04, 63.59it/s] 59%|█████▉    | 430/729 [00:14<00:04, 64.28it/s] 60%|█████▉    | 437/729 [00:14<00:04, 64.37it/s] 61%|██████    | 444/729 [00:14<00:04, 64.60it/s] 62%|██████▏   | 451/729 [00:14<00:04, 64.78it/s] 63%|██████▎   | 458/729 [00:14<00:04, 64.77it/s] 64%|██████▍   | 465/729 [00:15<00:04, 65.99it/s] 65%|██████▌   | 474/729 [00:15<00:03, 71.52it/s] 66%|██████▋   | 483/729 [00:15<00:03, 74.88it/s] 67%|██████▋   | 492/729 [00:15<00:03, 77.46it/s] 69%|██████▊   | 501/729 [00:15<00:02, 78.72it/s] 70%|██████▉   | 510/729 [00:15<00:02, 80.46it/s] 71%|███████   | 519/729 [00:15<00:02, 80.42it/s] 72%|███████▏  | 528/729 [00:15<00:02, 80.59it/s] 74%|███████▎  | 537/729 [00:15<00:02, 82.12it/s] 75%|███████▍  | 546/729 [00:16<00:02, 82.58it/s] 76%|███████▌  | 555/729 [00:16<00:02, 78.10it/s] 77%|███████▋  | 563/729 [00:16<00:02, 75.64it/s] 78%|███████▊  | 571/729 [00:16<00:02, 75.63it/s] 80%|███████▉  | 580/729 [00:16<00:01, 77.88it/s] 81%|████████  | 589/729 [00:16<00:01, 80.02it/s] 82%|████████▏ | 598/729 [00:16<00:01, 80.75it/s] 83%|████████▎ | 607/729 [00:16<00:01, 80.96it/s] 84%|████████▍ | 616/729 [00:16<00:01, 81.23it/s] 86%|████████▌ | 625/729 [00:17<00:01, 81.13it/s] 87%|████████▋ | 634/729 [00:17<00:01, 81.95it/s] 88%|████████▊ | 643/729 [00:17<00:01, 82.70it/s] 89%|████████▉ | 652/729 [00:17<00:00, 83.42it/s] 91%|█████████ | 661/729 [00:17<00:00, 83.36it/s] 92%|█████████▏| 670/729 [00:17<00:00, 83.33it/s] 93%|█████████▎| 679/729 [00:17<00:00, 82.85it/s] 94%|█████████▍| 688/729 [00:17<00:00, 82.64it/s] 96%|█████████▌| 697/729 [00:17<00:00, 81.99it/s] 97%|█████████▋| 706/729 [00:17<00:00, 82.69it/s] 98%|█████████▊| 715/729 [00:18<00:00, 83.46it/s] 99%|█████████▉| 724/729 [00:18<00:00, 82.11it/s]100%|██████████| 729/729 [00:18<00:00, 39.92it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'lm_head.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:04,  5.22it/s]  9%|▊         | 2/23 [00:01<00:17,  1.18it/s] 13%|█▎        | 3/23 [00:01<00:10,  1.92it/s] 17%|█▋        | 4/23 [00:04<00:30,  1.59s/it] 22%|██▏       | 5/23 [00:04<00:19,  1.06s/it] 26%|██▌       | 6/23 [00:05<00:12,  1.34it/s] 30%|███       | 7/23 [00:05<00:08,  1.84it/s] 35%|███▍      | 8/23 [00:05<00:06,  2.38it/s] 39%|███▉      | 9/23 [00:05<00:04,  2.98it/s] 43%|████▎     | 10/23 [00:05<00:03,  3.62it/s] 48%|████▊     | 11/23 [00:05<00:02,  4.42it/s] 52%|█████▏    | 12/23 [00:05<00:02,  4.84it/s] 57%|█████▋    | 13/23 [00:06<00:01,  5.40it/s] 61%|██████    | 14/23 [00:06<00:01,  5.90it/s] 65%|██████▌   | 15/23 [00:06<00:01,  6.21it/s] 70%|██████▉   | 16/23 [00:06<00:01,  6.46it/s] 74%|███████▍  | 17/23 [00:06<00:00,  6.85it/s] 78%|███████▊  | 18/23 [00:06<00:00,  6.95it/s] 83%|████████▎ | 19/23 [00:06<00:00,  6.80it/s] 87%|████████▋ | 20/23 [00:07<00:00,  6.99it/s] 91%|█████████▏| 21/23 [00:07<00:00,  6.94it/s] 96%|█████████▌| 22/23 [00:07<00:00,  6.79it/s]100%|██████████| 23/23 [00:07<00:00,  7.50it/s]100%|██████████| 23/23 [00:07<00:00,  3.08it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
28it [00:00, 272.31it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
61it [00:00, 303.48it/s]93it [00:00, 308.78it/s]124it [00:00, 307.32it/s]155it [00:00, 308.07it/s]186it [00:00, 302.66it/s]217it [00:00, 300.88it/s]252it [00:00, 313.90it/s]285it [00:00, 318.63it/s]317it [00:01, 317.57it/s]353it [00:01, 325.77it/s]386it [00:01, 325.20it/s]419it [00:01, 325.63it/s]452it [00:01, 274.83it/s]481it [00:01, 259.67it/s]508it [00:01, 250.95it/s]534it [00:01, 253.11it/s]560it [00:01, 245.47it/s]585it [00:02, 244.91it/s]610it [00:02, 246.30it/s]635it [00:02, 245.96it/s]662it [00:02, 252.11it/s]688it [00:02, 252.70it/s]715it [00:02, 256.74it/s]729it [00:02, 278.00it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 9/729 [00:00<00:08, 81.07it/s]  2%|▏         | 18/729 [00:00<00:08, 81.74it/s]  4%|▎         | 27/729 [00:00<00:08, 80.88it/s]  5%|▍         | 36/729 [00:00<00:08, 80.01it/s]  6%|▌         | 45/729 [00:00<00:08, 80.35it/s]  7%|▋         | 54/729 [00:00<00:08, 81.33it/s]  9%|▊         | 63/729 [00:00<00:08, 81.06it/s] 10%|▉         | 72/729 [00:00<00:08, 81.24it/s] 11%|█         | 81/729 [00:00<00:07, 81.39it/s] 12%|█▏        | 90/729 [00:01<00:07, 80.65it/s] 14%|█▎        | 99/729 [00:01<00:07, 80.14it/s] 15%|█▍        | 108/729 [00:01<00:07, 80.25it/s] 16%|█▌        | 117/729 [00:01<00:07, 79.22it/s] 17%|█▋        | 125/729 [00:01<00:07, 78.82it/s] 18%|█▊        | 133/729 [00:01<00:07, 79.00it/s] 19%|█▉        | 141/729 [00:01<00:07, 79.08it/s] 21%|██        | 150/729 [00:01<00:07, 80.14it/s] 22%|██▏       | 159/729 [00:01<00:07, 80.71it/s] 23%|██▎       | 168/729 [00:02<00:06, 81.31it/s] 24%|██▍       | 177/729 [00:02<00:06, 81.26it/s] 26%|██▌       | 186/729 [00:02<00:07, 71.64it/s] 27%|██▋       | 194/729 [00:02<00:07, 70.14it/s] 28%|██▊       | 202/729 [00:02<00:07, 68.30it/s] 29%|██▊       | 209/729 [00:02<00:07, 67.81it/s] 30%|██▉       | 216/729 [00:02<00:07, 67.72it/s] 31%|███       | 223/729 [00:02<00:07, 65.96it/s] 32%|███▏      | 231/729 [00:03<00:07, 69.16it/s] 33%|███▎      | 240/729 [00:03<00:06, 73.49it/s] 34%|███▍      | 249/729 [00:03<00:06, 75.47it/s] 35%|███▌      | 257/729 [00:03<00:06, 73.31it/s] 36%|███▋      | 265/729 [00:03<00:06, 71.52it/s] 37%|███▋      | 273/729 [00:03<00:06, 70.28it/s] 39%|███▊      | 281/729 [00:03<00:06, 68.64it/s] 40%|███▉      | 288/729 [00:03<00:06, 68.07it/s] 40%|████      | 295/729 [00:03<00:06, 66.97it/s] 41%|████▏     | 302/729 [00:04<00:07, 59.20it/s] 42%|████▏     | 309/729 [00:04<00:07, 57.12it/s] 43%|████▎     | 315/729 [00:04<00:07, 56.07it/s] 44%|████▍     | 321/729 [00:04<00:07, 55.15it/s] 45%|████▍     | 327/729 [00:04<00:07, 55.13it/s] 46%|████▌     | 333/729 [00:04<00:07, 54.87it/s] 47%|████▋     | 339/729 [00:04<00:07, 54.90it/s] 47%|████▋     | 345/729 [00:04<00:06, 55.34it/s] 48%|████▊     | 352/729 [00:04<00:06, 58.58it/s] 49%|████▉     | 359/729 [00:05<00:06, 59.96it/s] 50%|█████     | 366/729 [00:05<00:06, 55.71it/s] 51%|█████     | 373/729 [00:05<00:06, 58.63it/s] 52%|█████▏    | 380/729 [00:05<00:05, 60.14it/s] 53%|█████▎    | 387/729 [00:05<00:05, 60.83it/s] 54%|█████▍    | 394/729 [00:05<00:05, 61.79it/s] 55%|█████▌    | 401/729 [00:05<00:05, 62.71it/s] 56%|█████▌    | 410/729 [00:05<00:04, 68.18it/s] 57%|█████▋    | 417/729 [00:06<00:05, 61.27it/s] 58%|█████▊    | 424/729 [00:06<00:04, 62.30it/s] 59%|█████▉    | 431/729 [00:06<00:04, 63.68it/s] 60%|██████    | 438/729 [00:06<00:04, 64.22it/s] 61%|██████    | 445/729 [00:06<00:04, 64.90it/s] 62%|██████▏   | 452/729 [00:06<00:04, 65.56it/s] 63%|██████▎   | 459/729 [00:06<00:04, 65.89it/s] 64%|██████▍   | 466/729 [00:06<00:04, 55.95it/s] 65%|██████▍   | 473/729 [00:06<00:04, 57.60it/s] 66%|██████▌   | 480/729 [00:07<00:04, 59.18it/s] 67%|██████▋   | 487/729 [00:07<00:03, 61.01it/s] 68%|██████▊   | 494/729 [00:07<00:03, 62.40it/s] 69%|██████▉   | 502/729 [00:07<00:03, 65.78it/s] 70%|██████▉   | 510/729 [00:07<00:03, 68.00it/s] 71%|███████   | 518/729 [00:07<00:02, 70.45it/s] 72%|███████▏  | 526/729 [00:07<00:02, 72.60it/s] 73%|███████▎  | 534/729 [00:07<00:02, 74.37it/s] 74%|███████▍  | 543/729 [00:07<00:02, 76.33it/s] 76%|███████▌  | 552/729 [00:08<00:02, 77.51it/s] 77%|███████▋  | 560/729 [00:08<00:02, 78.15it/s] 78%|███████▊  | 568/729 [00:08<00:02, 78.24it/s] 79%|███████▉  | 577/729 [00:08<00:01, 78.81it/s] 80%|████████  | 586/729 [00:08<00:01, 79.57it/s] 82%|████████▏ | 595/729 [00:08<00:01, 80.53it/s] 83%|████████▎ | 604/729 [00:08<00:01, 80.79it/s] 84%|████████▍ | 613/729 [00:08<00:01, 72.54it/s] 85%|████████▌ | 621/729 [00:09<00:01, 59.19it/s] 86%|████████▌ | 628/729 [00:09<00:01, 58.30it/s] 87%|████████▋ | 635/729 [00:09<00:01, 60.55it/s] 88%|████████▊ | 642/729 [00:09<00:01, 57.60it/s] 89%|████████▉ | 648/729 [00:09<00:01, 56.26it/s] 90%|████████▉ | 655/729 [00:09<00:01, 58.03it/s] 91%|█████████ | 662/729 [00:09<00:01, 61.12it/s] 92%|█████████▏| 669/729 [00:09<00:00, 62.71it/s] 93%|█████████▎| 676/729 [00:09<00:00, 64.24it/s] 94%|█████████▎| 683/729 [00:10<00:00, 65.54it/s] 95%|█████████▍| 690/729 [00:10<00:00, 66.57it/s] 96%|█████████▌| 698/729 [00:10<00:00, 68.43it/s] 97%|█████████▋| 707/729 [00:10<00:00, 73.24it/s] 98%|█████████▊| 716/729 [00:10<00:00, 76.55it/s] 99%|█████████▉| 725/729 [00:10<00:00, 78.26it/s]100%|██████████| 729/729 [00:10<00:00, 68.65it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'lm_head.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:04,  5.33it/s]  9%|▊         | 2/23 [00:00<00:03,  6.33it/s] 13%|█▎        | 3/23 [00:00<00:03,  6.53it/s] 17%|█▋        | 4/23 [00:03<00:24,  1.28s/it] 22%|██▏       | 5/23 [00:03<00:15,  1.15it/s] 26%|██▌       | 6/23 [00:03<00:10,  1.59it/s] 30%|███       | 7/23 [00:03<00:07,  2.10it/s] 35%|███▍      | 8/23 [00:04<00:05,  2.65it/s] 39%|███▉      | 9/23 [00:04<00:04,  3.24it/s] 43%|████▎     | 10/23 [00:04<00:03,  3.72it/s] 48%|████▊     | 11/23 [00:04<00:02,  4.28it/s] 52%|█████▏    | 12/23 [00:04<00:02,  4.66it/s] 57%|█████▋    | 13/23 [00:04<00:02,  4.94it/s] 61%|██████    | 14/23 [00:05<00:01,  5.41it/s] 65%|██████▌   | 15/23 [00:05<00:01,  5.68it/s] 70%|██████▉   | 16/23 [00:05<00:01,  5.71it/s] 74%|███████▍  | 17/23 [00:05<00:00,  6.02it/s] 78%|███████▊  | 18/23 [00:05<00:00,  6.28it/s] 83%|████████▎ | 19/23 [00:05<00:00,  5.96it/s] 87%|████████▋ | 20/23 [00:06<00:00,  6.37it/s] 91%|█████████▏| 21/23 [00:06<00:00,  6.49it/s] 96%|█████████▌| 22/23 [00:06<00:00,  6.57it/s]100%|██████████| 23/23 [00:06<00:00,  7.21it/s]100%|██████████| 23/23 [00:06<00:00,  3.58it/s]
{'bleu-1': 0.55734679043814, 'bleu-2': 0.3446628822197362, 'bleu-3': 0.21899613906132376, 'bleu-4': 0.14089571059231143}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_sen_loss_1119-s-104384-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 30.665234
begin predicting
acc : 0.013717421124828532
{'distinct-1': 0.024219096236232623, 'distinct-2': 0.36585285302641246, 'distinct-3': 0.7683905372628501, 'distinct-4': 0.9409405059377063}
{'bleu-1': 0.5372481900537261, 'bleu-2': 0.3308934892607399, 'bleu-3': 0.21079669277191707, 'bleu-4': 0.13506940664677175}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_sen_loss_1119-s-104384-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 30.25407
begin predicting
acc : 0.10013717421124829
{'distinct-1': 0.022217371997982977, 'distinct-2': 0.3679582261238081, 'distinct-3': 0.7736186468408276, 'distinct-4': 0.9419659937864434}
