0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 1.049 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:01,  1.05s/it]30it [00:01, 35.35it/s]61it [00:01, 75.11it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
93it [00:01, 116.96it/s]132it [00:01, 170.26it/s]172it [00:01, 219.79it/s]206it [00:01, 248.18it/s]245it [00:01, 282.95it/s]284it [00:01, 310.33it/s]321it [00:01, 323.51it/s]362it [00:02, 346.77it/s]400it [00:02, 352.16it/s]438it [00:02, 354.26it/s]476it [00:02, 361.01it/s]514it [00:02, 355.63it/s]553it [00:02, 363.89it/s]594it [00:02, 374.60it/s]632it [00:02, 375.58it/s]670it [00:02, 367.97it/s]708it [00:03, 370.17it/s]729it [00:03, 238.05it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:05<1:09:04,  5.69s/it]  1%|          | 4/729 [00:05<13:24,  1.11s/it]    1%|          | 9/729 [00:05<04:43,  2.54it/s]  2%|▏         | 15/729 [00:06<02:18,  5.16it/s]  3%|▎         | 21/729 [00:06<01:30,  7.86it/s]  4%|▎         | 26/729 [00:06<01:05, 10.74it/s]  4%|▍         | 32/729 [00:06<00:45, 15.23it/s]  5%|▌         | 37/729 [00:06<00:43, 15.82it/s]  6%|▌         | 44/729 [00:06<00:30, 22.32it/s]  7%|▋         | 53/729 [00:07<00:21, 31.74it/s]  8%|▊         | 61/729 [00:07<00:16, 39.45it/s]  9%|▉         | 69/729 [00:07<00:14, 47.04it/s] 11%|█         | 77/729 [00:07<00:12, 53.44it/s] 12%|█▏        | 85/729 [00:07<00:10, 59.13it/s] 13%|█▎        | 93/729 [00:07<00:10, 63.48it/s] 14%|█▍        | 101/729 [00:07<00:09, 67.15it/s] 15%|█▌        | 110/729 [00:07<00:08, 70.97it/s] 16%|█▌        | 118/729 [00:07<00:08, 73.30it/s] 17%|█▋        | 126/729 [00:07<00:08, 74.36it/s] 18%|█▊        | 134/729 [00:08<00:08, 74.28it/s] 19%|█▉        | 142/729 [00:08<00:07, 75.37it/s] 21%|██        | 150/729 [00:08<00:07, 76.12it/s] 22%|██▏       | 158/729 [00:08<00:07, 75.95it/s] 23%|██▎       | 166/729 [00:08<00:07, 76.23it/s] 24%|██▍       | 174/729 [00:08<00:07, 76.52it/s] 25%|██▍       | 182/729 [00:08<00:07, 77.44it/s] 26%|██▌       | 190/729 [00:08<00:07, 76.59it/s] 27%|██▋       | 198/729 [00:08<00:06, 76.87it/s] 28%|██▊       | 206/729 [00:08<00:06, 76.38it/s] 29%|██▉       | 214/729 [00:09<00:06, 76.34it/s] 30%|███       | 222/729 [00:09<00:06, 76.90it/s] 32%|███▏      | 230/729 [00:09<00:06, 77.65it/s] 33%|███▎      | 238/729 [00:09<00:06, 77.51it/s] 34%|███▎      | 246/729 [00:09<00:06, 78.02it/s] 35%|███▍      | 254/729 [00:09<00:06, 78.24it/s] 36%|███▌      | 262/729 [00:09<00:05, 78.20it/s] 37%|███▋      | 270/729 [00:09<00:05, 78.30it/s] 38%|███▊      | 278/729 [00:09<00:05, 78.40it/s] 39%|███▉      | 286/729 [00:10<00:05, 76.50it/s] 40%|████      | 294/729 [00:10<00:05, 76.30it/s] 41%|████▏     | 302/729 [00:10<00:05, 76.73it/s] 43%|████▎     | 310/729 [00:10<00:05, 76.30it/s] 44%|████▎     | 318/729 [00:10<00:05, 75.68it/s] 45%|████▍     | 326/729 [00:10<00:05, 75.95it/s] 46%|████▌     | 334/729 [00:10<00:05, 65.92it/s] 47%|████▋     | 341/729 [00:10<00:06, 61.79it/s] 48%|████▊     | 348/729 [00:10<00:06, 62.10it/s] 49%|████▊     | 355/729 [00:11<00:05, 62.64it/s] 50%|████▉     | 362/729 [00:11<00:05, 63.60it/s] 51%|█████     | 369/729 [00:11<00:05, 64.75it/s] 52%|█████▏    | 377/729 [00:11<00:05, 67.97it/s] 53%|█████▎    | 384/729 [00:11<00:05, 64.89it/s] 54%|█████▎    | 391/729 [00:11<00:05, 56.49it/s] 54%|█████▍    | 397/729 [00:11<00:05, 56.50it/s] 55%|█████▌    | 404/729 [00:11<00:05, 58.47it/s] 56%|█████▋    | 411/729 [00:11<00:05, 60.24it/s] 57%|█████▋    | 418/729 [00:12<00:05, 62.14it/s] 58%|█████▊    | 425/729 [00:12<00:04, 63.32it/s] 59%|█████▉    | 432/729 [00:12<00:04, 65.09it/s] 60%|██████    | 439/729 [00:12<00:04, 65.06it/s] 61%|██████    | 446/729 [00:12<00:04, 65.74it/s] 62%|██████▏   | 453/729 [00:12<00:04, 65.73it/s] 63%|██████▎   | 460/729 [00:12<00:04, 66.13it/s] 64%|██████▍   | 467/729 [00:12<00:03, 66.34it/s] 65%|██████▌   | 474/729 [00:12<00:03, 67.04it/s] 66%|██████▌   | 481/729 [00:13<00:03, 66.74it/s] 67%|██████▋   | 488/729 [00:13<00:03, 67.12it/s] 68%|██████▊   | 495/729 [00:13<00:03, 63.84it/s] 69%|██████▉   | 502/729 [00:13<00:03, 64.16it/s] 70%|██████▉   | 509/729 [00:13<00:03, 64.58it/s] 71%|███████   | 516/729 [00:13<00:03, 64.19it/s] 72%|███████▏  | 523/729 [00:13<00:03, 62.27it/s] 73%|███████▎  | 530/729 [00:13<00:03, 61.27it/s] 74%|███████▎  | 537/729 [00:13<00:03, 60.22it/s] 75%|███████▍  | 544/729 [00:14<00:03, 58.78it/s] 75%|███████▌  | 550/729 [00:14<00:03, 58.98it/s] 76%|███████▋  | 556/729 [00:14<00:02, 58.25it/s] 77%|███████▋  | 563/729 [00:14<00:02, 60.35it/s] 78%|███████▊  | 571/729 [00:14<00:02, 63.80it/s] 79%|███████▉  | 579/729 [00:14<00:02, 65.82it/s] 80%|████████  | 586/729 [00:14<00:02, 66.62it/s] 81%|████████▏ | 593/729 [00:14<00:02, 67.52it/s] 82%|████████▏ | 601/729 [00:14<00:01, 68.63it/s] 83%|████████▎ | 608/729 [00:15<00:01, 68.81it/s] 84%|████████▍ | 615/729 [00:15<00:01, 68.84it/s] 85%|████████▌ | 622/729 [00:15<00:01, 67.94it/s] 86%|████████▋ | 629/729 [00:15<00:01, 67.90it/s] 87%|████████▋ | 637/729 [00:15<00:01, 68.72it/s] 88%|████████▊ | 644/729 [00:15<00:01, 69.07it/s] 89%|████████▉ | 652/729 [00:15<00:01, 69.21it/s] 91%|█████████ | 660/729 [00:15<00:00, 70.04it/s] 92%|█████████▏| 668/729 [00:15<00:00, 69.66it/s] 93%|█████████▎| 675/729 [00:15<00:00, 69.10it/s] 94%|█████████▎| 682/729 [00:16<00:00, 68.64it/s] 95%|█████████▍| 689/729 [00:16<00:00, 68.88it/s] 95%|█████████▌| 696/729 [00:16<00:00, 67.76it/s] 97%|█████████▋| 704/729 [00:16<00:00, 68.73it/s] 98%|█████████▊| 711/729 [00:16<00:00, 68.91it/s] 98%|█████████▊| 718/729 [00:16<00:00, 68.51it/s] 99%|█████████▉| 725/729 [00:16<00:00, 67.43it/s]100%|██████████| 729/729 [00:16<00:00, 43.38it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'lm_head.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:02<00:52,  2.41s/it]  9%|▊         | 2/23 [00:02<00:22,  1.07s/it] 13%|█▎        | 3/23 [00:02<00:12,  1.58it/s] 17%|█▋        | 4/23 [00:02<00:08,  2.33it/s] 22%|██▏       | 5/23 [00:02<00:05,  3.15it/s] 26%|██▌       | 6/23 [00:02<00:04,  4.10it/s] 30%|███       | 7/23 [00:03<00:03,  5.00it/s] 35%|███▍      | 8/23 [00:03<00:02,  5.57it/s] 39%|███▉      | 9/23 [00:03<00:02,  6.34it/s] 43%|████▎     | 10/23 [00:03<00:01,  6.75it/s] 48%|████▊     | 11/23 [00:03<00:01,  7.38it/s] 52%|█████▏    | 12/23 [00:03<00:01,  7.55it/s] 57%|█████▋    | 13/23 [00:03<00:01,  7.92it/s] 61%|██████    | 14/23 [00:03<00:01,  8.14it/s] 65%|██████▌   | 15/23 [00:04<00:00,  8.30it/s] 70%|██████▉   | 16/23 [00:04<00:01,  3.84it/s] 74%|███████▍  | 17/23 [00:04<00:01,  4.56it/s] 78%|███████▊  | 18/23 [00:04<00:00,  5.34it/s] 83%|████████▎ | 19/23 [00:05<00:01,  3.36it/s] 87%|████████▋ | 20/23 [00:05<00:00,  4.14it/s] 91%|█████████▏| 21/23 [00:05<00:00,  4.78it/s] 96%|█████████▌| 22/23 [00:05<00:00,  5.43it/s]100%|██████████| 23/23 [00:05<00:00,  3.92it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
18it [00:00, 167.65it/s]39it [00:00, 191.45it/s]62it [00:00, 206.28it/s]86it [00:00, 213.59it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
109it [00:00, 219.05it/s]131it [00:00, 214.23it/s]154it [00:00, 217.29it/s]176it [00:00, 214.09it/s]198it [00:00, 214.45it/s]221it [00:01, 217.47it/s]244it [00:01, 218.57it/s]266it [00:01, 216.44it/s]288it [00:01, 211.24it/s]312it [00:01, 217.73it/s]337it [00:01, 224.79it/s]360it [00:01, 224.40it/s]383it [00:01, 223.28it/s]406it [00:01, 222.68it/s]431it [00:01, 227.87it/s]454it [00:02, 225.16it/s]477it [00:02, 225.47it/s]500it [00:02, 224.46it/s]523it [00:02, 224.35it/s]546it [00:02, 223.59it/s]569it [00:02, 224.88it/s]592it [00:02, 221.79it/s]615it [00:02, 218.61it/s]640it [00:02, 226.95it/s]667it [00:03, 236.65it/s]691it [00:03, 235.68it/s]718it [00:03, 245.31it/s]729it [00:03, 219.93it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 8/729 [00:00<00:09, 75.51it/s]  2%|▏         | 16/729 [00:00<00:09, 75.24it/s]  3%|▎         | 24/729 [00:00<00:09, 73.02it/s]  4%|▍         | 32/729 [00:00<00:09, 74.16it/s]  5%|▌         | 40/729 [00:00<00:10, 63.85it/s]  6%|▋         | 47/729 [00:00<00:10, 63.10it/s]  7%|▋         | 54/729 [00:00<00:10, 62.76it/s]  8%|▊         | 61/729 [00:00<00:10, 62.92it/s]  9%|▉         | 68/729 [00:01<00:10, 62.88it/s] 10%|█         | 75/729 [00:01<00:10, 63.25it/s] 11%|█         | 82/729 [00:01<00:10, 59.61it/s] 12%|█▏        | 89/729 [00:01<00:11, 57.11it/s] 13%|█▎        | 95/729 [00:01<00:11, 55.43it/s] 14%|█▍        | 101/729 [00:01<00:11, 54.24it/s] 15%|█▍        | 107/729 [00:01<00:11, 53.56it/s] 16%|█▌        | 114/729 [00:01<00:10, 57.04it/s] 17%|█▋        | 121/729 [00:01<00:10, 59.12it/s] 18%|█▊        | 128/729 [00:02<00:09, 60.97it/s] 19%|█▊        | 135/729 [00:02<00:09, 61.74it/s] 19%|█▉        | 142/729 [00:02<00:09, 63.20it/s] 20%|██        | 149/729 [00:02<00:09, 64.00it/s] 21%|██▏       | 156/729 [00:02<00:08, 64.48it/s] 22%|██▏       | 163/729 [00:02<00:08, 64.52it/s] 23%|██▎       | 170/729 [00:02<00:08, 65.10it/s] 24%|██▍       | 177/729 [00:02<00:08, 64.57it/s] 25%|██▌       | 184/729 [00:02<00:09, 60.16it/s] 26%|██▌       | 191/729 [00:03<00:09, 59.21it/s] 27%|██▋       | 198/729 [00:03<00:08, 59.80it/s] 28%|██▊       | 205/729 [00:03<00:08, 61.19it/s] 29%|██▉       | 212/729 [00:03<00:08, 61.88it/s] 30%|███       | 219/729 [00:03<00:08, 62.74it/s] 31%|███       | 226/729 [00:03<00:08, 57.58it/s] 32%|███▏      | 232/729 [00:03<00:09, 54.61it/s] 33%|███▎      | 238/729 [00:03<00:09, 54.29it/s] 33%|███▎      | 244/729 [00:04<00:09, 53.26it/s] 34%|███▍      | 250/729 [00:04<00:09, 53.14it/s] 35%|███▌      | 256/729 [00:04<00:08, 53.32it/s] 36%|███▌      | 262/729 [00:04<00:08, 53.34it/s] 37%|███▋      | 269/729 [00:04<00:08, 56.20it/s] 38%|███▊      | 276/729 [00:04<00:07, 58.39it/s] 39%|███▉      | 283/729 [00:04<00:07, 59.40it/s] 40%|███▉      | 290/729 [00:04<00:07, 60.13it/s] 41%|████      | 297/729 [00:04<00:07, 61.45it/s] 42%|████▏     | 304/729 [00:05<00:06, 61.72it/s] 43%|████▎     | 311/729 [00:05<00:06, 61.46it/s] 44%|████▎     | 318/729 [00:05<00:06, 61.53it/s] 45%|████▍     | 325/729 [00:05<00:06, 62.01it/s] 46%|████▌     | 332/729 [00:05<00:06, 62.03it/s] 47%|████▋     | 339/729 [00:05<00:06, 62.30it/s] 47%|████▋     | 346/729 [00:05<00:06, 62.40it/s] 48%|████▊     | 353/729 [00:05<00:05, 62.84it/s] 49%|████▉     | 360/729 [00:05<00:05, 62.29it/s] 50%|█████     | 367/729 [00:06<00:05, 61.96it/s] 51%|█████▏    | 374/729 [00:06<00:05, 62.38it/s] 52%|█████▏    | 381/729 [00:06<00:05, 62.49it/s] 53%|█████▎    | 388/729 [00:06<00:05, 62.89it/s] 54%|█████▍    | 395/729 [00:06<00:05, 63.04it/s] 55%|█████▌    | 402/729 [00:06<00:05, 62.91it/s] 56%|█████▌    | 409/729 [00:06<00:05, 63.19it/s] 57%|█████▋    | 416/729 [00:06<00:05, 62.53it/s] 58%|█████▊    | 423/729 [00:06<00:04, 62.36it/s] 59%|█████▉    | 430/729 [00:07<00:04, 62.97it/s] 60%|█████▉    | 437/729 [00:07<00:04, 63.24it/s] 61%|██████    | 444/729 [00:07<00:04, 63.79it/s] 62%|██████▏   | 451/729 [00:07<00:04, 64.16it/s] 63%|██████▎   | 458/729 [00:07<00:04, 64.09it/s] 64%|██████▍   | 465/729 [00:07<00:04, 63.56it/s] 65%|██████▍   | 472/729 [00:07<00:04, 63.51it/s] 66%|██████▌   | 479/729 [00:07<00:03, 62.93it/s] 67%|██████▋   | 486/729 [00:07<00:03, 62.06it/s] 68%|██████▊   | 493/729 [00:08<00:03, 62.13it/s] 69%|██████▊   | 500/729 [00:08<00:03, 61.82it/s] 70%|██████▉   | 507/729 [00:08<00:03, 60.99it/s] 71%|███████   | 514/729 [00:08<00:03, 61.52it/s] 71%|███████▏  | 521/729 [00:08<00:03, 61.48it/s] 72%|███████▏  | 528/729 [00:08<00:03, 61.51it/s] 73%|███████▎  | 535/729 [00:08<00:03, 62.02it/s] 74%|███████▍  | 542/729 [00:08<00:02, 62.70it/s] 75%|███████▌  | 549/729 [00:08<00:02, 63.45it/s] 76%|███████▋  | 556/729 [00:09<00:02, 63.68it/s] 77%|███████▋  | 563/729 [00:09<00:02, 63.72it/s] 78%|███████▊  | 570/729 [00:09<00:02, 62.85it/s] 79%|███████▉  | 577/729 [00:09<00:02, 62.22it/s] 80%|████████  | 584/729 [00:09<00:02, 61.49it/s] 81%|████████  | 591/729 [00:09<00:02, 62.55it/s] 82%|████████▏ | 598/729 [00:09<00:02, 62.20it/s] 83%|████████▎ | 605/729 [00:09<00:01, 62.26it/s] 84%|████████▍ | 612/729 [00:09<00:01, 61.95it/s] 85%|████████▍ | 619/729 [00:10<00:01, 62.12it/s] 86%|████████▌ | 626/729 [00:10<00:01, 61.94it/s] 87%|████████▋ | 633/729 [00:10<00:01, 61.84it/s] 88%|████████▊ | 640/729 [00:10<00:01, 61.68it/s] 89%|████████▉ | 647/729 [00:10<00:01, 61.20it/s] 90%|████████▉ | 654/729 [00:10<00:01, 62.54it/s] 91%|█████████ | 661/729 [00:10<00:01, 62.16it/s] 92%|█████████▏| 668/729 [00:10<00:00, 62.37it/s] 93%|█████████▎| 675/729 [00:10<00:00, 61.80it/s] 94%|█████████▎| 682/729 [00:11<00:00, 61.58it/s] 95%|█████████▍| 689/729 [00:11<00:00, 61.60it/s] 95%|█████████▌| 696/729 [00:11<00:00, 61.11it/s] 96%|█████████▋| 703/729 [00:11<00:00, 61.02it/s] 97%|█████████▋| 710/729 [00:11<00:00, 61.33it/s] 98%|█████████▊| 717/729 [00:11<00:00, 62.21it/s] 99%|█████████▉| 724/729 [00:11<00:00, 61.53it/s]100%|██████████| 729/729 [00:11<00:00, 61.45it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'lm_head.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:01<00:27,  1.24s/it]  9%|▊         | 2/23 [00:01<00:13,  1.55it/s] 13%|█▎        | 3/23 [00:01<00:09,  2.15it/s] 17%|█▋        | 4/23 [00:01<00:06,  2.72it/s] 22%|██▏       | 5/23 [00:02<00:05,  3.18it/s] 26%|██▌       | 6/23 [00:02<00:04,  3.47it/s] 30%|███       | 7/23 [00:02<00:04,  3.74it/s] 35%|███▍      | 8/23 [00:02<00:03,  3.97it/s] 39%|███▉      | 9/23 [00:03<00:03,  4.10it/s] 43%|████▎     | 10/23 [00:03<00:03,  4.21it/s] 48%|████▊     | 11/23 [00:03<00:02,  4.22it/s] 52%|█████▏    | 12/23 [00:03<00:02,  4.21it/s] 57%|█████▋    | 13/23 [00:04<00:02,  4.13it/s] 61%|██████    | 14/23 [00:04<00:02,  4.14it/s] 65%|██████▌   | 15/23 [00:04<00:01,  4.17it/s] 70%|██████▉   | 16/23 [00:04<00:01,  4.04it/s] 74%|███████▍  | 17/23 [00:05<00:01,  4.06it/s] 78%|███████▊  | 18/23 [00:05<00:01,  4.19it/s] 83%|████████▎ | 19/23 [00:05<00:01,  3.88it/s] 87%|████████▋ | 20/23 [00:05<00:00,  3.99it/s] 91%|█████████▏| 21/23 [00:05<00:00,  4.13it/s] 96%|█████████▌| 22/23 [00:06<00:00,  4.02it/s]100%|██████████| 23/23 [00:06<00:00,  4.00it/s]100%|██████████| 23/23 [00:06<00:00,  3.54it/s]
{'bleu-1': 0.2716010208851576, 'bleu-2': 0.09315572781751888, 'bleu-3': 0.015051394200910721, 'bleu-4': 0.001510219650161923}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_token_mean_1119-s-104384-s-LongLM-7456/test_sen_mse_add_mask_in_input_ablation_token_mean.0
perplexity: 32.93188
begin predicting
acc : 0.7119341563786008
{'distinct-1': 0.029617067041705875, 'distinct-2': 0.37443212603676074, 'distinct-3': 0.7706603109939838, 'distinct-4': 0.943130118289354}
{'bleu-1': 0.22461351472070612, 'bleu-2': 0.07364731050173565, 'bleu-3': 0.011108803347795154, 'bleu-4': 0.0009287033629244647}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_token_mean_1119-s-104384-s-LongLM-7456/test_sen_mse_add_mask_in_input_ablation_token_mean.1
perplexity: 25.261076
begin predicting
acc : 0.9615912208504801
{'distinct-1': 0.015276684708347617, 'distinct-2': 0.2902550585316691, 'distinct-3': 0.6768714594831149, 'distinct-4': 0.8773947668914632}
