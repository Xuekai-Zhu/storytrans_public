0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 1.058 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:01,  1.06s/it]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
26it [00:01, 30.40it/s]55it [00:01, 67.55it/s]86it [00:01, 108.22it/s]117it [00:01, 147.16it/s]151it [00:01, 189.02it/s]186it [00:01, 226.23it/s]219it [00:01, 251.07it/s]251it [00:01, 252.21it/s]281it [00:02, 254.82it/s]310it [00:02, 252.88it/s]344it [00:02, 274.71it/s]377it [00:02, 287.65it/s]408it [00:02, 292.04it/s]442it [00:02, 303.74it/s]474it [00:02, 303.63it/s]505it [00:02, 303.33it/s]536it [00:02, 288.30it/s]566it [00:02, 289.75it/s]599it [00:03, 297.81it/s]631it [00:03, 300.29it/s]667it [00:03, 315.57it/s]700it [00:03, 318.49it/s]729it [00:03, 209.91it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:06<1:14:48,  6.17s/it]  1%|          | 7/729 [00:06<07:57,  1.51it/s]    2%|▏         | 11/729 [00:10<10:33,  1.13it/s]  2%|▏         | 15/729 [00:10<06:27,  1.84it/s]  3%|▎         | 21/729 [00:11<03:34,  3.31it/s]  4%|▎         | 27/729 [00:11<02:13,  5.27it/s]  5%|▍         | 33/729 [00:11<01:28,  7.84it/s]  5%|▌         | 38/729 [00:11<01:10,  9.74it/s]  6%|▌         | 44/729 [00:11<00:50, 13.52it/s]  7%|▋         | 50/729 [00:11<00:37, 18.02it/s]  8%|▊         | 56/729 [00:11<00:29, 22.95it/s]  9%|▊         | 62/729 [00:11<00:23, 28.09it/s]  9%|▉         | 68/729 [00:12<00:19, 33.44it/s] 10%|█         | 74/729 [00:13<00:48, 13.52it/s] 11%|█         | 80/729 [00:13<00:36, 17.58it/s] 12%|█▏        | 86/729 [00:13<00:29, 22.16it/s] 13%|█▎        | 92/729 [00:13<00:23, 27.07it/s] 13%|█▎        | 98/729 [00:13<00:19, 31.99it/s] 14%|█▍        | 104/729 [00:13<00:16, 36.87it/s] 15%|█▌        | 110/729 [00:13<00:15, 41.15it/s] 16%|█▌        | 117/729 [00:13<00:13, 47.05it/s] 17%|█▋        | 124/729 [00:13<00:11, 51.14it/s] 18%|█▊        | 131/729 [00:14<00:10, 54.77it/s] 19%|█▉        | 138/729 [00:14<00:25, 23.00it/s] 20%|█▉        | 144/729 [00:14<00:21, 27.52it/s] 21%|██        | 150/729 [00:14<00:17, 32.23it/s] 21%|██▏       | 156/729 [00:15<00:15, 36.71it/s] 22%|██▏       | 162/729 [00:15<00:14, 39.92it/s] 23%|██▎       | 168/729 [00:15<00:13, 42.92it/s] 24%|██▍       | 174/729 [00:15<00:11, 46.51it/s] 25%|██▍       | 181/729 [00:15<00:10, 50.29it/s] 26%|██▌       | 187/729 [00:15<00:10, 51.80it/s] 26%|██▋       | 193/729 [00:16<00:21, 25.16it/s] 27%|██▋       | 198/729 [00:16<00:19, 27.92it/s] 28%|██▊       | 204/729 [00:16<00:16, 32.62it/s] 29%|██▉       | 210/729 [00:16<00:14, 36.99it/s] 30%|██▉       | 216/729 [00:16<00:12, 40.50it/s] 30%|███       | 222/729 [00:16<00:11, 43.14it/s] 31%|███▏      | 228/729 [00:16<00:10, 45.61it/s] 32%|███▏      | 234/729 [00:16<00:10, 48.15it/s] 33%|███▎      | 240/729 [00:17<00:09, 50.70it/s] 34%|███▍      | 247/729 [00:17<00:09, 53.54it/s] 35%|███▍      | 254/729 [00:17<00:08, 55.76it/s] 36%|███▌      | 260/729 [00:17<00:08, 56.01it/s] 36%|███▋      | 266/729 [00:17<00:08, 56.81it/s] 37%|███▋      | 273/729 [00:17<00:07, 58.23it/s] 38%|███▊      | 280/729 [00:17<00:07, 58.97it/s] 39%|███▉      | 287/729 [00:17<00:07, 59.16it/s] 40%|████      | 294/729 [00:17<00:07, 59.20it/s] 41%|████▏     | 301/729 [00:18<00:07, 59.07it/s] 42%|████▏     | 307/729 [00:18<00:07, 58.81it/s] 43%|████▎     | 313/729 [00:18<00:07, 58.50it/s] 44%|████▍     | 319/729 [00:18<00:07, 58.19it/s] 45%|████▍     | 325/729 [00:18<00:06, 58.26it/s] 45%|████▌     | 331/729 [00:18<00:07, 56.70it/s] 46%|████▌     | 337/729 [00:18<00:06, 57.61it/s] 47%|████▋     | 343/729 [00:18<00:07, 52.80it/s] 48%|████▊     | 350/729 [00:18<00:06, 55.07it/s] 49%|████▉     | 356/729 [00:19<00:06, 55.39it/s] 50%|████▉     | 362/729 [00:19<00:06, 56.09it/s] 50%|█████     | 368/729 [00:19<00:06, 57.18it/s] 51%|█████▏    | 375/729 [00:19<00:06, 58.63it/s] 52%|█████▏    | 382/729 [00:19<00:05, 60.24it/s] 53%|█████▎    | 389/729 [00:19<00:05, 59.86it/s] 54%|█████▍    | 396/729 [00:19<00:05, 60.71it/s] 55%|█████▌    | 403/729 [00:19<00:05, 61.72it/s] 56%|█████▌    | 410/729 [00:19<00:05, 60.10it/s] 57%|█████▋    | 417/729 [00:20<00:05, 59.21it/s] 58%|█████▊    | 424/729 [00:20<00:05, 59.35it/s] 59%|█████▉    | 431/729 [00:20<00:04, 59.84it/s] 60%|█████▉    | 437/729 [00:20<00:04, 59.71it/s] 61%|██████    | 443/729 [00:20<00:04, 59.32it/s] 62%|██████▏   | 449/729 [00:20<00:04, 59.23it/s] 63%|██████▎   | 456/729 [00:20<00:04, 59.90it/s] 64%|██████▎   | 463/729 [00:20<00:04, 60.25it/s] 64%|██████▍   | 470/729 [00:20<00:04, 59.93it/s] 65%|██████▌   | 476/729 [00:21<00:04, 59.50it/s] 66%|██████▌   | 482/729 [00:21<00:04, 58.09it/s] 67%|██████▋   | 488/729 [00:21<00:04, 58.62it/s] 68%|██████▊   | 494/729 [00:21<00:04, 58.46it/s] 69%|██████▊   | 501/729 [00:21<00:03, 59.29it/s] 70%|██████▉   | 507/729 [00:21<00:03, 59.39it/s] 70%|███████   | 513/729 [00:21<00:03, 58.87it/s] 71%|███████   | 519/729 [00:21<00:03, 58.25it/s] 72%|███████▏  | 526/729 [00:21<00:03, 59.27it/s] 73%|███████▎  | 533/729 [00:22<00:03, 59.94it/s] 74%|███████▍  | 539/729 [00:22<00:03, 59.91it/s] 75%|███████▍  | 545/729 [00:22<00:03, 59.86it/s] 76%|███████▌  | 551/729 [00:22<00:03, 54.99it/s] 76%|███████▋  | 557/729 [00:22<00:03, 56.18it/s] 77%|███████▋  | 563/729 [00:22<00:02, 56.87it/s] 78%|███████▊  | 569/729 [00:22<00:02, 56.99it/s] 79%|███████▉  | 575/729 [00:22<00:02, 56.13it/s] 80%|███████▉  | 581/729 [00:22<00:02, 56.62it/s] 81%|████████  | 588/729 [00:23<00:02, 58.25it/s] 82%|████████▏ | 595/729 [00:23<00:02, 59.45it/s] 82%|████████▏ | 601/729 [00:23<00:02, 58.79it/s] 83%|████████▎ | 607/729 [00:23<00:02, 58.91it/s] 84%|████████▍ | 613/729 [00:23<00:01, 59.10it/s] 85%|████████▍ | 619/729 [00:23<00:01, 59.28it/s] 86%|████████▌ | 625/729 [00:23<00:01, 58.85it/s] 87%|████████▋ | 631/729 [00:23<00:01, 59.02it/s] 88%|████████▊ | 638/729 [00:23<00:01, 59.48it/s] 88%|████████▊ | 645/729 [00:23<00:01, 60.75it/s] 89%|████████▉ | 652/729 [00:24<00:01, 60.98it/s] 90%|█████████ | 659/729 [00:24<00:01, 61.17it/s] 91%|█████████▏| 666/729 [00:24<00:01, 60.96it/s] 92%|█████████▏| 673/729 [00:24<00:00, 60.78it/s] 93%|█████████▎| 680/729 [00:24<00:00, 60.32it/s] 94%|█████████▍| 687/729 [00:24<00:00, 60.82it/s] 95%|█████████▌| 694/729 [00:24<00:00, 61.55it/s] 96%|█████████▌| 701/729 [00:24<00:00, 61.62it/s] 97%|█████████▋| 708/729 [00:24<00:00, 61.16it/s] 98%|█████████▊| 715/729 [00:25<00:00, 60.64it/s] 99%|█████████▉| 722/729 [00:25<00:00, 60.44it/s]100%|██████████| 729/729 [00:25<00:00, 60.67it/s]100%|██████████| 729/729 [00:25<00:00, 28.76it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'lm_head.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.final_layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.embed_tokens.weight', 'decoder.block.4.layer.0.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:04,  5.23it/s]  9%|▊         | 2/23 [00:00<00:03,  5.81it/s] 13%|█▎        | 3/23 [00:00<00:03,  5.42it/s] 17%|█▋        | 4/23 [00:00<00:03,  5.77it/s] 22%|██▏       | 5/23 [00:00<00:02,  6.12it/s] 26%|██▌       | 6/23 [00:01<00:02,  6.06it/s] 30%|███       | 7/23 [00:01<00:02,  6.21it/s] 35%|███▍      | 8/23 [00:01<00:02,  5.83it/s] 39%|███▉      | 9/23 [00:01<00:02,  5.59it/s] 43%|████▎     | 10/23 [00:01<00:02,  5.52it/s] 48%|████▊     | 11/23 [00:01<00:02,  5.91it/s] 52%|█████▏    | 12/23 [00:02<00:01,  5.70it/s] 57%|█████▋    | 13/23 [00:02<00:01,  5.64it/s] 61%|██████    | 14/23 [00:02<00:01,  5.98it/s] 65%|██████▌   | 15/23 [00:02<00:01,  6.22it/s] 70%|██████▉   | 16/23 [00:02<00:01,  6.23it/s] 74%|███████▍  | 17/23 [00:02<00:00,  6.61it/s] 78%|███████▊  | 18/23 [00:03<00:00,  6.20it/s] 83%|████████▎ | 19/23 [00:03<00:00,  6.32it/s] 87%|████████▋ | 20/23 [00:03<00:00,  6.49it/s] 91%|█████████▏| 21/23 [00:03<00:00,  6.21it/s] 96%|█████████▌| 22/23 [00:03<00:00,  6.60it/s]100%|██████████| 23/23 [00:03<00:00,  6.19it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
18it [00:00, 177.71it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
40it [00:00, 198.20it/s]63it [00:00, 210.53it/s]87it [00:00, 221.86it/s]112it [00:00, 230.21it/s]136it [00:00, 220.01it/s]160it [00:00, 225.70it/s]183it [00:00, 224.61it/s]206it [00:00, 226.22it/s]230it [00:01, 228.58it/s]255it [00:01, 232.10it/s]279it [00:01, 232.85it/s]303it [00:01, 231.24it/s]328it [00:01, 233.85it/s]353it [00:01, 237.23it/s]377it [00:01, 235.86it/s]401it [00:01, 233.78it/s]425it [00:01, 233.70it/s]450it [00:01, 236.96it/s]474it [00:02, 235.79it/s]499it [00:02, 236.29it/s]523it [00:02, 237.36it/s]548it [00:02, 238.91it/s]575it [00:02, 245.47it/s]601it [00:02, 246.79it/s]626it [00:02, 240.66it/s]651it [00:02, 236.98it/s]675it [00:02, 236.04it/s]700it [00:03, 239.84it/s]726it [00:03, 245.54it/s]729it [00:03, 233.39it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 7/729 [00:00<00:11, 63.86it/s]  2%|▏         | 14/729 [00:00<00:11, 63.11it/s]  3%|▎         | 21/729 [00:00<00:11, 64.29it/s]  4%|▍         | 28/729 [00:00<00:10, 64.95it/s]  5%|▍         | 35/729 [00:00<00:10, 65.64it/s]  6%|▌         | 42/729 [00:00<00:10, 66.27it/s]  7%|▋         | 49/729 [00:00<00:10, 66.44it/s]  8%|▊         | 56/729 [00:00<00:10, 66.31it/s]  9%|▊         | 63/729 [00:00<00:10, 66.58it/s] 10%|▉         | 70/729 [00:01<00:09, 66.53it/s] 11%|█         | 77/729 [00:01<00:09, 66.66it/s] 12%|█▏        | 84/729 [00:01<00:09, 66.40it/s] 12%|█▏        | 91/729 [00:01<00:09, 65.45it/s] 13%|█▎        | 98/729 [00:01<00:09, 65.83it/s] 14%|█▍        | 105/729 [00:01<00:09, 66.07it/s] 15%|█▌        | 112/729 [00:01<00:09, 66.40it/s] 16%|█▋        | 119/729 [00:01<00:09, 65.84it/s] 17%|█▋        | 126/729 [00:01<00:09, 65.52it/s] 18%|█▊        | 133/729 [00:02<00:09, 65.17it/s] 19%|█▉        | 140/729 [00:02<00:08, 65.54it/s] 20%|██        | 147/729 [00:02<00:08, 65.70it/s] 21%|██        | 154/729 [00:02<00:08, 66.05it/s] 22%|██▏       | 161/729 [00:02<00:08, 65.85it/s] 23%|██▎       | 168/729 [00:02<00:08, 66.12it/s] 24%|██▍       | 175/729 [00:02<00:08, 65.87it/s] 25%|██▍       | 182/729 [00:02<00:08, 65.94it/s] 26%|██▌       | 189/729 [00:02<00:08, 65.21it/s] 27%|██▋       | 196/729 [00:02<00:08, 65.49it/s] 28%|██▊       | 203/729 [00:03<00:07, 65.86it/s] 29%|██▉       | 210/729 [00:03<00:07, 65.91it/s] 30%|██▉       | 217/729 [00:03<00:07, 65.03it/s] 31%|███       | 224/729 [00:03<00:07, 66.00it/s] 32%|███▏      | 231/729 [00:03<00:07, 65.56it/s] 33%|███▎      | 238/729 [00:03<00:07, 64.49it/s] 34%|███▎      | 245/729 [00:03<00:07, 63.31it/s] 35%|███▍      | 252/729 [00:03<00:07, 64.18it/s] 36%|███▌      | 259/729 [00:03<00:07, 64.70it/s] 36%|███▋      | 266/729 [00:04<00:07, 65.99it/s] 37%|███▋      | 273/729 [00:04<00:06, 65.82it/s] 38%|███▊      | 280/729 [00:04<00:06, 65.53it/s] 39%|███▉      | 287/729 [00:04<00:06, 65.00it/s] 40%|████      | 294/729 [00:04<00:06, 65.10it/s] 41%|████▏     | 301/729 [00:04<00:06, 64.99it/s] 42%|████▏     | 308/729 [00:04<00:06, 65.19it/s] 43%|████▎     | 315/729 [00:04<00:06, 63.96it/s] 44%|████▍     | 322/729 [00:04<00:06, 63.61it/s] 45%|████▌     | 329/729 [00:05<00:06, 63.32it/s] 46%|████▌     | 336/729 [00:05<00:06, 62.32it/s] 47%|████▋     | 343/729 [00:05<00:06, 62.70it/s] 48%|████▊     | 350/729 [00:05<00:05, 63.19it/s] 49%|████▉     | 357/729 [00:05<00:05, 63.81it/s] 50%|████▉     | 364/729 [00:05<00:05, 64.54it/s] 51%|█████     | 371/729 [00:05<00:05, 64.98it/s] 52%|█████▏    | 378/729 [00:05<00:05, 65.28it/s] 53%|█████▎    | 385/729 [00:05<00:05, 65.29it/s] 54%|█████▍    | 392/729 [00:06<00:05, 65.49it/s] 55%|█████▍    | 399/729 [00:06<00:05, 65.52it/s] 56%|█████▌    | 406/729 [00:06<00:04, 66.04it/s] 57%|█████▋    | 413/729 [00:06<00:04, 65.96it/s] 58%|█████▊    | 420/729 [00:06<00:04, 66.01it/s] 59%|█████▊    | 427/729 [00:06<00:04, 64.79it/s] 60%|█████▉    | 434/729 [00:06<00:04, 64.13it/s] 60%|██████    | 441/729 [00:06<00:04, 63.95it/s] 61%|██████▏   | 448/729 [00:06<00:04, 63.10it/s] 62%|██████▏   | 455/729 [00:06<00:04, 63.47it/s] 63%|██████▎   | 462/729 [00:07<00:04, 63.60it/s] 64%|██████▍   | 469/729 [00:07<00:04, 64.44it/s] 65%|██████▌   | 476/729 [00:07<00:03, 64.05it/s] 66%|██████▋   | 483/729 [00:07<00:03, 63.91it/s] 67%|██████▋   | 490/729 [00:07<00:03, 64.04it/s] 68%|██████▊   | 497/729 [00:07<00:03, 63.75it/s] 69%|██████▉   | 504/729 [00:07<00:03, 62.75it/s] 70%|███████   | 511/729 [00:07<00:03, 63.16it/s] 71%|███████   | 518/729 [00:07<00:03, 63.63it/s] 72%|███████▏  | 525/729 [00:08<00:03, 64.02it/s] 73%|███████▎  | 532/729 [00:08<00:03, 63.27it/s] 74%|███████▍  | 539/729 [00:08<00:03, 63.11it/s] 75%|███████▍  | 546/729 [00:08<00:02, 63.54it/s] 76%|███████▌  | 553/729 [00:08<00:02, 64.07it/s] 77%|███████▋  | 560/729 [00:08<00:02, 64.33it/s] 78%|███████▊  | 567/729 [00:08<00:02, 64.32it/s] 79%|███████▊  | 574/729 [00:08<00:02, 63.76it/s] 80%|███████▉  | 581/729 [00:08<00:02, 63.24it/s] 81%|████████  | 588/729 [00:09<00:02, 63.96it/s] 82%|████████▏ | 595/729 [00:09<00:02, 63.98it/s] 83%|████████▎ | 602/729 [00:09<00:01, 63.98it/s] 84%|████████▎ | 609/729 [00:09<00:01, 63.47it/s] 84%|████████▍ | 616/729 [00:09<00:01, 63.44it/s] 85%|████████▌ | 623/729 [00:09<00:01, 63.51it/s] 86%|████████▋ | 630/729 [00:09<00:01, 64.43it/s] 87%|████████▋ | 637/729 [00:09<00:01, 63.59it/s] 88%|████████▊ | 644/729 [00:09<00:01, 63.97it/s] 89%|████████▉ | 651/729 [00:10<00:01, 63.51it/s] 90%|█████████ | 658/729 [00:10<00:01, 62.89it/s] 91%|█████████ | 665/729 [00:10<00:01, 62.64it/s] 92%|█████████▏| 672/729 [00:10<00:00, 62.11it/s] 93%|█████████▎| 679/729 [00:10<00:00, 62.08it/s] 94%|█████████▍| 686/729 [00:10<00:00, 61.64it/s] 95%|█████████▌| 693/729 [00:10<00:00, 61.96it/s] 96%|█████████▌| 700/729 [00:10<00:00, 62.05it/s] 97%|█████████▋| 707/729 [00:10<00:00, 62.72it/s] 98%|█████████▊| 714/729 [00:11<00:00, 63.40it/s] 99%|█████████▉| 721/729 [00:11<00:00, 63.52it/s]100%|█████████▉| 728/729 [00:11<00:00, 63.39it/s]100%|██████████| 729/729 [00:11<00:00, 64.43it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'lm_head.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.final_layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.embed_tokens.weight', 'decoder.block.4.layer.0.layer_norm.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:06,  3.48it/s]  9%|▊         | 2/23 [00:00<00:05,  4.07it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.27it/s] 17%|█▋        | 4/23 [00:00<00:04,  4.38it/s] 22%|██▏       | 5/23 [00:01<00:04,  4.39it/s] 26%|██▌       | 6/23 [00:01<00:03,  4.53it/s] 30%|███       | 7/23 [00:01<00:03,  4.51it/s] 35%|███▍      | 8/23 [00:01<00:03,  4.47it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.46it/s] 43%|████▎     | 10/23 [00:02<00:02,  4.43it/s] 48%|████▊     | 11/23 [00:02<00:02,  4.43it/s] 52%|█████▏    | 12/23 [00:02<00:02,  4.30it/s] 57%|█████▋    | 13/23 [00:02<00:02,  4.39it/s] 61%|██████    | 14/23 [00:03<00:02,  4.39it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.51it/s] 70%|██████▉   | 16/23 [00:03<00:01,  4.53it/s] 74%|███████▍  | 17/23 [00:03<00:01,  4.64it/s] 78%|███████▊  | 18/23 [00:04<00:01,  4.63it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.67it/s] 87%|████████▋ | 20/23 [00:04<00:00,  4.31it/s] 91%|█████████▏| 21/23 [00:04<00:00,  4.33it/s] 96%|█████████▌| 22/23 [00:04<00:00,  4.46it/s]100%|██████████| 23/23 [00:05<00:00,  4.75it/s]100%|██████████| 23/23 [00:05<00:00,  4.47it/s]
{'bleu-1': 0.2856397606556043, 'bleu-2': 0.09385479627369459, 'bleu-3': 0.012755439923227656, 'bleu-4': 0.0010350938897545423}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-59648-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 31.96815
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.666907 R=0.669004 F=0.667816
begin predicting
acc : 0.5994513031550068
{'distinct-1': 0.0252047462163238, 'distinct-2': 0.35426608944641397, 'distinct-3': 0.7558118595377953, 'distinct-4': 0.9381892549574582}
{'bleu-1': 0.2240881003800269, 'bleu-2': 0.06908083923588275, 'bleu-3': 0.008699149416264727, 'bleu-4': 0.0006088604854859133}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-59648-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 26.38149
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.641305 R=0.676770 F=0.658406
begin predicting
acc : 0.906721536351166
{'distinct-1': 0.016161605727065653, 'distinct-2': 0.29396820788270306, 'distinct-3': 0.6795611738806864, 'distinct-4': 0.8780534949326089}
