0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 0.961 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:00,  1.04it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
35it [00:01, 44.58it/s]76it [00:01, 100.87it/s]111it [00:01, 145.83it/s]147it [00:01, 188.97it/s]181it [00:01, 222.83it/s]217it [00:01, 254.22it/s]254it [00:01, 283.05it/s]289it [00:01, 298.02it/s]324it [00:01, 311.49it/s]359it [00:01, 318.97it/s]394it [00:02, 323.68it/s]428it [00:02, 328.10it/s]462it [00:02, 329.96it/s]497it [00:02, 332.56it/s]534it [00:02, 342.38it/s]569it [00:02, 340.55it/s]604it [00:02, 340.11it/s]639it [00:02, 331.39it/s]675it [00:02, 336.18it/s]711it [00:03, 341.61it/s]729it [00:03, 237.14it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:01<14:58,  1.23s/it]  1%|          | 7/729 [00:01<01:45,  6.83it/s]  2%|▏         | 12/729 [00:01<00:58, 12.30it/s]  2%|▏         | 18/729 [00:01<00:36, 19.37it/s]  3%|▎         | 23/729 [00:01<00:30, 23.49it/s]  4%|▍         | 28/729 [00:01<00:25, 27.48it/s]  5%|▍         | 34/729 [00:01<00:20, 33.69it/s]  5%|▌         | 40/729 [00:02<00:17, 39.10it/s]  6%|▋         | 46/729 [00:02<00:15, 43.30it/s]  7%|▋         | 52/729 [00:02<00:14, 47.00it/s]  8%|▊         | 58/729 [00:02<00:13, 49.61it/s]  9%|▉         | 64/729 [00:02<00:12, 51.35it/s] 10%|▉         | 70/729 [00:02<00:12, 53.20it/s] 11%|█         | 77/729 [00:02<00:11, 57.04it/s] 12%|█▏        | 84/729 [00:02<00:10, 58.92it/s] 12%|█▏        | 91/729 [00:02<00:10, 60.00it/s] 13%|█▎        | 98/729 [00:02<00:10, 60.93it/s] 14%|█▍        | 105/729 [00:03<00:09, 62.43it/s] 15%|█▌        | 112/729 [00:03<00:09, 62.29it/s] 16%|█▋        | 119/729 [00:03<00:09, 62.77it/s] 17%|█▋        | 126/729 [00:03<00:09, 62.88it/s] 18%|█▊        | 133/729 [00:03<00:09, 63.66it/s] 19%|█▉        | 140/729 [00:03<00:09, 64.06it/s] 20%|██        | 147/729 [00:03<00:08, 64.71it/s] 21%|██        | 154/729 [00:03<00:08, 64.24it/s] 22%|██▏       | 161/729 [00:03<00:08, 64.41it/s] 23%|██▎       | 168/729 [00:04<00:08, 64.02it/s] 24%|██▍       | 175/729 [00:04<00:08, 64.45it/s] 25%|██▍       | 182/729 [00:04<00:08, 64.08it/s] 26%|██▌       | 189/729 [00:04<00:08, 64.49it/s] 27%|██▋       | 196/729 [00:04<00:08, 64.16it/s] 28%|██▊       | 203/729 [00:04<00:08, 64.20it/s] 29%|██▉       | 210/729 [00:04<00:08, 64.87it/s] 30%|██▉       | 217/729 [00:04<00:07, 64.37it/s] 31%|███       | 224/729 [00:04<00:07, 64.56it/s] 32%|███▏      | 231/729 [00:05<00:07, 64.85it/s] 33%|███▎      | 238/729 [00:05<00:07, 65.28it/s] 34%|███▎      | 245/729 [00:05<00:07, 63.12it/s] 35%|███▍      | 252/729 [00:05<00:07, 63.71it/s] 36%|███▌      | 259/729 [00:05<00:07, 64.03it/s] 36%|███▋      | 266/729 [00:05<00:07, 64.53it/s] 37%|███▋      | 273/729 [00:05<00:07, 64.30it/s] 38%|███▊      | 280/729 [00:05<00:06, 64.48it/s] 39%|███▉      | 287/729 [00:05<00:06, 64.12it/s] 40%|████      | 294/729 [00:06<00:06, 64.02it/s] 41%|████▏     | 301/729 [00:06<00:06, 63.90it/s] 42%|████▏     | 308/729 [00:06<00:06, 64.23it/s] 43%|████▎     | 315/729 [00:06<00:06, 63.75it/s] 44%|████▍     | 322/729 [00:06<00:06, 63.73it/s] 45%|████▌     | 329/729 [00:06<00:06, 61.91it/s] 46%|████▌     | 336/729 [00:06<00:06, 61.19it/s] 47%|████▋     | 343/729 [00:06<00:06, 58.71it/s] 48%|████▊     | 349/729 [00:06<00:06, 55.07it/s] 49%|████▊     | 355/729 [00:07<00:07, 51.83it/s] 50%|████▉     | 361/729 [00:07<00:07, 51.68it/s] 50%|█████     | 367/729 [00:07<00:07, 51.16it/s] 51%|█████▏    | 374/729 [00:07<00:06, 56.05it/s] 52%|█████▏    | 382/729 [00:07<00:05, 61.52it/s] 53%|█████▎    | 390/729 [00:07<00:05, 65.33it/s] 55%|█████▍    | 398/729 [00:07<00:04, 68.67it/s] 56%|█████▌    | 406/729 [00:07<00:04, 71.31it/s] 57%|█████▋    | 414/729 [00:07<00:04, 72.63it/s] 58%|█████▊    | 422/729 [00:08<00:04, 73.69it/s] 59%|█████▉    | 430/729 [00:08<00:03, 74.84it/s] 60%|██████    | 438/729 [00:08<00:03, 74.70it/s] 61%|██████    | 446/729 [00:08<00:03, 74.33it/s] 62%|██████▏   | 454/729 [00:08<00:03, 74.61it/s] 63%|██████▎   | 462/729 [00:08<00:03, 75.33it/s] 64%|██████▍   | 470/729 [00:08<00:03, 75.73it/s] 66%|██████▌   | 478/729 [00:08<00:03, 76.55it/s] 67%|██████▋   | 486/729 [00:08<00:03, 76.20it/s] 68%|██████▊   | 494/729 [00:09<00:03, 76.68it/s] 69%|██████▉   | 502/729 [00:09<00:02, 77.13it/s] 70%|███████   | 511/729 [00:09<00:02, 78.27it/s] 71%|███████   | 519/729 [00:09<00:02, 77.24it/s] 72%|███████▏  | 527/729 [00:09<00:02, 78.00it/s] 74%|███████▎  | 536/729 [00:09<00:02, 78.83it/s] 75%|███████▍  | 544/729 [00:09<00:03, 61.01it/s] 76%|███████▌  | 551/729 [00:09<00:02, 62.67it/s] 77%|███████▋  | 559/729 [00:09<00:02, 66.92it/s] 78%|███████▊  | 567/729 [00:10<00:02, 69.63it/s] 79%|███████▉  | 575/729 [00:10<00:02, 71.72it/s] 80%|███████▉  | 583/729 [00:10<00:01, 73.13it/s] 81%|████████  | 591/729 [00:10<00:01, 74.60it/s] 82%|████████▏ | 599/729 [00:10<00:01, 75.45it/s] 83%|████████▎ | 607/729 [00:10<00:01, 76.22it/s] 84%|████████▍ | 615/729 [00:10<00:01, 75.93it/s] 85%|████████▌ | 623/729 [00:10<00:01, 76.70it/s] 87%|████████▋ | 631/729 [00:10<00:01, 76.83it/s] 88%|████████▊ | 639/729 [00:11<00:01, 77.34it/s] 89%|████████▉ | 647/729 [00:11<00:01, 77.64it/s] 90%|████████▉ | 655/729 [00:11<00:00, 78.30it/s] 91%|█████████ | 663/729 [00:11<00:00, 78.04it/s] 92%|█████████▏| 671/729 [00:11<00:00, 75.08it/s] 93%|█████████▎| 679/729 [00:11<00:00, 74.62it/s] 94%|█████████▍| 687/729 [00:11<00:00, 75.28it/s] 95%|█████████▌| 695/729 [00:11<00:00, 76.20it/s] 96%|█████████▋| 703/729 [00:11<00:00, 76.97it/s] 98%|█████████▊| 711/729 [00:11<00:00, 77.09it/s] 99%|█████████▊| 719/729 [00:12<00:00, 77.29it/s]100%|█████████▉| 727/729 [00:12<00:00, 76.90it/s]100%|██████████| 729/729 [00:12<00:00, 59.87it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'lm_head.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:04,  5.39it/s]  9%|▊         | 2/23 [00:00<00:02,  7.28it/s] 13%|█▎        | 3/23 [00:00<00:02,  7.78it/s] 17%|█▋        | 4/23 [00:00<00:02,  7.80it/s] 22%|██▏       | 5/23 [00:00<00:02,  8.24it/s] 26%|██▌       | 6/23 [00:00<00:01,  8.57it/s] 30%|███       | 7/23 [00:00<00:01,  8.28it/s] 35%|███▍      | 8/23 [00:01<00:01,  7.81it/s] 39%|███▉      | 9/23 [00:01<00:01,  8.14it/s] 43%|████▎     | 10/23 [00:01<00:01,  8.29it/s] 48%|████▊     | 11/23 [00:01<00:01,  8.21it/s] 52%|█████▏    | 12/23 [00:01<00:01,  8.43it/s] 57%|█████▋    | 13/23 [00:01<00:01,  7.95it/s] 61%|██████    | 14/23 [00:01<00:01,  7.70it/s] 65%|██████▌   | 15/23 [00:01<00:00,  8.12it/s] 70%|██████▉   | 16/23 [00:02<00:00,  7.33it/s] 74%|███████▍  | 17/23 [00:02<00:00,  6.11it/s] 78%|███████▊  | 18/23 [00:02<00:00,  6.47it/s] 83%|████████▎ | 19/23 [00:02<00:00,  6.77it/s] 87%|████████▋ | 20/23 [00:02<00:00,  7.00it/s] 91%|█████████▏| 21/23 [00:02<00:00,  7.42it/s] 96%|█████████▌| 22/23 [00:02<00:00,  7.37it/s]100%|██████████| 23/23 [00:03<00:00,  7.93it/s]100%|██████████| 23/23 [00:03<00:00,  7.60it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
18it [00:00, 169.54it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
37it [00:00, 176.42it/s]56it [00:00, 180.93it/s]76it [00:00, 187.06it/s]95it [00:00, 184.85it/s]118it [00:00, 199.54it/s]139it [00:00, 201.69it/s]162it [00:00, 210.46it/s]184it [00:00, 211.96it/s]207it [00:01, 216.56it/s]229it [00:01, 217.31it/s]254it [00:01, 226.78it/s]277it [00:01, 227.68it/s]300it [00:01, 221.24it/s]323it [00:01, 221.11it/s]347it [00:01, 226.43it/s]370it [00:01, 224.07it/s]393it [00:01, 223.01it/s]417it [00:01, 225.35it/s]441it [00:02, 228.96it/s]464it [00:02, 223.39it/s]489it [00:02, 228.92it/s]512it [00:02, 221.81it/s]535it [00:02, 195.18it/s]556it [00:02, 192.53it/s]576it [00:02, 186.52it/s]595it [00:02, 178.68it/s]614it [00:02, 177.91it/s]634it [00:03, 182.44it/s]655it [00:03, 189.38it/s]675it [00:03, 191.61it/s]695it [00:03, 192.85it/s]716it [00:03, 197.02it/s]729it [00:03, 205.00it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 8/729 [00:00<00:10, 70.57it/s]  2%|▏         | 16/729 [00:00<00:10, 71.08it/s]  3%|▎         | 24/729 [00:00<00:09, 71.35it/s]  4%|▍         | 32/729 [00:00<00:09, 71.12it/s]  5%|▌         | 40/729 [00:00<00:09, 70.62it/s]  7%|▋         | 48/729 [00:00<00:09, 71.33it/s]  8%|▊         | 56/729 [00:00<00:09, 71.29it/s]  9%|▉         | 64/729 [00:00<00:09, 71.81it/s] 10%|▉         | 72/729 [00:01<00:09, 72.20it/s] 11%|█         | 80/729 [00:01<00:08, 72.51it/s] 12%|█▏        | 88/729 [00:01<00:08, 72.52it/s] 13%|█▎        | 96/729 [00:01<00:08, 70.79it/s] 14%|█▍        | 104/729 [00:01<00:08, 71.48it/s] 15%|█▌        | 112/729 [00:01<00:08, 72.19it/s] 16%|█▋        | 120/729 [00:01<00:08, 71.60it/s] 18%|█▊        | 128/729 [00:01<00:08, 70.94it/s] 19%|█▊        | 136/729 [00:01<00:08, 71.26it/s] 20%|█▉        | 144/729 [00:02<00:08, 71.17it/s] 21%|██        | 152/729 [00:02<00:08, 70.14it/s] 22%|██▏       | 160/729 [00:02<00:08, 70.32it/s] 23%|██▎       | 168/729 [00:02<00:07, 70.49it/s] 24%|██▍       | 176/729 [00:02<00:07, 70.91it/s] 25%|██▌       | 184/729 [00:02<00:07, 70.16it/s] 26%|██▋       | 192/729 [00:02<00:07, 70.45it/s] 27%|██▋       | 200/729 [00:02<00:07, 70.67it/s] 29%|██▊       | 208/729 [00:02<00:07, 70.49it/s] 30%|██▉       | 216/729 [00:03<00:07, 70.51it/s] 31%|███       | 224/729 [00:03<00:07, 70.84it/s] 32%|███▏      | 232/729 [00:03<00:07, 70.89it/s] 33%|███▎      | 240/729 [00:03<00:06, 71.48it/s] 34%|███▍      | 248/729 [00:03<00:06, 71.86it/s] 35%|███▌      | 256/729 [00:03<00:06, 72.31it/s] 36%|███▌      | 264/729 [00:03<00:06, 72.28it/s] 37%|███▋      | 272/729 [00:03<00:06, 71.47it/s] 38%|███▊      | 280/729 [00:03<00:06, 71.56it/s] 40%|███▉      | 288/729 [00:04<00:06, 71.25it/s] 41%|████      | 296/729 [00:04<00:06, 69.49it/s] 42%|████▏     | 304/729 [00:04<00:06, 69.78it/s] 43%|████▎     | 312/729 [00:04<00:05, 70.19it/s] 44%|████▍     | 320/729 [00:04<00:05, 69.99it/s] 45%|████▍     | 328/729 [00:04<00:05, 69.99it/s] 46%|████▌     | 336/729 [00:04<00:05, 70.17it/s] 47%|████▋     | 344/729 [00:04<00:05, 70.92it/s] 48%|████▊     | 352/729 [00:04<00:05, 71.07it/s] 49%|████▉     | 360/729 [00:05<00:05, 71.43it/s] 50%|█████     | 368/729 [00:05<00:05, 71.28it/s] 52%|█████▏    | 376/729 [00:05<00:04, 71.10it/s] 53%|█████▎    | 384/729 [00:05<00:04, 71.04it/s] 54%|█████▍    | 392/729 [00:05<00:04, 70.99it/s] 55%|█████▍    | 400/729 [00:05<00:04, 70.82it/s] 56%|█████▌    | 408/729 [00:05<00:04, 71.51it/s] 57%|█████▋    | 416/729 [00:05<00:04, 71.96it/s] 58%|█████▊    | 424/729 [00:05<00:04, 71.40it/s] 59%|█████▉    | 432/729 [00:06<00:04, 72.03it/s] 60%|██████    | 440/729 [00:06<00:04, 71.36it/s] 61%|██████▏   | 448/729 [00:06<00:03, 71.04it/s] 63%|██████▎   | 456/729 [00:06<00:03, 71.01it/s] 64%|██████▎   | 464/729 [00:06<00:03, 70.70it/s] 65%|██████▍   | 472/729 [00:06<00:03, 71.01it/s] 66%|██████▌   | 480/729 [00:06<00:03, 71.73it/s] 67%|██████▋   | 488/729 [00:06<00:03, 71.57it/s] 68%|██████▊   | 496/729 [00:06<00:03, 70.85it/s] 69%|██████▉   | 504/729 [00:07<00:03, 70.37it/s] 70%|███████   | 512/729 [00:07<00:03, 70.33it/s] 71%|███████▏  | 520/729 [00:07<00:03, 69.66it/s] 72%|███████▏  | 527/729 [00:07<00:02, 69.15it/s] 73%|███████▎  | 535/729 [00:07<00:02, 69.77it/s] 74%|███████▍  | 543/729 [00:07<00:02, 70.30it/s] 76%|███████▌  | 551/729 [00:07<00:02, 70.31it/s] 77%|███████▋  | 559/729 [00:07<00:02, 70.90it/s] 78%|███████▊  | 567/729 [00:07<00:02, 70.98it/s] 79%|███████▉  | 575/729 [00:08<00:02, 71.26it/s] 80%|███████▉  | 583/729 [00:08<00:02, 71.45it/s] 81%|████████  | 591/729 [00:08<00:01, 72.49it/s] 82%|████████▏ | 599/729 [00:08<00:01, 72.54it/s] 83%|████████▎ | 607/729 [00:08<00:01, 72.03it/s] 84%|████████▍ | 615/729 [00:08<00:01, 71.40it/s] 85%|████████▌ | 623/729 [00:08<00:01, 72.21it/s] 87%|████████▋ | 631/729 [00:08<00:01, 72.40it/s] 88%|████████▊ | 639/729 [00:08<00:01, 72.20it/s] 89%|████████▉ | 647/729 [00:09<00:01, 72.40it/s] 90%|████████▉ | 655/729 [00:09<00:01, 72.89it/s] 91%|█████████ | 663/729 [00:09<00:00, 73.30it/s] 92%|█████████▏| 671/729 [00:09<00:00, 72.29it/s] 93%|█████████▎| 679/729 [00:09<00:00, 71.96it/s] 94%|█████████▍| 687/729 [00:09<00:00, 72.11it/s] 95%|█████████▌| 695/729 [00:09<00:00, 72.10it/s] 96%|█████████▋| 703/729 [00:09<00:00, 71.62it/s] 98%|█████████▊| 711/729 [00:09<00:00, 71.91it/s] 99%|█████████▊| 719/729 [00:10<00:00, 72.32it/s]100%|█████████▉| 727/729 [00:10<00:00, 70.91it/s]100%|██████████| 729/729 [00:10<00:00, 71.23it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'lm_head.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  3.88it/s]  9%|▊         | 2/23 [00:00<00:05,  4.16it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.32it/s] 17%|█▋        | 4/23 [00:00<00:04,  4.13it/s] 22%|██▏       | 5/23 [00:01<00:04,  4.21it/s] 26%|██▌       | 6/23 [00:01<00:04,  3.68it/s] 30%|███       | 7/23 [00:01<00:04,  3.81it/s] 35%|███▍      | 8/23 [00:02<00:03,  3.78it/s] 39%|███▉      | 9/23 [00:02<00:03,  3.82it/s] 43%|████▎     | 10/23 [00:02<00:04,  3.19it/s] 48%|████▊     | 11/23 [00:03<00:04,  2.85it/s] 52%|█████▏    | 12/23 [00:03<00:03,  3.22it/s] 57%|█████▋    | 13/23 [00:03<00:02,  3.41it/s] 61%|██████    | 14/23 [00:03<00:02,  3.59it/s] 65%|██████▌   | 15/23 [00:04<00:02,  3.70it/s] 70%|██████▉   | 16/23 [00:04<00:01,  3.79it/s] 74%|███████▍  | 17/23 [00:04<00:01,  3.95it/s] 78%|███████▊  | 18/23 [00:04<00:01,  4.08it/s] 83%|████████▎ | 19/23 [00:05<00:00,  4.19it/s] 87%|████████▋ | 20/23 [00:05<00:00,  4.28it/s] 91%|█████████▏| 21/23 [00:05<00:00,  4.34it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.34it/s]100%|██████████| 23/23 [00:05<00:00,  4.66it/s]100%|██████████| 23/23 [00:05<00:00,  3.89it/s]
{'bleu-1': 0.20942701165595692, 'bleu-2': 0.05720202120194367, 'bleu-3': 0.0043992990257946726, 'bleu-4': 0.0001734385220595668}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-27960-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 33.566055
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.643547 R=0.635164 F=0.639154
begin predicting
acc : 0.831275720164609
{'distinct-1': 0.02750569956936587, 'distinct-2': 0.3540073392543743, 'distinct-3': 0.7562763973157816, 'distinct-4': 0.9421545642144841}
{'bleu-1': 0.1847642577002686, 'bleu-2': 0.04852477304283878, 'bleu-3': 0.002556060459285871, 'bleu-4': 0.00014670169197717562}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-27960-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 26.81856
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.621171 R=0.653642 F=0.636761
begin predicting
acc : 0.9588477366255144
{'distinct-1': 0.015848615991087497, 'distinct-2': 0.27876865170049736, 'distinct-3': 0.6457845243621995, 'distinct-4': 0.8481440734458071}
