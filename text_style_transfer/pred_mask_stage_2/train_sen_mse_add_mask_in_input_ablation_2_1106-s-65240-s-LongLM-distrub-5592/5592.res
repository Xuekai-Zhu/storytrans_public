0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 0.943 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:00,  1.05it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
32it [00:01, 41.32it/s]68it [00:01, 91.12it/s]101it [00:01, 134.40it/s]132it [00:01, 170.17it/s]164it [00:01, 203.96it/s]197it [00:01, 234.07it/s]230it [00:01, 257.58it/s]263it [00:01, 276.47it/s]296it [00:01, 290.30it/s]329it [00:01, 295.22it/s]364it [00:02, 309.69it/s]397it [00:02, 311.86it/s]433it [00:02, 323.97it/s]467it [00:02, 318.84it/s]500it [00:02, 310.71it/s]535it [00:02, 320.72it/s]568it [00:02, 322.66it/s]601it [00:02, 324.10it/s]634it [00:02, 323.17it/s]669it [00:03, 328.50it/s]704it [00:03, 334.64it/s]729it [00:03, 229.05it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:04<48:57,  4.04s/it]  0%|          | 3/729 [00:04<13:27,  1.11s/it]  1%|          | 7/729 [00:04<04:31,  2.66it/s]  2%|▏         | 13/729 [00:04<01:57,  6.09it/s]  3%|▎         | 19/729 [00:04<01:08, 10.29it/s]  3%|▎         | 25/729 [00:04<00:46, 15.22it/s]  4%|▍         | 31/729 [00:04<00:33, 20.72it/s]  5%|▌         | 37/729 [00:04<00:26, 26.53it/s]  6%|▌         | 43/729 [00:04<00:21, 32.25it/s]  7%|▋         | 50/729 [00:05<00:17, 38.97it/s]  8%|▊         | 57/729 [00:05<00:14, 45.64it/s]  9%|▉         | 64/729 [00:05<00:12, 51.26it/s] 10%|▉         | 72/729 [00:05<00:11, 56.60it/s] 11%|█         | 79/729 [00:05<00:10, 60.05it/s] 12%|█▏        | 87/729 [00:05<00:10, 62.91it/s] 13%|█▎        | 94/729 [00:05<00:09, 63.80it/s] 14%|█▍        | 101/729 [00:05<00:09, 65.09it/s] 15%|█▍        | 108/729 [00:05<00:09, 66.15it/s] 16%|█▌        | 115/729 [00:06<00:09, 67.23it/s] 17%|█▋        | 122/729 [00:06<00:08, 67.76it/s] 18%|█▊        | 129/729 [00:06<00:08, 68.41it/s] 19%|█▉        | 137/729 [00:06<00:08, 69.20it/s] 20%|█▉        | 145/729 [00:06<00:08, 69.39it/s] 21%|██        | 152/729 [00:06<00:13, 43.68it/s] 22%|██▏       | 159/729 [00:06<00:11, 47.70it/s] 23%|██▎       | 166/729 [00:06<00:10, 52.36it/s] 24%|██▍       | 174/729 [00:07<00:09, 57.13it/s] 25%|██▍       | 181/729 [00:07<00:13, 41.83it/s] 26%|██▌       | 188/729 [00:07<00:11, 47.04it/s] 27%|██▋       | 195/729 [00:07<00:10, 51.52it/s] 28%|██▊       | 202/729 [00:07<00:09, 54.82it/s] 29%|██▊       | 209/729 [00:07<00:09, 57.61it/s] 30%|██▉       | 216/729 [00:07<00:08, 60.09it/s] 31%|███       | 224/729 [00:08<00:07, 63.26it/s] 32%|███▏      | 231/729 [00:08<00:07, 64.67it/s] 33%|███▎      | 238/729 [00:08<00:07, 66.09it/s] 34%|███▎      | 246/729 [00:08<00:07, 67.44it/s] 35%|███▍      | 254/729 [00:08<00:06, 68.39it/s] 36%|███▌      | 261/729 [00:08<00:06, 68.51it/s] 37%|███▋      | 268/729 [00:08<00:06, 68.91it/s] 38%|███▊      | 275/729 [00:08<00:06, 69.00it/s] 39%|███▉      | 283/729 [00:08<00:06, 69.49it/s] 40%|███▉      | 291/729 [00:08<00:06, 69.62it/s] 41%|████      | 298/729 [00:09<00:06, 69.61it/s] 42%|████▏     | 305/729 [00:09<00:06, 69.27it/s] 43%|████▎     | 312/729 [00:09<00:06, 69.11it/s] 44%|████▍     | 319/729 [00:09<00:05, 68.62it/s] 45%|████▍     | 326/729 [00:09<00:05, 67.96it/s] 46%|████▌     | 333/729 [00:09<00:05, 67.14it/s] 47%|████▋     | 340/729 [00:09<00:05, 67.97it/s] 48%|████▊     | 347/729 [00:09<00:05, 68.56it/s] 49%|████▊     | 355/729 [00:09<00:05, 69.53it/s] 50%|████▉     | 363/729 [00:10<00:05, 70.01it/s] 51%|█████     | 371/729 [00:10<00:05, 70.38it/s] 52%|█████▏    | 379/729 [00:10<00:04, 70.14it/s] 53%|█████▎    | 387/729 [00:10<00:04, 69.01it/s] 54%|█████▍    | 394/729 [00:10<00:05, 64.90it/s] 55%|█████▌    | 401/729 [00:10<00:05, 62.14it/s] 56%|█████▌    | 409/729 [00:10<00:04, 64.41it/s] 57%|█████▋    | 417/729 [00:10<00:04, 66.19it/s] 58%|█████▊    | 425/729 [00:10<00:04, 67.82it/s] 59%|█████▉    | 433/729 [00:11<00:04, 68.95it/s] 60%|██████    | 441/729 [00:11<00:04, 69.42it/s] 62%|██████▏   | 449/729 [00:11<00:03, 70.01it/s] 63%|██████▎   | 457/729 [00:11<00:03, 70.21it/s] 64%|██████▍   | 465/729 [00:11<00:03, 70.66it/s] 65%|██████▍   | 473/729 [00:11<00:03, 70.90it/s] 66%|██████▌   | 481/729 [00:11<00:03, 69.73it/s] 67%|██████▋   | 489/729 [00:11<00:03, 69.95it/s] 68%|██████▊   | 497/729 [00:11<00:03, 69.01it/s] 69%|██████▉   | 504/729 [00:12<00:03, 68.40it/s] 70%|███████   | 512/729 [00:12<00:03, 69.41it/s] 71%|███████▏  | 520/729 [00:12<00:02, 70.02it/s] 72%|███████▏  | 528/729 [00:12<00:02, 70.48it/s] 74%|███████▎  | 536/729 [00:12<00:02, 71.02it/s] 75%|███████▍  | 544/729 [00:12<00:02, 67.96it/s] 76%|███████▌  | 552/729 [00:12<00:02, 68.97it/s] 77%|███████▋  | 560/729 [00:12<00:02, 69.30it/s] 78%|███████▊  | 568/729 [00:12<00:02, 69.88it/s] 79%|███████▉  | 576/729 [00:13<00:02, 69.78it/s] 80%|███████▉  | 583/729 [00:13<00:02, 69.34it/s] 81%|████████  | 590/729 [00:13<00:02, 69.09it/s] 82%|████████▏ | 598/729 [00:13<00:01, 69.60it/s] 83%|████████▎ | 606/729 [00:13<00:01, 69.91it/s] 84%|████████▍ | 614/729 [00:13<00:01, 70.12it/s] 85%|████████▌ | 622/729 [00:13<00:01, 70.20it/s] 86%|████████▋ | 630/729 [00:13<00:01, 70.29it/s] 88%|████████▊ | 638/729 [00:13<00:01, 70.80it/s] 89%|████████▊ | 646/729 [00:14<00:01, 71.22it/s] 90%|████████▉ | 654/729 [00:14<00:01, 71.89it/s] 91%|█████████ | 662/729 [00:14<00:00, 71.32it/s] 92%|█████████▏| 670/729 [00:14<00:00, 70.36it/s] 93%|█████████▎| 678/729 [00:14<00:00, 69.76it/s] 94%|█████████▍| 685/729 [00:14<00:00, 69.37it/s] 95%|█████████▌| 693/729 [00:14<00:00, 69.81it/s] 96%|█████████▌| 700/729 [00:14<00:00, 69.40it/s] 97%|█████████▋| 707/729 [00:14<00:00, 68.99it/s] 98%|█████████▊| 714/729 [00:15<00:00, 67.90it/s] 99%|█████████▉| 721/729 [00:15<00:00, 67.30it/s]100%|█████████▉| 728/729 [00:15<00:00, 66.32it/s]100%|██████████| 729/729 [00:15<00:00, 47.60it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'lm_head.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:04,  5.05it/s]  9%|▊         | 2/23 [00:00<00:04,  4.40it/s] 13%|█▎        | 3/23 [00:02<00:23,  1.20s/it] 17%|█▋        | 4/23 [00:02<00:14,  1.27it/s] 22%|██▏       | 5/23 [00:03<00:12,  1.45it/s] 26%|██▌       | 6/23 [00:03<00:08,  2.01it/s] 30%|███       | 7/23 [00:03<00:06,  2.58it/s] 35%|███▍      | 8/23 [00:03<00:04,  3.18it/s] 39%|███▉      | 9/23 [00:04<00:03,  3.89it/s] 43%|████▎     | 10/23 [00:04<00:02,  4.43it/s] 48%|████▊     | 11/23 [00:04<00:02,  5.03it/s] 52%|█████▏    | 12/23 [00:04<00:02,  5.38it/s] 57%|█████▋    | 13/23 [00:04<00:01,  5.76it/s] 61%|██████    | 14/23 [00:04<00:01,  5.91it/s] 65%|██████▌   | 15/23 [00:04<00:01,  6.16it/s] 70%|██████▉   | 16/23 [00:05<00:01,  6.38it/s] 74%|███████▍  | 17/23 [00:05<00:00,  6.54it/s] 78%|███████▊  | 18/23 [00:05<00:00,  6.40it/s] 83%|████████▎ | 19/23 [00:05<00:00,  6.54it/s] 87%|████████▋ | 20/23 [00:05<00:00,  6.62it/s] 91%|█████████▏| 21/23 [00:05<00:00,  6.88it/s] 96%|█████████▌| 22/23 [00:05<00:00,  7.10it/s]100%|██████████| 23/23 [00:06<00:00,  7.54it/s]100%|██████████| 23/23 [00:06<00:00,  3.79it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
21it [00:00, 206.64it/s]45it [00:00, 224.13it/s]70it [00:00, 233.59it/s]94it [00:00, 228.07it/s]118it [00:00, 230.71it/s]143it [00:00, 233.89it/s]167it [00:00, 233.41it/s]191it [00:00, 232.08it/s]215it [00:00, 227.10it/s]238it [00:01, 226.96it/s]261it [00:01, 225.97it/s]286it [00:01, 230.73it/s]310it [00:01, 232.38it/s]334it [00:01, 227.04it/s]357it [00:01, 223.89it/s]381it [00:01, 227.73it/s]404it [00:01, 227.41it/s]428it [00:01, 229.95it/s]452it [00:01, 228.84it/s]475it [00:02, 229.09it/s]498it [00:02, 229.20it/s]523it [00:02, 233.44it/s]548it [00:02, 237.62it/s]572it [00:02, 236.42it/s]597it [00:02, 238.65it/s]621it [00:02, 232.69it/s]645it [00:02, 232.73it/s]670it [00:02, 237.33it/s]694it [00:03, 230.98it/s]718it [00:03, 224.82it/s]729it [00:03, 229.34it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 8/729 [00:00<00:09, 74.33it/s]  2%|▏         | 16/729 [00:00<00:09, 73.85it/s]  3%|▎         | 24/729 [00:00<00:09, 76.38it/s]  4%|▍         | 32/729 [00:00<00:09, 77.00it/s]  6%|▌         | 41/729 [00:00<00:08, 78.36it/s]  7%|▋         | 49/729 [00:00<00:08, 77.85it/s]  8%|▊         | 57/729 [00:00<00:08, 77.79it/s]  9%|▉         | 65/729 [00:00<00:08, 76.87it/s] 10%|█         | 73/729 [00:00<00:08, 77.31it/s] 11%|█         | 81/729 [00:01<00:08, 76.08it/s] 12%|█▏        | 89/729 [00:01<00:08, 76.92it/s] 13%|█▎        | 97/729 [00:01<00:08, 77.34it/s] 14%|█▍        | 105/729 [00:01<00:08, 77.68it/s] 16%|█▌        | 113/729 [00:01<00:08, 76.76it/s] 17%|█▋        | 121/729 [00:01<00:07, 76.29it/s] 18%|█▊        | 129/729 [00:01<00:07, 76.09it/s] 19%|█▉        | 137/729 [00:01<00:07, 76.55it/s] 20%|█▉        | 145/729 [00:01<00:07, 75.05it/s] 21%|██        | 153/729 [00:02<00:08, 66.67it/s] 22%|██▏       | 160/729 [00:02<00:08, 65.47it/s] 23%|██▎       | 167/729 [00:02<00:08, 64.37it/s] 24%|██▍       | 174/729 [00:02<00:08, 63.92it/s] 25%|██▍       | 181/729 [00:02<00:08, 64.40it/s] 26%|██▌       | 188/729 [00:02<00:08, 64.33it/s] 27%|██▋       | 196/729 [00:02<00:07, 67.31it/s] 28%|██▊       | 204/729 [00:02<00:07, 69.83it/s] 29%|██▉       | 212/729 [00:02<00:07, 72.21it/s] 30%|███       | 221/729 [00:03<00:06, 75.11it/s] 31%|███▏      | 229/729 [00:03<00:06, 76.44it/s] 33%|███▎      | 237/729 [00:03<00:06, 76.79it/s] 34%|███▎      | 245/729 [00:03<00:06, 77.55it/s] 35%|███▍      | 253/729 [00:03<00:06, 77.98it/s] 36%|███▌      | 261/729 [00:03<00:06, 69.22it/s] 37%|███▋      | 269/729 [00:03<00:06, 65.90it/s] 38%|███▊      | 276/729 [00:03<00:06, 66.16it/s] 39%|███▉      | 283/729 [00:03<00:06, 65.24it/s] 40%|███▉      | 290/729 [00:04<00:06, 65.71it/s] 41%|████      | 297/729 [00:04<00:06, 65.43it/s] 42%|████▏     | 304/729 [00:04<00:06, 65.36it/s] 43%|████▎     | 311/729 [00:04<00:06, 65.42it/s] 44%|████▎     | 318/729 [00:04<00:06, 64.91it/s] 45%|████▍     | 325/729 [00:04<00:06, 64.60it/s] 46%|████▌     | 332/729 [00:04<00:06, 63.99it/s] 47%|████▋     | 339/729 [00:04<00:06, 60.45it/s] 47%|████▋     | 346/729 [00:04<00:06, 60.25it/s] 48%|████▊     | 353/729 [00:05<00:06, 61.62it/s] 49%|████▉     | 360/729 [00:05<00:05, 62.21it/s] 50%|█████     | 367/729 [00:05<00:05, 63.14it/s] 51%|█████▏    | 374/729 [00:05<00:05, 63.86it/s] 52%|█████▏    | 381/729 [00:05<00:05, 62.81it/s] 53%|█████▎    | 388/729 [00:05<00:05, 61.64it/s] 54%|█████▍    | 396/729 [00:05<00:05, 65.18it/s] 55%|█████▌    | 404/729 [00:05<00:04, 68.58it/s] 57%|█████▋    | 412/729 [00:05<00:04, 71.42it/s] 58%|█████▊    | 421/729 [00:06<00:04, 73.63it/s] 59%|█████▉    | 429/729 [00:06<00:03, 75.15it/s] 60%|█████▉    | 437/729 [00:06<00:03, 75.61it/s] 61%|██████    | 445/729 [00:06<00:03, 76.65it/s] 62%|██████▏   | 453/729 [00:06<00:03, 77.47it/s] 63%|██████▎   | 461/729 [00:06<00:03, 77.69it/s] 64%|██████▍   | 469/729 [00:06<00:03, 72.38it/s] 65%|██████▌   | 477/729 [00:06<00:03, 65.78it/s] 66%|██████▋   | 484/729 [00:06<00:04, 59.24it/s] 67%|██████▋   | 491/729 [00:07<00:03, 60.11it/s] 68%|██████▊   | 498/729 [00:07<00:03, 61.81it/s] 69%|██████▉   | 505/729 [00:07<00:03, 62.27it/s] 70%|███████   | 512/729 [00:07<00:03, 62.95it/s] 71%|███████   | 519/729 [00:07<00:03, 62.46it/s] 72%|███████▏  | 526/729 [00:07<00:03, 62.51it/s] 73%|███████▎  | 533/729 [00:07<00:03, 63.70it/s] 74%|███████▍  | 540/729 [00:07<00:02, 64.00it/s] 75%|███████▌  | 547/729 [00:07<00:03, 59.52it/s] 76%|███████▌  | 554/729 [00:08<00:02, 60.59it/s] 77%|███████▋  | 561/729 [00:08<00:02, 62.36it/s] 78%|███████▊  | 568/729 [00:08<00:02, 63.12it/s] 79%|███████▉  | 575/729 [00:08<00:02, 63.64it/s] 80%|███████▉  | 582/729 [00:08<00:02, 64.68it/s] 81%|████████  | 589/729 [00:08<00:02, 65.06it/s] 82%|████████▏ | 597/729 [00:08<00:01, 69.26it/s] 83%|████████▎ | 604/729 [00:08<00:01, 68.10it/s] 84%|████████▍ | 612/729 [00:08<00:01, 71.39it/s] 85%|████████▌ | 621/729 [00:09<00:01, 74.21it/s] 86%|████████▋ | 630/729 [00:09<00:01, 76.73it/s] 88%|████████▊ | 638/729 [00:09<00:01, 76.67it/s] 89%|████████▊ | 646/729 [00:09<00:01, 76.83it/s] 90%|████████▉ | 654/729 [00:09<00:00, 77.25it/s] 91%|█████████ | 662/729 [00:09<00:01, 64.50it/s] 92%|█████████▏| 669/729 [00:09<00:00, 61.34it/s] 93%|█████████▎| 676/729 [00:09<00:00, 57.36it/s] 94%|█████████▎| 683/729 [00:10<00:00, 59.21it/s] 95%|█████████▍| 690/729 [00:10<00:00, 60.35it/s] 96%|█████████▌| 697/729 [00:10<00:00, 61.37it/s] 97%|█████████▋| 704/729 [00:10<00:00, 62.02it/s] 98%|█████████▊| 711/729 [00:10<00:00, 62.16it/s] 99%|█████████▊| 719/729 [00:10<00:00, 64.78it/s]100%|█████████▉| 727/729 [00:10<00:00, 64.36it/s]100%|██████████| 729/729 [00:10<00:00, 67.93it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'lm_head.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  3.87it/s]  9%|▊         | 2/23 [00:00<00:05,  4.00it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.26it/s] 17%|█▋        | 4/23 [00:00<00:04,  4.29it/s] 22%|██▏       | 5/23 [00:01<00:04,  4.50it/s] 26%|██▌       | 6/23 [00:01<00:04,  4.24it/s] 30%|███       | 7/23 [00:01<00:03,  4.06it/s] 35%|███▍      | 8/23 [00:01<00:03,  4.25it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.41it/s] 43%|████▎     | 10/23 [00:02<00:02,  4.56it/s] 48%|████▊     | 11/23 [00:02<00:02,  4.56it/s] 52%|█████▏    | 12/23 [00:02<00:02,  4.49it/s] 57%|█████▋    | 13/23 [00:02<00:02,  4.53it/s] 61%|██████    | 14/23 [00:03<00:02,  4.35it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.38it/s] 70%|██████▉   | 16/23 [00:03<00:01,  4.42it/s] 74%|███████▍  | 17/23 [00:03<00:01,  4.50it/s] 78%|███████▊  | 18/23 [00:04<00:01,  4.55it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.50it/s] 87%|████████▋ | 20/23 [00:04<00:00,  4.48it/s] 91%|█████████▏| 21/23 [00:04<00:00,  4.49it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.37it/s]100%|██████████| 23/23 [00:05<00:00,  4.62it/s]100%|██████████| 23/23 [00:05<00:00,  4.42it/s]
{'bleu-1': 0.23963399786276274, 'bleu-2': 0.07052513255334238, 'bleu-3': 0.008124086027187723, 'bleu-4': 0.0003563596002417458}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-65240-s-LongLM-distrub-5592/test_sen_mse_add_mask_in_input.0
perplexity: 30.940704
begin predicting
acc : 0.8737997256515775
{'bleu-1': 0.1814280229418461, 'bleu-2': 0.0475624936803802, 'bleu-3': 0.0032975760123322958, 'bleu-4': 0.00017902287394765668}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-65240-s-LongLM-distrub-5592/test_sen_mse_add_mask_in_input.1
perplexity: 24.500551
begin predicting
acc : 0.9917695473251029
