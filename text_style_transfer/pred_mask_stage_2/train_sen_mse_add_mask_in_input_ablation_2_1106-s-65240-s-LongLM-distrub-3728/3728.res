0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 1.044 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:01,  1.05s/it]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
28it [00:01, 33.12it/s]63it [00:01, 79.21it/s]98it [00:01, 125.08it/s]132it [00:01, 166.94it/s]166it [00:01, 204.63it/s]200it [00:01, 236.10it/s]235it [00:01, 264.51it/s]270it [00:01, 285.74it/s]304it [00:01, 295.75it/s]340it [00:02, 313.14it/s]375it [00:02, 323.59it/s]410it [00:02, 293.71it/s]442it [00:02, 281.19it/s]472it [00:02, 272.66it/s]501it [00:02, 264.89it/s]529it [00:02, 268.39it/s]557it [00:02, 268.25it/s]585it [00:02, 267.91it/s]617it [00:03, 281.57it/s]649it [00:03, 290.70it/s]684it [00:03, 306.71it/s]721it [00:03, 323.48it/s]729it [00:03, 213.42it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:03<38:31,  3.18s/it]  0%|          | 3/729 [00:03<10:34,  1.14it/s]  1%|          | 6/729 [00:03<04:47,  2.51it/s]  2%|▏         | 12/729 [00:03<01:52,  6.40it/s]  3%|▎         | 19/729 [00:03<00:59, 11.94it/s]  3%|▎         | 24/729 [00:05<02:22,  4.96it/s]  4%|▍         | 31/729 [00:06<01:28,  7.93it/s]  5%|▌         | 38/729 [00:06<00:58, 11.74it/s]  6%|▌         | 44/729 [00:06<00:44, 15.45it/s]  7%|▋         | 51/729 [00:06<00:32, 20.93it/s]  8%|▊         | 58/729 [00:06<00:24, 27.08it/s]  9%|▉         | 65/729 [00:06<00:19, 33.64it/s] 10%|▉         | 72/729 [00:06<00:16, 40.14it/s] 11%|█         | 79/729 [00:06<00:14, 45.91it/s] 12%|█▏        | 86/729 [00:06<00:12, 51.18it/s] 13%|█▎        | 93/729 [00:08<01:04,  9.85it/s] 14%|█▎        | 100/729 [00:09<00:47, 13.21it/s] 15%|█▍        | 107/729 [00:09<00:35, 17.44it/s] 16%|█▌        | 114/729 [00:09<00:27, 22.43it/s] 17%|█▋        | 121/729 [00:09<00:21, 27.99it/s] 18%|█▊        | 128/729 [00:09<00:17, 33.98it/s] 19%|█▊        | 135/729 [00:09<00:14, 39.80it/s] 20%|█▉        | 143/729 [00:09<00:12, 47.54it/s] 21%|██        | 151/729 [00:11<00:58,  9.93it/s] 21%|██▏       | 156/729 [00:11<00:47, 12.04it/s] 22%|██▏       | 163/729 [00:12<00:35, 16.10it/s] 23%|██▎       | 170/729 [00:12<00:26, 21.00it/s] 24%|██▍       | 177/729 [00:12<00:20, 26.59it/s] 25%|██▌       | 184/729 [00:12<00:16, 32.66it/s] 26%|██▌       | 191/729 [00:12<00:13, 38.48it/s] 27%|██▋       | 198/729 [00:12<00:12, 43.93it/s] 28%|██▊       | 205/729 [00:12<00:10, 49.39it/s] 29%|██▉       | 213/729 [00:12<00:09, 55.67it/s] 30%|███       | 221/729 [00:12<00:08, 61.37it/s] 31%|███▏      | 229/729 [00:12<00:07, 66.19it/s] 33%|███▎      | 238/729 [00:13<00:06, 70.90it/s] 34%|███▍      | 247/729 [00:13<00:06, 74.37it/s] 35%|███▌      | 256/729 [00:14<00:29, 15.85it/s] 36%|███▌      | 262/729 [00:14<00:24, 18.85it/s] 37%|███▋      | 268/729 [00:14<00:20, 22.43it/s] 38%|███▊      | 275/729 [00:15<00:16, 27.57it/s] 39%|███▊      | 282/729 [00:15<00:13, 33.33it/s] 40%|███▉      | 289/729 [00:15<00:11, 39.10it/s] 41%|████      | 296/729 [00:15<00:09, 44.17it/s] 42%|████▏     | 303/729 [00:15<00:08, 48.75it/s] 43%|████▎     | 310/729 [00:15<00:07, 52.90it/s] 44%|████▎     | 318/729 [00:15<00:07, 58.03it/s] 45%|████▍     | 327/729 [00:15<00:06, 64.22it/s] 46%|████▌     | 336/729 [00:15<00:05, 68.68it/s] 47%|████▋     | 344/729 [00:16<00:05, 70.86it/s] 48%|████▊     | 352/729 [00:16<00:05, 72.15it/s] 49%|████▉     | 360/729 [00:16<00:05, 72.01it/s] 50%|█████     | 368/729 [00:16<00:05, 63.92it/s] 51%|█████▏    | 375/729 [00:16<00:05, 64.22it/s] 53%|█████▎    | 383/729 [00:16<00:05, 68.05it/s] 54%|█████▎    | 391/729 [00:16<00:04, 70.53it/s] 55%|█████▍    | 399/729 [00:16<00:04, 72.72it/s] 56%|█████▌    | 408/729 [00:16<00:04, 75.08it/s] 57%|█████▋    | 417/729 [00:17<00:04, 76.63it/s] 58%|█████▊    | 426/729 [00:17<00:03, 78.15it/s] 60%|█████▉    | 435/729 [00:17<00:03, 78.93it/s] 61%|██████    | 443/729 [00:17<00:03, 75.32it/s] 62%|██████▏   | 451/729 [00:17<00:03, 74.70it/s] 63%|██████▎   | 459/729 [00:17<00:03, 74.73it/s] 64%|██████▍   | 467/729 [00:17<00:03, 75.15it/s] 65%|██████▌   | 475/729 [00:17<00:03, 76.36it/s] 66%|██████▋   | 483/729 [00:17<00:03, 77.38it/s] 67%|██████▋   | 492/729 [00:18<00:03, 78.69it/s] 69%|██████▊   | 501/729 [00:18<00:02, 80.05it/s] 70%|██████▉   | 510/729 [00:18<00:02, 81.24it/s] 71%|███████   | 519/729 [00:18<00:02, 79.82it/s] 72%|███████▏  | 527/729 [00:18<00:02, 71.71it/s] 73%|███████▎  | 535/729 [00:18<00:02, 71.32it/s] 74%|███████▍  | 543/729 [00:18<00:02, 66.37it/s] 75%|███████▌  | 550/729 [00:18<00:02, 67.21it/s] 76%|███████▋  | 557/729 [00:18<00:02, 67.52it/s] 77%|███████▋  | 564/729 [00:19<00:02, 67.59it/s] 78%|███████▊  | 571/729 [00:19<00:02, 68.04it/s] 79%|███████▉  | 578/729 [00:19<00:02, 67.56it/s] 80%|████████  | 585/729 [00:19<00:02, 67.94it/s] 81%|████████▏ | 594/729 [00:19<00:01, 72.93it/s] 83%|████████▎ | 603/729 [00:19<00:01, 75.55it/s] 84%|████████▍ | 611/729 [00:19<00:01, 76.67it/s] 85%|████████▌ | 620/729 [00:19<00:01, 77.59it/s] 86%|████████▋ | 629/729 [00:19<00:01, 79.52it/s] 88%|████████▊ | 638/729 [00:20<00:01, 80.44it/s] 89%|████████▉ | 647/729 [00:20<00:01, 80.78it/s] 90%|████████▉ | 656/729 [00:20<00:00, 77.41it/s] 91%|█████████ | 664/729 [00:20<00:00, 74.09it/s] 92%|█████████▏| 672/729 [00:20<00:00, 75.68it/s] 93%|█████████▎| 680/729 [00:20<00:00, 76.57it/s] 94%|█████████▍| 688/729 [00:20<00:00, 77.03it/s] 95%|█████████▌| 696/729 [00:20<00:00, 77.28it/s] 97%|█████████▋| 704/729 [00:20<00:00, 77.24it/s] 98%|█████████▊| 712/729 [00:20<00:00, 77.68it/s] 99%|█████████▉| 720/729 [00:21<00:00, 78.01it/s]100%|█████████▉| 728/729 [00:21<00:00, 77.52it/s]100%|██████████| 729/729 [00:21<00:00, 34.37it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.final_layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'lm_head.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:08,  2.47it/s]  9%|▊         | 2/23 [00:02<00:33,  1.59s/it] 13%|█▎        | 3/23 [00:03<00:19,  1.04it/s] 17%|█▋        | 4/23 [00:03<00:12,  1.56it/s] 22%|██▏       | 5/23 [00:03<00:10,  1.78it/s] 26%|██▌       | 6/23 [00:03<00:06,  2.43it/s] 30%|███       | 7/23 [00:03<00:05,  3.12it/s] 35%|███▍      | 8/23 [00:04<00:04,  3.72it/s] 39%|███▉      | 9/23 [00:04<00:03,  4.44it/s] 43%|████▎     | 10/23 [00:04<00:02,  4.90it/s] 48%|████▊     | 11/23 [00:04<00:02,  5.38it/s] 52%|█████▏    | 12/23 [00:04<00:01,  5.63it/s] 57%|█████▋    | 13/23 [00:04<00:01,  5.90it/s] 61%|██████    | 14/23 [00:04<00:01,  6.40it/s] 65%|██████▌   | 15/23 [00:05<00:01,  6.51it/s] 70%|██████▉   | 16/23 [00:05<00:01,  6.75it/s] 74%|███████▍  | 17/23 [00:05<00:00,  6.86it/s] 78%|███████▊  | 18/23 [00:05<00:00,  6.67it/s] 83%|████████▎ | 19/23 [00:05<00:00,  6.72it/s] 87%|████████▋ | 20/23 [00:05<00:00,  6.62it/s] 91%|█████████▏| 21/23 [00:05<00:00,  6.87it/s] 96%|█████████▌| 22/23 [00:06<00:00,  7.04it/s]100%|██████████| 23/23 [00:06<00:00,  7.28it/s]100%|██████████| 23/23 [00:06<00:00,  3.73it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
21it [00:00, 205.10it/s]44it [00:00, 217.53it/s]69it [00:00, 229.42it/s]93it [00:00, 233.46it/s]117it [00:00, 235.17it/s]141it [00:00, 233.01it/s]165it [00:00, 233.71it/s]189it [00:00, 231.87it/s]214it [00:00, 233.19it/s]238it [00:01, 234.07it/s]262it [00:01, 232.87it/s]286it [00:01, 232.18it/s]311it [00:01, 235.62it/s]335it [00:01, 232.63it/s]359it [00:01, 225.26it/s]382it [00:01, 190.68it/s]402it [00:01, 188.36it/s]422it [00:01, 189.00it/s]442it [00:02, 189.01it/s]462it [00:02, 186.09it/s]482it [00:02, 187.24it/s]504it [00:02, 196.41it/s]526it [00:02, 199.37it/s]547it [00:02, 182.71it/s]567it [00:02, 186.69it/s]587it [00:02, 188.81it/s]607it [00:02, 189.74it/s]627it [00:03, 190.02it/s]647it [00:03, 188.66it/s]667it [00:03, 191.47it/s]690it [00:03, 202.36it/s]716it [00:03, 216.52it/s]729it [00:03, 208.91it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 8/729 [00:00<00:10, 70.69it/s]  2%|▏         | 16/729 [00:00<00:09, 71.87it/s]  3%|▎         | 24/729 [00:00<00:09, 72.48it/s]  4%|▍         | 32/729 [00:00<00:09, 72.97it/s]  5%|▌         | 40/729 [00:00<00:11, 60.22it/s]  6%|▋         | 47/729 [00:00<00:11, 58.99it/s]  7%|▋         | 54/729 [00:00<00:11, 59.45it/s]  8%|▊         | 61/729 [00:00<00:10, 60.78it/s]  9%|▉         | 68/729 [00:01<00:10, 60.62it/s] 10%|█         | 75/729 [00:01<00:10, 61.47it/s] 11%|█         | 82/729 [00:01<00:10, 60.95it/s] 12%|█▏        | 89/729 [00:01<00:10, 61.81it/s] 13%|█▎        | 97/729 [00:01<00:09, 65.95it/s] 14%|█▍        | 105/729 [00:01<00:09, 68.21it/s] 16%|█▌        | 113/729 [00:01<00:08, 70.46it/s] 17%|█▋        | 121/729 [00:01<00:08, 72.57it/s] 18%|█▊        | 129/729 [00:01<00:08, 73.84it/s] 19%|█▉        | 137/729 [00:02<00:07, 74.48it/s] 20%|█▉        | 145/729 [00:02<00:07, 74.90it/s] 21%|██        | 153/729 [00:02<00:07, 74.76it/s] 22%|██▏       | 161/729 [00:02<00:07, 75.35it/s] 23%|██▎       | 169/729 [00:02<00:07, 75.12it/s] 24%|██▍       | 177/729 [00:02<00:08, 63.06it/s] 25%|██▌       | 184/729 [00:02<00:09, 59.25it/s] 26%|██▌       | 191/729 [00:02<00:09, 59.73it/s] 27%|██▋       | 198/729 [00:03<00:08, 60.13it/s] 28%|██▊       | 205/729 [00:03<00:08, 60.28it/s] 29%|██▉       | 212/729 [00:03<00:08, 60.89it/s] 30%|███       | 219/729 [00:03<00:08, 61.51it/s] 31%|███       | 227/729 [00:03<00:07, 64.98it/s] 32%|███▏      | 235/729 [00:03<00:07, 67.32it/s] 33%|███▎      | 243/729 [00:03<00:07, 68.60it/s] 34%|███▍      | 251/729 [00:03<00:06, 69.51it/s] 36%|███▌      | 259/729 [00:03<00:06, 70.86it/s] 37%|███▋      | 267/729 [00:04<00:06, 72.59it/s] 38%|███▊      | 275/729 [00:04<00:06, 73.49it/s] 39%|███▉      | 283/729 [00:04<00:06, 69.37it/s] 40%|███▉      | 290/729 [00:04<00:06, 67.37it/s] 41%|████      | 298/729 [00:04<00:06, 69.34it/s] 42%|████▏     | 306/729 [00:04<00:06, 70.26it/s] 43%|████▎     | 314/729 [00:04<00:05, 72.48it/s] 44%|████▍     | 322/729 [00:04<00:05, 74.05it/s] 45%|████▌     | 330/729 [00:04<00:05, 74.22it/s] 46%|████▋     | 338/729 [00:04<00:05, 74.52it/s] 47%|████▋     | 346/729 [00:05<00:05, 75.44it/s] 49%|████▊     | 354/729 [00:05<00:05, 73.32it/s] 50%|████▉     | 362/729 [00:05<00:05, 64.10it/s] 51%|█████     | 369/729 [00:05<00:06, 59.40it/s] 52%|█████▏    | 376/729 [00:05<00:05, 59.15it/s] 53%|█████▎    | 383/729 [00:05<00:05, 60.20it/s] 53%|█████▎    | 390/729 [00:05<00:05, 60.91it/s] 54%|█████▍    | 397/729 [00:05<00:05, 61.56it/s] 55%|█████▌    | 404/729 [00:06<00:05, 62.49it/s] 56%|█████▋    | 411/729 [00:06<00:05, 62.79it/s] 57%|█████▋    | 418/729 [00:06<00:04, 62.88it/s] 58%|█████▊    | 425/729 [00:06<00:04, 63.72it/s] 59%|█████▉    | 433/729 [00:06<00:04, 67.86it/s] 60%|██████    | 441/729 [00:06<00:04, 69.56it/s] 62%|██████▏   | 449/729 [00:06<00:03, 71.49it/s] 63%|██████▎   | 457/729 [00:06<00:03, 73.29it/s] 64%|██████▍   | 465/729 [00:06<00:03, 74.40it/s] 65%|██████▌   | 474/729 [00:07<00:03, 76.06it/s] 66%|██████▌   | 482/729 [00:07<00:03, 76.04it/s] 67%|██████▋   | 490/729 [00:07<00:03, 70.42it/s] 68%|██████▊   | 498/729 [00:07<00:03, 64.87it/s] 69%|██████▉   | 506/729 [00:07<00:03, 68.06it/s] 71%|███████   | 514/729 [00:07<00:03, 71.10it/s] 72%|███████▏  | 522/729 [00:07<00:02, 72.27it/s] 73%|███████▎  | 530/729 [00:07<00:02, 73.33it/s] 74%|███████▍  | 538/729 [00:07<00:02, 74.48it/s] 75%|███████▍  | 546/729 [00:08<00:02, 75.64it/s] 76%|███████▌  | 554/729 [00:08<00:02, 75.66it/s] 77%|███████▋  | 562/729 [00:08<00:02, 75.52it/s] 78%|███████▊  | 570/729 [00:08<00:02, 76.52it/s] 79%|███████▉  | 578/729 [00:08<00:01, 76.08it/s] 80%|████████  | 586/729 [00:08<00:01, 76.36it/s] 81%|████████▏ | 594/729 [00:08<00:01, 76.11it/s] 83%|████████▎ | 602/729 [00:08<00:01, 76.43it/s] 84%|████████▎ | 610/729 [00:08<00:01, 76.47it/s] 85%|████████▍ | 618/729 [00:08<00:01, 76.59it/s] 86%|████████▌ | 626/729 [00:09<00:01, 77.24it/s] 87%|████████▋ | 634/729 [00:09<00:01, 77.62it/s] 88%|████████▊ | 642/729 [00:09<00:01, 77.31it/s] 89%|████████▉ | 650/729 [00:09<00:01, 76.98it/s] 90%|█████████ | 658/729 [00:09<00:01, 63.74it/s] 91%|█████████ | 665/729 [00:09<00:01, 63.46it/s] 92%|█████████▏| 672/729 [00:09<00:00, 62.51it/s] 93%|█████████▎| 679/729 [00:09<00:00, 61.87it/s] 94%|█████████▍| 686/729 [00:10<00:00, 61.53it/s] 95%|█████████▌| 693/729 [00:10<00:00, 62.35it/s] 96%|█████████▌| 701/729 [00:10<00:00, 65.82it/s] 97%|█████████▋| 709/729 [00:10<00:00, 69.01it/s] 98%|█████████▊| 717/729 [00:10<00:00, 71.15it/s] 99%|█████████▉| 725/729 [00:10<00:00, 72.63it/s]100%|██████████| 729/729 [00:10<00:00, 68.67it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.final_layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'lm_head.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  3.84it/s]  9%|▊         | 2/23 [00:00<00:05,  3.96it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.19it/s] 17%|█▋        | 4/23 [00:00<00:04,  4.31it/s] 22%|██▏       | 5/23 [00:01<00:04,  4.42it/s] 26%|██▌       | 6/23 [00:01<00:04,  4.21it/s] 30%|███       | 7/23 [00:01<00:03,  4.19it/s] 35%|███▍      | 8/23 [00:01<00:03,  4.29it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.41it/s] 43%|████▎     | 10/23 [00:02<00:02,  4.42it/s] 48%|████▊     | 11/23 [00:02<00:02,  4.47it/s] 52%|█████▏    | 12/23 [00:02<00:02,  4.50it/s] 57%|█████▋    | 13/23 [00:02<00:02,  4.50it/s] 61%|██████    | 14/23 [00:03<00:02,  4.33it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.39it/s] 70%|██████▉   | 16/23 [00:03<00:01,  4.31it/s] 74%|███████▍  | 17/23 [00:03<00:01,  4.40it/s] 78%|███████▊  | 18/23 [00:04<00:01,  4.53it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.34it/s] 87%|████████▋ | 20/23 [00:04<00:00,  4.36it/s] 91%|█████████▏| 21/23 [00:04<00:00,  4.39it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.26it/s]100%|██████████| 23/23 [00:05<00:00,  4.54it/s]100%|██████████| 23/23 [00:05<00:00,  4.37it/s]
{'bleu-1': 0.23706274673124283, 'bleu-2': 0.07016694616216282, 'bleu-3': 0.007412481051187097, 'bleu-4': 0.00036095230481889375}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-65240-s-LongLM-distrub-3728/test_sen_mse_add_mask_in_input.0
perplexity: 31.373398
begin predicting
acc : 0.8806584362139918
{'bleu-1': 0.18201352050176217, 'bleu-2': 0.04733134398424666, 'bleu-3': 0.00323664673325854, 'bleu-4': 0.00015242672753153195}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_1106-s-65240-s-LongLM-distrub-3728/test_sen_mse_add_mask_in_input.1
perplexity: 24.715794
begin predicting
acc : 0.99039780521262
