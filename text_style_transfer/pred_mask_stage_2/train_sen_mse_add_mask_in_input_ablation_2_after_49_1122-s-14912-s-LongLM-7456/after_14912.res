0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache /home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/jieba/cache/jieba.cache
Loading model cost 1.017 seconds.
Prefix dict has been built successfully.
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1it [00:01,  1.03s/it]19it [00:01, 22.65it/s]46it [00:01, 59.11it/s]76it [00:01, 100.95it/s]105it [00:01, 137.97it/s]131it [00:01, 164.57it/s]162it [00:01, 197.74it/s]190it [00:01, 218.28it/s]221it [00:01, 242.54it/s]258it [00:01, 275.70it/s]294it [00:02, 296.11it/s]330it [00:02, 314.18it/s]370it [00:02, 337.33it/s]406it [00:02, 339.46it/s]443it [00:02, 346.87it/s]479it [00:02, 346.66it/s]515it [00:02, 345.12it/s]550it [00:02, 345.92it/s]587it [00:02, 351.87it/s]623it [00:02, 350.62it/s]662it [00:03, 360.24it/s]701it [00:03, 367.77it/s]729it [00:03, 224.28it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  0%|          | 1/729 [00:05<1:09:17,  5.71s/it]  0%|          | 3/729 [00:05<18:34,  1.54s/it]    1%|          | 8/729 [00:05<05:16,  2.28it/s]  2%|▏         | 11/729 [00:06<03:46,  3.17it/s]  2%|▏         | 15/729 [00:06<02:18,  5.17it/s]  3%|▎         | 21/729 [00:06<01:18,  9.04it/s]  4%|▎         | 27/729 [00:06<00:51, 13.59it/s]  5%|▍         | 34/729 [00:10<03:05,  3.75it/s]  5%|▌         | 39/729 [00:10<02:14,  5.12it/s]  6%|▋         | 46/729 [00:10<01:28,  7.74it/s]  7%|▋         | 53/729 [00:10<01:00, 11.13it/s]  8%|▊         | 60/729 [00:10<00:43, 15.36it/s]  9%|▉         | 67/729 [00:11<00:32, 20.41it/s] 10%|█         | 74/729 [00:11<00:25, 26.18it/s] 11%|█         | 82/729 [00:11<00:19, 33.55it/s] 12%|█▏        | 90/729 [00:11<00:15, 41.30it/s] 13%|█▎        | 97/729 [00:11<00:14, 43.47it/s] 14%|█▍        | 104/729 [00:11<00:13, 48.05it/s] 15%|█▌        | 111/729 [00:11<00:11, 52.80it/s] 16%|█▌        | 118/729 [00:11<00:10, 56.62it/s] 17%|█▋        | 125/729 [00:11<00:10, 58.81it/s] 18%|█▊        | 132/729 [00:12<00:09, 60.70it/s] 19%|█▉        | 139/729 [00:12<00:09, 62.20it/s] 20%|██        | 147/729 [00:12<00:08, 65.90it/s] 21%|██▏       | 156/729 [00:12<00:08, 70.25it/s] 22%|██▏       | 164/729 [00:12<00:07, 72.09it/s] 24%|██▎       | 172/729 [00:12<00:07, 73.67it/s] 25%|██▍       | 181/729 [00:12<00:07, 76.10it/s] 26%|██▌       | 189/729 [00:12<00:07, 76.57it/s] 27%|██▋       | 198/729 [00:12<00:06, 79.05it/s] 28%|██▊       | 207/729 [00:13<00:06, 79.74it/s] 30%|██▉       | 216/729 [00:13<00:06, 81.27it/s] 31%|███       | 225/729 [00:13<00:06, 81.03it/s] 32%|███▏      | 234/729 [00:13<00:06, 82.13it/s] 33%|███▎      | 243/729 [00:13<00:05, 83.01it/s] 35%|███▍      | 252/729 [00:13<00:05, 83.85it/s] 36%|███▌      | 261/729 [00:13<00:05, 82.82it/s] 37%|███▋      | 270/729 [00:13<00:05, 82.91it/s] 38%|███▊      | 279/729 [00:13<00:05, 82.59it/s] 40%|███▉      | 288/729 [00:13<00:05, 82.08it/s] 41%|████      | 297/729 [00:14<00:05, 82.83it/s] 42%|████▏     | 306/729 [00:14<00:05, 82.88it/s] 43%|████▎     | 315/729 [00:14<00:05, 81.34it/s] 44%|████▍     | 324/729 [00:14<00:04, 81.50it/s] 46%|████▌     | 333/729 [00:14<00:04, 82.45it/s] 47%|████▋     | 342/729 [00:14<00:04, 83.12it/s] 48%|████▊     | 351/729 [00:14<00:04, 83.38it/s] 49%|████▉     | 360/729 [00:14<00:04, 83.19it/s] 51%|█████     | 369/729 [00:14<00:04, 83.69it/s] 52%|█████▏    | 378/729 [00:15<00:04, 83.75it/s] 53%|█████▎    | 387/729 [00:15<00:04, 82.58it/s] 54%|█████▍    | 396/729 [00:15<00:04, 82.06it/s] 56%|█████▌    | 405/729 [00:15<00:03, 82.77it/s] 57%|█████▋    | 414/729 [00:15<00:03, 84.02it/s] 58%|█████▊    | 423/729 [00:15<00:03, 84.56it/s] 59%|█████▉    | 432/729 [00:15<00:03, 84.64it/s] 60%|██████    | 441/729 [00:15<00:03, 83.62it/s] 62%|██████▏   | 450/729 [00:15<00:03, 81.90it/s] 63%|██████▎   | 459/729 [00:16<00:03, 80.50it/s] 64%|██████▍   | 468/729 [00:16<00:03, 80.63it/s] 65%|██████▌   | 477/729 [00:16<00:03, 81.33it/s] 67%|██████▋   | 486/729 [00:16<00:02, 81.14it/s] 68%|██████▊   | 495/729 [00:16<00:02, 82.23it/s] 69%|██████▉   | 504/729 [00:16<00:02, 82.12it/s] 70%|███████   | 513/729 [00:16<00:02, 81.76it/s] 72%|███████▏  | 522/729 [00:16<00:02, 81.73it/s] 73%|███████▎  | 531/729 [00:16<00:02, 81.40it/s] 74%|███████▍  | 540/729 [00:17<00:02, 80.90it/s] 75%|███████▌  | 549/729 [00:17<00:02, 81.15it/s] 77%|███████▋  | 558/729 [00:17<00:02, 82.10it/s] 78%|███████▊  | 567/729 [00:17<00:01, 82.86it/s] 79%|███████▉  | 576/729 [00:17<00:01, 83.15it/s] 80%|████████  | 585/729 [00:17<00:01, 83.72it/s] 81%|████████▏ | 594/729 [00:17<00:01, 83.81it/s] 83%|████████▎ | 603/729 [00:17<00:01, 83.88it/s] 84%|████████▍ | 612/729 [00:17<00:01, 82.79it/s] 85%|████████▌ | 621/729 [00:18<00:01, 82.59it/s] 86%|████████▋ | 630/729 [00:18<00:01, 82.86it/s] 88%|████████▊ | 639/729 [00:18<00:01, 83.31it/s] 89%|████████▉ | 648/729 [00:18<00:00, 83.70it/s] 90%|█████████ | 657/729 [00:18<00:00, 83.94it/s] 91%|█████████▏| 666/729 [00:18<00:00, 84.55it/s] 93%|█████████▎| 675/729 [00:18<00:00, 83.46it/s] 94%|█████████▍| 684/729 [00:18<00:00, 83.99it/s] 95%|█████████▌| 693/729 [00:18<00:00, 83.39it/s] 96%|█████████▋| 702/729 [00:18<00:00, 83.29it/s] 98%|█████████▊| 711/729 [00:19<00:00, 84.02it/s] 99%|█████████▉| 720/729 [00:19<00:00, 83.76it/s]100%|██████████| 729/729 [00:19<00:00, 82.32it/s]100%|██████████| 729/729 [00:19<00:00, 37.75it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.embed_tokens.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'lm_head.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:02,  7.89it/s]  9%|▊         | 2/23 [00:00<00:03,  6.88it/s] 13%|█▎        | 3/23 [00:00<00:02,  7.34it/s] 17%|█▋        | 4/23 [00:00<00:02,  7.29it/s] 22%|██▏       | 5/23 [00:00<00:02,  7.84it/s] 26%|██▌       | 6/23 [00:00<00:02,  8.25it/s] 30%|███       | 7/23 [00:00<00:01,  8.40it/s] 35%|███▍      | 8/23 [00:01<00:01,  8.27it/s] 39%|███▉      | 9/23 [00:01<00:01,  7.89it/s] 43%|████▎     | 10/23 [00:01<00:01,  7.80it/s] 48%|████▊     | 11/23 [00:01<00:01,  8.32it/s] 52%|█████▏    | 12/23 [00:01<00:01,  8.44it/s] 57%|█████▋    | 13/23 [00:01<00:01,  7.75it/s] 61%|██████    | 14/23 [00:01<00:01,  7.99it/s] 65%|██████▌   | 15/23 [00:01<00:01,  7.97it/s] 70%|██████▉   | 16/23 [00:02<00:00,  7.70it/s] 74%|███████▍  | 17/23 [00:02<00:00,  7.97it/s] 78%|███████▊  | 18/23 [00:02<00:00,  8.16it/s] 83%|████████▎ | 19/23 [00:02<00:00,  7.85it/s] 87%|████████▋ | 20/23 [00:02<00:00,  7.86it/s] 91%|█████████▏| 21/23 [00:02<00:00,  8.06it/s]100%|██████████| 23/23 [00:02<00:00,  9.13it/s]100%|██████████| 23/23 [00:02<00:00,  8.13it/s]
0it [00:00, ?it/s]/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/home/zhuxuekai/anaconda3/envs/py37/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
21it [00:00, 204.44it/s]45it [00:00, 224.94it/s]70it [00:00, 235.37it/s]94it [00:00, 233.12it/s]118it [00:00, 228.24it/s]141it [00:00, 227.40it/s]167it [00:00, 236.19it/s]191it [00:00, 236.35it/s]216it [00:00, 238.24it/s]242it [00:01, 243.75it/s]267it [00:01, 243.08it/s]292it [00:01, 235.99it/s]316it [00:01, 231.44it/s]342it [00:01, 237.04it/s]367it [00:01, 238.33it/s]391it [00:01, 236.52it/s]415it [00:01, 235.83it/s]439it [00:01, 235.24it/s]464it [00:01, 238.03it/s]489it [00:02, 239.94it/s]514it [00:02, 232.63it/s]538it [00:02, 233.42it/s]563it [00:02, 236.70it/s]588it [00:02, 237.74it/s]612it [00:02, 236.03it/s]637it [00:02, 237.40it/s]664it [00:02, 246.71it/s]689it [00:02, 246.75it/s]714it [00:03, 243.81it/s]729it [00:03, 237.04it/s]
  0%|          | 0/729 [00:00<?, ?it/s]  1%|          | 8/729 [00:00<00:09, 73.89it/s]  2%|▏         | 16/729 [00:00<00:09, 74.12it/s]  3%|▎         | 24/729 [00:00<00:09, 74.82it/s]  4%|▍         | 32/729 [00:00<00:09, 75.36it/s]  5%|▌         | 40/729 [00:00<00:09, 75.61it/s]  7%|▋         | 48/729 [00:00<00:08, 76.12it/s]  8%|▊         | 56/729 [00:00<00:09, 73.71it/s]  9%|▉         | 64/729 [00:00<00:09, 70.00it/s] 10%|▉         | 72/729 [00:01<00:09, 68.29it/s] 11%|█         | 79/729 [00:01<00:09, 67.20it/s] 12%|█▏        | 86/729 [00:01<00:09, 66.54it/s] 13%|█▎        | 94/729 [00:01<00:09, 68.77it/s] 14%|█▍        | 102/729 [00:01<00:08, 71.24it/s] 15%|█▌        | 110/729 [00:01<00:08, 71.47it/s] 16%|█▌        | 118/729 [00:01<00:08, 71.55it/s] 17%|█▋        | 126/729 [00:01<00:08, 71.59it/s] 18%|█▊        | 134/729 [00:01<00:08, 72.38it/s] 19%|█▉        | 142/729 [00:01<00:08, 73.19it/s] 21%|██        | 150/729 [00:02<00:07, 73.41it/s] 22%|██▏       | 158/729 [00:02<00:07, 72.94it/s] 23%|██▎       | 166/729 [00:02<00:07, 72.79it/s] 24%|██▍       | 174/729 [00:02<00:07, 74.08it/s] 25%|██▍       | 182/729 [00:02<00:07, 75.42it/s] 26%|██▌       | 190/729 [00:02<00:07, 76.10it/s] 27%|██▋       | 198/729 [00:02<00:07, 75.36it/s] 28%|██▊       | 206/729 [00:02<00:06, 75.82it/s] 29%|██▉       | 214/729 [00:02<00:06, 75.49it/s] 30%|███       | 222/729 [00:03<00:06, 75.06it/s] 32%|███▏      | 230/729 [00:03<00:06, 75.28it/s] 33%|███▎      | 238/729 [00:03<00:06, 75.67it/s] 34%|███▎      | 246/729 [00:03<00:06, 74.87it/s] 35%|███▍      | 254/729 [00:03<00:06, 74.21it/s] 36%|███▌      | 262/729 [00:03<00:06, 73.61it/s] 37%|███▋      | 270/729 [00:03<00:06, 73.34it/s] 38%|███▊      | 278/729 [00:03<00:06, 73.17it/s] 39%|███▉      | 286/729 [00:03<00:06, 73.00it/s] 40%|████      | 294/729 [00:04<00:05, 73.94it/s] 41%|████▏     | 302/729 [00:04<00:05, 74.21it/s] 43%|████▎     | 310/729 [00:04<00:05, 73.98it/s] 44%|████▎     | 318/729 [00:04<00:05, 74.21it/s] 45%|████▍     | 326/729 [00:04<00:05, 75.05it/s] 46%|████▌     | 334/729 [00:04<00:05, 75.13it/s] 47%|████▋     | 342/729 [00:04<00:05, 74.40it/s] 48%|████▊     | 350/729 [00:04<00:05, 74.82it/s] 49%|████▉     | 358/729 [00:04<00:04, 74.89it/s] 50%|█████     | 366/729 [00:04<00:04, 74.22it/s] 51%|█████▏    | 374/729 [00:05<00:04, 74.07it/s] 52%|█████▏    | 382/729 [00:05<00:04, 73.77it/s] 53%|█████▎    | 390/729 [00:05<00:04, 73.49it/s] 55%|█████▍    | 398/729 [00:05<00:04, 73.53it/s] 56%|█████▌    | 406/729 [00:05<00:04, 73.68it/s] 57%|█████▋    | 414/729 [00:05<00:04, 73.33it/s] 58%|█████▊    | 422/729 [00:05<00:04, 73.26it/s] 59%|█████▉    | 430/729 [00:05<00:04, 74.19it/s] 60%|██████    | 438/729 [00:05<00:03, 75.10it/s] 61%|██████▏   | 447/729 [00:06<00:03, 76.32it/s] 62%|██████▏   | 455/729 [00:06<00:03, 76.75it/s] 64%|██████▎   | 463/729 [00:06<00:03, 75.78it/s] 65%|██████▍   | 471/729 [00:06<00:03, 75.51it/s] 66%|██████▌   | 479/729 [00:06<00:03, 76.03it/s] 67%|██████▋   | 487/729 [00:06<00:03, 77.04it/s] 68%|██████▊   | 495/729 [00:06<00:03, 76.17it/s] 69%|██████▉   | 503/729 [00:06<00:03, 74.81it/s] 70%|███████   | 511/729 [00:06<00:02, 75.15it/s] 71%|███████   | 519/729 [00:07<00:02, 74.92it/s] 72%|███████▏  | 527/729 [00:07<00:02, 75.16it/s] 73%|███████▎  | 535/729 [00:07<00:02, 75.92it/s] 74%|███████▍  | 543/729 [00:07<00:02, 76.60it/s] 76%|███████▌  | 551/729 [00:07<00:02, 77.26it/s] 77%|███████▋  | 559/729 [00:07<00:02, 76.20it/s] 78%|███████▊  | 567/729 [00:07<00:02, 76.73it/s] 79%|███████▉  | 575/729 [00:07<00:01, 77.29it/s] 80%|███████▉  | 583/729 [00:07<00:01, 76.90it/s] 81%|████████  | 591/729 [00:07<00:01, 76.83it/s] 82%|████████▏ | 599/729 [00:08<00:01, 76.25it/s] 83%|████████▎ | 607/729 [00:08<00:01, 76.29it/s] 84%|████████▍ | 615/729 [00:08<00:01, 76.28it/s] 85%|████████▌ | 623/729 [00:08<00:01, 77.00it/s] 87%|████████▋ | 631/729 [00:08<00:01, 77.72it/s] 88%|████████▊ | 639/729 [00:08<00:01, 77.72it/s] 89%|████████▉ | 648/729 [00:08<00:01, 78.53it/s] 90%|████████▉ | 656/729 [00:08<00:00, 78.25it/s] 91%|█████████ | 664/729 [00:08<00:00, 78.00it/s] 92%|█████████▏| 672/729 [00:09<00:00, 77.42it/s] 93%|█████████▎| 680/729 [00:09<00:00, 77.19it/s] 94%|█████████▍| 688/729 [00:09<00:00, 76.78it/s] 95%|█████████▌| 696/729 [00:09<00:00, 76.20it/s] 97%|█████████▋| 704/729 [00:09<00:00, 76.46it/s] 98%|█████████▊| 712/729 [00:09<00:00, 75.65it/s] 99%|█████████▉| 720/729 [00:09<00:00, 75.02it/s]100%|█████████▉| 728/729 [00:09<00:00, 75.71it/s]100%|██████████| 729/729 [00:09<00:00, 74.70it/s]
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at pretrained_model/LongLM-base were not used when initializing T5EncoderModel_AddSoftSampling: ['decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.embed_tokens.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'lm_head.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight']
- This IS expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing T5EncoderModel_AddSoftSampling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:05,  4.08it/s]  9%|▊         | 2/23 [00:00<00:04,  4.52it/s] 13%|█▎        | 3/23 [00:00<00:04,  4.44it/s] 17%|█▋        | 4/23 [00:00<00:04,  4.35it/s] 22%|██▏       | 5/23 [00:01<00:04,  4.33it/s] 26%|██▌       | 6/23 [00:01<00:03,  4.45it/s] 30%|███       | 7/23 [00:01<00:03,  4.31it/s] 35%|███▍      | 8/23 [00:01<00:03,  4.62it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.66it/s] 43%|████▎     | 10/23 [00:02<00:02,  4.68it/s] 48%|████▊     | 11/23 [00:02<00:02,  4.60it/s] 52%|█████▏    | 12/23 [00:02<00:02,  4.64it/s] 57%|█████▋    | 13/23 [00:02<00:02,  4.35it/s] 61%|██████    | 14/23 [00:03<00:02,  4.46it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.50it/s] 70%|██████▉   | 16/23 [00:03<00:01,  4.56it/s] 74%|███████▍  | 17/23 [00:03<00:01,  4.57it/s] 78%|███████▊  | 18/23 [00:03<00:01,  4.56it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.53it/s] 87%|████████▋ | 20/23 [00:04<00:00,  4.65it/s] 91%|█████████▏| 21/23 [00:04<00:00,  4.60it/s] 96%|█████████▌| 22/23 [00:04<00:00,  4.70it/s]100%|██████████| 23/23 [00:05<00:00,  4.86it/s]100%|██████████| 23/23 [00:05<00:00,  4.57it/s]
{'bleu-1': 0.25104065337300646, 'bleu-2': 0.07868904469540398, 'bleu-3': 0.008930229686090455, 'bleu-4': 0.0005705318245613955}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-14912-s-LongLM-7456/test_sen_mse_add_mask_in_input.0
perplexity: 31.963734
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.661606 R=0.652051 F=0.656655
begin predicting
acc : 0.7283950617283951
{'distinct-1': 0.02864061456245825, 'distinct-2': 0.36392894329978226, 'distinct-3': 0.762798092209857, 'distinct-4': 0.9417972463443031}
{'bleu-1': 0.20707974621913622, 'bleu-2': 0.05935213133711195, 'bleu-3': 0.005133445027924561, 'bleu-4': 0.0002722129440475266}
./pred_mask_stage_2/train_sen_mse_add_mask_in_input_ablation_2_after_49_1122-s-14912-s-LongLM-7456/test_sen_mse_add_mask_in_input.1
perplexity: 25.751245
bert-base-chinese_L8_no-idf_version=0.3.10(hug_trans=4.9.0): P=0.629563 R=0.663594 F=0.645963
begin predicting
acc : 0.9821673525377229
{'distinct-1': 0.01616974415168812, 'distinct-2': 0.2797263273125342, 'distinct-3': 0.6475757126135483, 'distinct-4': 0.8471380805773496}
